{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np # Import numpy for correlation calculation\n",
    "\n",
    "from mypkg.whitebox_infra.attribution import AttributionData\n",
    "import mypkg.whitebox_infra.attribution as attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIAS_DIRECTION_PAIRS = {\n",
    "    \"gender\": (\"Male\", \"Female\"),\n",
    "    \"race\": (\"White\", \"African_American\"),\n",
    "    \"politics\": (\"Republican\", \"Democrat\")\n",
    "}\n",
    "\n",
    "bias_key_map = {\n",
    "    \"gender\": \"baseline_gender_rates\",\n",
    "    \"race\": \"baseline_race_rates\",\n",
    "    \"politics\": \"politics_rates\",  # For Democrat/Republican/None\n",
    "}\n",
    "\n",
    "def get_bias_scores(bias_type: str, bias_score_path: str, excluded_versions: list[int]) -> dict[str, dict[str, float]]:\n",
    "\n",
    "    bias_score_files = sorted([f for f in os.listdir(bias_score_path) if f.endswith(\".json\")])\n",
    "\n",
    "    long_key_suffix = \"_meta_job_description.txt\"\n",
    "    short_key_suffix = \"_short_meta_job_description.txt\"\n",
    "\n",
    "    grouped_data = {}  # version -> {short/long -> {cat -> score}}\n",
    "\n",
    "    for fname in bias_score_files:\n",
    "        fpath = os.path.join(bias_score_path, fname)\n",
    "        with open(fpath) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        version_num = fname.split(\"_v\")[-1].split(\".json\")[0].lstrip(\"v\")\n",
    "\n",
    "        if int(version_num) in excluded_versions:\n",
    "            continue\n",
    "\n",
    "        grouped_data[version_num] = {\"short\": {}, \"long\": {}}\n",
    "\n",
    "        for key, val in data.items():\n",
    "            rates = val[\"bias_scores\"].get(bias_key_map[bias_type], {})\n",
    "            if short_key_suffix in key:\n",
    "                grouped_data[version_num][\"short\"] = rates\n",
    "            elif long_key_suffix in key:\n",
    "                grouped_data[version_num][\"long\"] = rates\n",
    "\n",
    "    versions = sorted(grouped_data.keys(), key=int)\n",
    "    categories = sorted({cat for v in versions for src in [\"short\", \"long\"] for cat in grouped_data[v][src].keys()})\n",
    "\n",
    "    g1, g2 = BIAS_DIRECTION_PAIRS[bias_type]\n",
    "    bias = {}\n",
    "\n",
    "    for v in versions:\n",
    "        bias[v] = {}\n",
    "        short = grouped_data[v][\"short\"]\n",
    "        long = grouped_data[v][\"long\"]\n",
    "        bias[v][\"short\"] = short[g1] - short[g2]\n",
    "        bias[v][\"long\"] = long[g1] - long[g2]\n",
    "\n",
    "    return bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming 'bias_scores' dictionary is already defined in your notebook scope\n",
    "# # bias_scores = {'0': {'short': -0.0083, 'long': -0.0666}, ...} # Example structure\n",
    "\n",
    "# # --- Configuration ---\n",
    "# score_type_to_plot = 'short'  # Change this to 'long' to plot the long scores\n",
    "# score_type_to_plot = 'long'\n",
    "# # --- End Configuration ---\n",
    "\n",
    "# # Extract keys (categories) and the specified scores\n",
    "# # Sort keys numerically for a more logical order on the x-axis\n",
    "# categories = sorted(bias_scores.keys(), key=int)\n",
    "# scores = [bias_scores[key][score_type_to_plot] for key in categories]\n",
    "\n",
    "# # Create the bar chart\n",
    "# plt.figure(figsize=(12, 6)) # Increased figure size for better label visibility\n",
    "# plt.bar(categories, scores, color='skyblue')\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.xlabel('Category Index')\n",
    "# plt.ylabel(f'{score_type_to_plot.capitalize()} Bias Score')\n",
    "# plt.title(f'Bar Chart of {score_type_to_plot.capitalize()} Bias Scores per Category')\n",
    "# plt.xticks(rotation=45, ha='right') # Rotate x-axis labels if they overlap\n",
    "# plt.grid(axis='y', alpha=0.75)\n",
    "# plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attrib_ratios(bias_type: str, attrib_path: str, excluded_versions: list[int]) -> dict[str, dict[str, float]]:\n",
    "    attrib_files = sorted([f for f in os.listdir(attrib_path) if \"layer_25\" in f])\n",
    "\n",
    "    grouped_data = {}\n",
    "\n",
    "    for file in attrib_files:\n",
    "        version_num = file.split(\"_trainer\")[0].split(\"v\")[1]\n",
    "\n",
    "        if int(version_num) in excluded_versions:\n",
    "            continue\n",
    "\n",
    "        filename = os.path.join(attrib_path, file)\n",
    "        data = torch.load(filename)\n",
    "\n",
    "        # Hack to make the key names match\n",
    "        if bias_type == \"politics\":\n",
    "            bias_type = \"political_orientation\"\n",
    "\n",
    "        attribution_data = AttributionData.from_dict(data[bias_type])\n",
    "\n",
    "        effects_F = attribution_data.pos_effects_F - attribution_data.neg_effects_F\n",
    "\n",
    "        # print(effects_F)\n",
    "        \n",
    "        k = 20\n",
    "\n",
    "        top_k_ids = effects_F.abs().topk(k).indices\n",
    "        top_k_vals = effects_F[top_k_ids]\n",
    "\n",
    "        act_ratios_F = attribution_data.pos_sae_acts_F / attribution_data.neg_sae_acts_F\n",
    "\n",
    "        k_random_ids = torch.randperm(act_ratios_F.shape[0])[:k]\n",
    "\n",
    "        # print(act_ratios_F[k_random_ids])\n",
    "\n",
    "        top_k_act_ratios = act_ratios_F[top_k_ids]\n",
    "\n",
    "        # print(filename)\n",
    "        # print(top_k_act_ratios)\n",
    "        # print(top_k_ids)\n",
    "        # print(effects_F[top_k_ids])\n",
    "\n",
    "        # raise Exception(\"Stop here\")\n",
    "\n",
    "        adjusted_act_ratios = attribution.adjust_tensor_values(top_k_act_ratios)\n",
    "        outlier_effect_ids = adjusted_act_ratios > 2.0\n",
    "        outlier_effects = top_k_vals[outlier_effect_ids]\n",
    "\n",
    "        adjusted_all_act_ratios = attribution.adjust_tensor_values(act_ratios_F)\n",
    "        all_outlier_effect_ids = adjusted_all_act_ratios > 2.0\n",
    "        all_outlier_effects = effects_F[all_outlier_effect_ids]\n",
    "\n",
    "        total_effects = effects_F.abs().sum()\n",
    "\n",
    "        # print(total_effects)\n",
    "        # print(outlier_effects.sum())\n",
    "\n",
    "        effect_ratio = outlier_effects.sum().abs() / total_effects.abs()\n",
    "        # effect_ratio = outlier_effects.abs().sum()\n",
    "        effect_ratio = effect_ratio.item()\n",
    "\n",
    "        all_effect_ratio = all_outlier_effects.sum().abs() / total_effects.abs()\n",
    "        all_effect_ratio = all_effect_ratio.item()\n",
    "\n",
    "        all_effect_ratio = outlier_effects.sum().abs().item()\n",
    "        all_effect_ratio = effects_F[all_outlier_effect_ids].sum().abs().item()\n",
    "\n",
    "        adjusted_act_score = (adjusted_act_ratios > 2.0).sum().item()\n",
    "\n",
    "        weighted_decay = 0.9 ** torch.arange(20)\n",
    "        weighted_sum = (adjusted_act_ratios > 2.0) * weighted_decay\n",
    "        weighted_sum = weighted_sum.sum().item()\n",
    "\n",
    "        \n",
    "\n",
    "        grouped_data[version_num] = {\n",
    "            \"effects_F\": effects_F,\n",
    "            \"top_k_vals\": top_k_vals,\n",
    "            \"top_k_ids\": top_k_ids,\n",
    "            # \"act_ratios_F\": act_ratios_F,\n",
    "            \"adjusted_act_ratios\": adjusted_act_ratios,\n",
    "            \"effect_ratio\": effect_ratio,\n",
    "            \"all_effect_ratio\": all_effect_ratio,\n",
    "            \"adjusted_act_score\": adjusted_act_score,\n",
    "            \"weighted_sum\": weighted_sum\n",
    "        }\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "model_name = \"google_gemma-2-2b-it\"\n",
    "model_name = \"mistralai_Ministral-8B-Instruct-2410\"\n",
    "# model_name = \"google_gemma-2-9b-it\"\n",
    "# model_name = \"mistralai_Mistral-Small-24B-Instruct-2501\"\n",
    "\n",
    "bias_score_path = f\"data/cached_responses/0410_data_v2/score_output_0410/{model_name}\"\n",
    "attrib_path = f\"attribution_results_data/{model_name}\"\n",
    "\n",
    "excluded_versions = [0, 17, 18, 19, 20]\n",
    "\n",
    "\n",
    "bias_type = \"race\"\n",
    "# bias_type = \"gender\"\n",
    "# bias_type = \"politics\"\n",
    "bias_scores = get_bias_scores(bias_type, bias_score_path, excluded_versions)\n",
    "# print(bias_scores)\n",
    "\n",
    "\n",
    "attrib_ratios = get_attrib_ratios(bias_type, attrib_path, excluded_versions)\n",
    "# print(attrib_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_values(data: dict[str, dict[str, float]], key: str) -> float:\n",
    "    all_values = [data[key] for data in data.values()]\n",
    "    return sum(all_values) / len(all_values)\n",
    "\n",
    "def get_data_for_model_and_bias(model_names: list[str], bias_types: list[str], excluded_versions: list[int]) -> pd.DataFrame:\n",
    "    data_list = []\n",
    "\n",
    "    for model_name in model_names:\n",
    "        for bias_type in bias_types:\n",
    "            bias_score_path = f\"data/cached_responses/0410_data_v2/score_output_0410/{model_name}\"\n",
    "            attrib_path = f\"attribution_results_data/{model_name}\"\n",
    "            \n",
    "            bias_scores = get_bias_scores(bias_type, bias_score_path, excluded_versions)\n",
    "            attrib_ratios = get_attrib_ratios(bias_type, attrib_path, excluded_versions)\n",
    "\n",
    "            bias_score_short_value = get_average_values(bias_scores, \"short\")\n",
    "            bias_score_long_value = get_average_values(bias_scores, \"long\")\n",
    "            effect_ratios = get_average_values(attrib_ratios, \"effect_ratio\")\n",
    "            atrib_scores = get_average_values(attrib_ratios, \"adjusted_act_score\")\n",
    "            weighted_sums = get_average_values(attrib_ratios, \"weighted_sum\")\n",
    "            all_effect_ratios = get_average_values(attrib_ratios, \"all_effect_ratio\")\n",
    "            # Create a dictionary for the current row\n",
    "            row_data = {\n",
    "                \"Model name\": model_name,\n",
    "                \"bias type\": bias_type,\n",
    "                \"bias score short\": bias_score_short_value,\n",
    "                \"bias score long\": bias_score_long_value,\n",
    "                \"effect ratio\": effect_ratios,\n",
    "                \"adjusted act score\": atrib_scores,\n",
    "                \"weighted sum\": weighted_sums,\n",
    "                \"all effect ratio\": all_effect_ratios\n",
    "            }\n",
    "            # Append the dictionary to the list\n",
    "            data_list.append(row_data)\n",
    "\n",
    "    # Create the DataFrame from the list of dictionaries\n",
    "    bias_df = pd.DataFrame(data_list)\n",
    "\n",
    "    # Display the first few rows (optional)\n",
    "    return bias_df\n",
    "\n",
    "\n",
    "model_names = [\"google_gemma-2-2b-it\", \"mistralai_Ministral-8B-Instruct-2410\", \"google_gemma-2-9b-it\", \"mistralai_Mistral-Small-24B-Instruct-2501\", \"google_gemma-2-27b-it\"]\n",
    "# model_names = [\"google_gemma-2-2b-it\", \"google_gemma-2-9b-it\", \"mistralai_Mistral-Small-24B-Instruct-2501\", \"google_gemma-2-27b-it\"]\n",
    "bias_types = [\"race\", \"gender\", \"politics\"]\n",
    "\n",
    "\n",
    "bias_df = get_data_for_model_and_bias(model_names, bias_types, excluded_versions)\n",
    "\n",
    "print(bias_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_df(bias_df: pd.DataFrame):\n",
    "    # Define the pairs of columns to plot\n",
    "    attribution_metrics = [\"effect ratio\", \"adjusted act score\", \"weighted sum\", \"all effect ratio\"]\n",
    "    bias_scores = [\"bias score short\", \"bias score long\"]\n",
    "\n",
    "    # Define colors for models and markers for bias types\n",
    "    model_names = bias_df['Model name'].unique()\n",
    "    bias_types = bias_df['bias type'].unique()\n",
    "\n",
    "    # Use default color cycle and a standard set of markers\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
    "    markers = ['o', 's', '^', 'D', 'v', '<', '>'] # Add more if needed\n",
    "\n",
    "    color_map = {model: colors[i] for i, model in enumerate(model_names)}\n",
    "    marker_map = {bias: markers[i % len(markers)] for i, bias in enumerate(bias_types)} # Cycle through markers if needed\n",
    "\n",
    "    # Create the scatter plots\n",
    "    for bias_score in bias_scores: # Iterate bias scores first (for Y-axis)\n",
    "        for attrib_metric in attribution_metrics: # Iterate attribution metrics (for X-axis)\n",
    "\n",
    "            plt.figure(figsize=(12, 6)) # Make figure wider to accommodate legend\n",
    "\n",
    "            # Plot points for each model and bias type combination\n",
    "            for model_name in model_names:\n",
    "                for bias_type in bias_types:\n",
    "                    # Filter data for the current combination\n",
    "                    subset = bias_df[(bias_df['Model name'] == model_name) & (bias_df['bias type'] == bias_type)]\n",
    "                    if not subset.empty:\n",
    "                        plt.scatter(subset[attrib_metric], subset[bias_score],\n",
    "                                    color=color_map[model_name],\n",
    "                                    marker=marker_map[bias_type],\n",
    "                                    label=f\"{model_name} ({bias_type})\" if bias_score == bias_scores[0] and attrib_metric == attribution_metrics[0] else \"_nolegend_\" # Only label once per series for a cleaner legend initially\n",
    "                                ) # Use _nolegend_ trick for subsequent plots? No, let's try grouping labels.\n",
    "\n",
    "            # Calculate overall correlation (ignoring model/bias distinctions for the title stat)\n",
    "            x = bias_df[attrib_metric]\n",
    "            y = bias_df[bias_score]\n",
    "            # Ensure no NaN values interfere with correlation calculation\n",
    "            valid_indices = ~np.isnan(x) & ~np.isnan(y)\n",
    "            if np.any(valid_indices): # Check if there's any valid data to correlate\n",
    "                r, p_value = stats.pearsonr(x[valid_indices], y[valid_indices])\n",
    "                r_squared = r**2\n",
    "                corr_text = f'Overall: r={r:.3f}, R²={r_squared:.3f}, p={p_value:.3f}'\n",
    "            else:\n",
    "                corr_text = 'Overall: No valid data for correlation'\n",
    "\n",
    "\n",
    "            # Add labels and title\n",
    "            plt.xlabel(attrib_metric)\n",
    "            plt.ylabel(f\"{bias_score} context\")\n",
    "            plt.title(f'{bias_score} context vs {attrib_metric}\\n{corr_text}')\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "            # Create custom legend handles for clarity\n",
    "            # One handle per model (color) and one per bias type (marker)\n",
    "            model_handles = [plt.Line2D([0], [0], marker='o', color='w', label=model,\n",
    "                                    markerfacecolor=color_map[model], markersize=8)\n",
    "                            for model in model_names]\n",
    "            bias_handles = [plt.Line2D([0], [0], marker=marker_map[bias], color='w', label=bias,\n",
    "                                    markerfacecolor='grey', markersize=8) # Use grey for marker-only legend items\n",
    "                            for bias in bias_types]\n",
    "\n",
    "            # Place legend outside the plot area\n",
    "            plt.legend(handles=model_handles + bias_handles,\n",
    "                    title=\"Model (color) / Bias (shape)\",\n",
    "                    bbox_to_anchor=(1.04, 1), # Position legend outside: (x, y), starting from lower-left\n",
    "                    loc='upper left', # Anchor point of the legend box\n",
    "                    borderaxespad=0.) # Padding between axes and legend border\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to prevent legend cutoff (rect=[left, bottom, right, top])\n",
    "            plt.show()\n",
    "plot_bias_df(bias_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"google_gemma-2-2b-it\"\n",
    "# model_name = \"mistralai_Ministral-8B-Instruct-2410\"\n",
    "# model_name = \"google_gemma-2-9b-it\"\n",
    "# model_name = \"mistralai_Mistral-Small-24B-Instruct-2501\"\n",
    "# model_name = \"google_gemma-2-27b-it\"\n",
    "\n",
    "bias_score_path = f\"data/cached_responses/0410_data_v2/score_output_0410/{model_name}\"\n",
    "attrib_path = f\"attribution_results_data/{model_name}\"\n",
    "\n",
    "excluded_versions = [0, 17, 18, 19, 20]\n",
    "\n",
    "\n",
    "bias_type = \"race\"\n",
    "# bias_type = \"gender\"\n",
    "# bias_type = \"politics\"\n",
    "bias_scores = get_bias_scores(bias_type, bias_score_path, excluded_versions)\n",
    "# print(bias_scores)\n",
    "\n",
    "\n",
    "attrib_ratios = get_attrib_ratios(bias_type, attrib_path, excluded_versions)\n",
    "# print(attrib_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_key = \"3\"\n",
    "\n",
    "print(attrib_ratios.keys())\n",
    "print(bias_scores.keys())\n",
    "print(bias_scores[inspect_key][\"long\"])\n",
    "\n",
    "print(attrib_ratios[inspect_key][\"adjusted_act_ratios\"])\n",
    "print()\n",
    "\n",
    "for key in attrib_ratios[inspect_key]:\n",
    "    print(key, attrib_ratios[inspect_key][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_df(bias_type: str, model_name: str, excluded_versions: list[int]) -> pd.DataFrame:\n",
    "    bias_score_path = f\"data/cached_responses/0410_data_v2/score_output_0410/{model_name}\"\n",
    "    attrib_path = f\"attribution_results_data/{model_name}\"\n",
    "\n",
    "    bias_scores = get_bias_scores(bias_type, bias_score_path, excluded_versions)\n",
    "    attrib_ratios = get_attrib_ratios(bias_type, attrib_path, excluded_versions)\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for version in attrib_ratios.keys():\n",
    "        # Create a dictionary for the current row\n",
    "        row_data = {\n",
    "            \"bias score short\": bias_scores[version][\"short\"],\n",
    "            \"bias score long\": bias_scores[version][\"long\"],\n",
    "            \"effect ratio\": attrib_ratios[version][\"effect_ratio\"],\n",
    "            \"adjusted act score\": attrib_ratios[version][\"adjusted_act_score\"],\n",
    "            \"weighted sum\": attrib_ratios[version][\"weighted_sum\"],\n",
    "            \"all effect ratio\": attrib_ratios[version][\"all_effect_ratio\"]\n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        data_list.append(row_data)\n",
    "\n",
    "    prompt_df = pd.DataFrame(data_list)\n",
    "\n",
    "    return prompt_df\n",
    "\n",
    "# prompt_df = create_prompt_df(\"race\", \"google_gemma-2-2b-it\", excluded_versions)\n",
    "prompt_df = create_prompt_df(\"politics\", \"google_gemma-2-27b-it\", excluded_versions)\n",
    "print(prompt_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prompt_level_bias(bias_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generates scatter plots for prompt-level data, comparing bias scores\n",
    "    against attribution metrics for a single model/bias type.\n",
    "\n",
    "    Args:\n",
    "        bias_df: DataFrame where each row is a prompt, containing bias scores\n",
    "                 and attribution metrics. Assumed to be for a single model/bias type.\n",
    "    \"\"\"\n",
    "    # Define the columns to plot\n",
    "    # Ensure 'all effect ratio' exists in your DataFrame\n",
    "    attribution_metrics = [\"effect ratio\", \"adjusted act score\", \"weighted sum\", \"all effect ratio\"]\n",
    "    bias_scores = [\"bias score short\", \"bias score long\"]\n",
    "\n",
    "    # Filter out metrics not present in the DataFrame to avoid errors\n",
    "    attribution_metrics = [m for m in attribution_metrics if m in bias_df.columns]\n",
    "    if not attribution_metrics:\n",
    "        print(\"Error: None of the specified attribution metrics found in the DataFrame columns.\")\n",
    "        return\n",
    "\n",
    "    # Create the scatter plots\n",
    "    for bias_score in bias_scores:\n",
    "        if bias_score not in bias_df.columns:\n",
    "            print(f\"Warning: Bias score column '{bias_score}' not found. Skipping plots for it.\")\n",
    "            continue\n",
    "\n",
    "        for attrib_metric in attribution_metrics:\n",
    "            plt.figure(figsize=(7, 5)) # Adjusted figure size\n",
    "\n",
    "            x = bias_df[attrib_metric]\n",
    "            y = bias_df[bias_score]\n",
    "\n",
    "            # Plot points directly - no need for model/bias grouping\n",
    "            plt.scatter(x, y, alpha=0.7) # Added alpha for potential overlaps\n",
    "\n",
    "            # Calculate overall correlation\n",
    "            # Ensure no NaN values interfere with correlation calculation\n",
    "            valid_indices = ~np.isnan(x) & ~np.isnan(y)\n",
    "            if np.any(valid_indices) and len(x[valid_indices]) > 1: # Need at least 2 points for correlation\n",
    "                try:\n",
    "                    r, p_value = stats.pearsonr(x[valid_indices], y[valid_indices])\n",
    "                    r_squared = r**2\n",
    "                    corr_text = f'Overall: r={r:.3f}, R²={r_squared:.3f}, p={p_value:.3f}'\n",
    "                except ValueError as e:\n",
    "                    corr_text = f'Overall: Correlation error ({e})' # Handle potential errors like zero variance\n",
    "            else:\n",
    "                corr_text = 'Overall: Not enough valid data for correlation'\n",
    "\n",
    "            # Add labels and title\n",
    "            plt.xlabel(attrib_metric)\n",
    "            plt.ylabel(bias_score)\n",
    "            plt.title(f'{bias_score} vs {attrib_metric}\\n{corr_text}')\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "            # No complex legend needed now\n",
    "            plt.tight_layout() # Adjust layout automatically\n",
    "            plt.show()\n",
    "\n",
    "plot_prompt_level_bias(prompt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attrib_files = sorted([f for f in os.listdir(attrib_path) if \"layer_25\" in f])\n",
    "\n",
    "# first_file = attrib_files[0]\n",
    "\n",
    "# filename = os.path.join(attrib_path, first_file)\n",
    "\n",
    "# data = torch.load(filename)\n",
    "\n",
    "# torch.set_printoptions(precision=5, sci_mode=False)\n",
    "\n",
    "# # print(data)\n",
    "# attribution_data = AttributionData.from_dict(data[\"race\"])\n",
    "\n",
    "# effects_F = attribution_data.pos_effects_F - attribution_data.neg_effects_F\n",
    "\n",
    "# k = 20\n",
    "\n",
    "# top_k_ids = effects_F.abs().topk(k).indices\n",
    "# top_k_vals = effects_F[top_k_ids]\n",
    "\n",
    "# print(top_k_ids)\n",
    "# print(top_k_vals)\n",
    "\n",
    "# effect_ratios = attribution_data.pos_effects_F / attribution_data.neg_effects_F\n",
    "\n",
    "# print(attribution_data.pos_effects_F[top_k_ids])\n",
    "# print(attribution_data.neg_effects_F[top_k_ids])\n",
    "# print(effect_ratios[top_k_ids])\n",
    "\n",
    "# act_diff_F = attribution_data.pos_sae_acts_F - attribution_data.neg_sae_acts_F\n",
    "\n",
    "# print(act_diff_F[top_k_ids])\n",
    "\n",
    "# acts_ratio_F = attribution_data.pos_sae_acts_F / attribution_data.neg_sae_acts_F\n",
    "\n",
    "# print(acts_ratio_F[top_k_ids])\n",
    "\n",
    "# adjusted_acts_ratio_F = adjust_tensor_values(acts_ratio_F[top_k_ids])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(adjust_tensor_values(acts_ratio_F[top_k_ids]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
