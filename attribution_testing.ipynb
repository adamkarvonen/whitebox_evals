{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import random\n",
    "\n",
    "import mypkg.whitebox_infra.attribution as attribution\n",
    "import mypkg.whitebox_infra.dictionaries.batch_topk_sae as batch_topk_sae\n",
    "import mypkg.whitebox_infra.data_utils as data_utils\n",
    "import mypkg.whitebox_infra.model_utils as model_utils\n",
    "import mypkg.whitebox_infra.interp_utils as interp_utils\n",
    "import mypkg.pipeline.setup.dataset as dataset_setup\n",
    "import mypkg.pipeline.infra.hiring_bias_prompts as hiring_bias_prompts\n",
    "from mypkg.eval_config import EvalConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "model_name = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n",
    "# model_name = \"google/gemma-2-9b-it\"\n",
    "# model_name = \"google/gemma-2-27b-it\"\n",
    "dtype = torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=dtype, device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "gradient_checkpointing = False\n",
    "\n",
    "if model_name == \"google/gemma-2-27b-it\":\n",
    "    gradient_checkpointing = True\n",
    "    batch_size = 1\n",
    "elif model_name == \"mistralai/Mistral-Small-24B-Instruct-2501\":\n",
    "    batch_size = 2\n",
    "else:\n",
    "    batch_size = 4\n",
    "\n",
    "if gradient_checkpointing:\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "chosen_layer_percentage = [50]\n",
    "\n",
    "chosen_layers = []\n",
    "for layer_percent in chosen_layer_percentage:\n",
    "    chosen_layers.append(model_utils.MODEL_CONFIGS[model_name][\"layer_mappings\"][layer_percent][\"layer\"])\n",
    "\n",
    "eval_config = EvalConfig(\n",
    "        model_name=model_name,\n",
    "        political_orientation=True,\n",
    "        pregnancy=False,\n",
    "        employment_gap=False,\n",
    "        anthropic_dataset=False,\n",
    "        downsample=100,\n",
    "        gpu_inference=True,\n",
    "        anti_bias_statement_file=\"v1.txt\",\n",
    "        job_description_file=\"short_meta_job_description.txt\",\n",
    "        system_prompt_filename=\"yes_no.txt\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sae_repo = \"adamkarvonen/ministral_saes\"\n",
    "# sae_path = f\"mistralai_Ministral-8B-Instruct-2410_batch_top_k/resid_post_layer_{chosen_layers[0]}/trainer_1/ae.pt\"\n",
    "\n",
    "# sae = batch_topk_sae.load_dictionary_learning_batch_topk_sae(\n",
    "#     repo_id=sae_repo,\n",
    "#     filename=sae_path,\n",
    "#     model_name=model_name,\n",
    "#     device=device,\n",
    "#     dtype=dtype,\n",
    "#     layer=chosen_layers[0],\n",
    "#     local_dir=\"downloaded_saes\",\n",
    "# )\n",
    "trainer_id = 1\n",
    "sae = model_utils.load_model_sae(model_name, device, dtype, chosen_layer_percentage[0], trainer_id=trainer_id)\n",
    "\n",
    "submodules = [model_utils.get_submodule(model, chosen_layers[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset_setup.load_raw_dataset()\n",
    "\n",
    "industry = \"INFORMATION-TECHNOLOGY\"\n",
    "downsample = eval_config.downsample\n",
    "random_seed = eval_config.random_seed\n",
    "\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "df = dataset_setup.filter_by_industry(df, industry)\n",
    "\n",
    "df = dataset_setup.balanced_downsample(df, downsample, random_seed)\n",
    "\n",
    "\n",
    "args = hiring_bias_prompts.HiringBiasArgs(\n",
    "    political_orientation=True,\n",
    "    employment_gap=False,\n",
    "    pregnancy=False,\n",
    "    race=False,\n",
    "    gender=False,\n",
    ")\n",
    "\n",
    "\n",
    "args = hiring_bias_prompts.HiringBiasArgs(\n",
    "    political_orientation=False,\n",
    "    employment_gap=False,\n",
    "    pregnancy=False,\n",
    "    race=True,\n",
    "    gender=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "prompts = hiring_bias_prompts.create_all_prompts_hiring_bias(df, args, eval_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = hiring_bias_prompts.process_hiring_bias_resumes_prompts(prompts, args)\n",
    "\n",
    "train_texts = model_utils.add_chat_template(train_texts, model_name)\n",
    "\n",
    "dataloader = data_utils.create_simple_dataloader(\n",
    "    train_texts, train_labels, model_name, device, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the custom loss function\n",
    "yes_vs_no_loss_fn = attribution.make_yes_no_loss_fn(\n",
    "    tokenizer,\n",
    "    yes_candidates=[\"yes\", \" yes\", \"Yes\", \" Yes\", \"YES\", \" YES\"],\n",
    "    no_candidates=[\"no\", \" no\", \"No\", \" No\", \"NO\", \" NO\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "effects_F, error_effect = attribution.get_effects(\n",
    "    model,\n",
    "    sae,\n",
    "    dataloader,\n",
    "    yes_vs_no_loss_fn,\n",
    "    submodules,\n",
    "    chosen_layers,\n",
    "    device,\n",
    ")\n",
    "\n",
    "# Print peak memory usage\n",
    "if torch.cuda.is_available():\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # Convert to MB\n",
    "    print(f\"Peak CUDA memory usage: {peak_memory:.2f} MB\")\n",
    "\n",
    "# Peak CUDA memory usage: 33432.72 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ids = effects_F.abs().topk(20).indices\n",
    "print(top_k_ids)\n",
    "\n",
    "top_k_vals = effects_F[top_k_ids]\n",
    "print(top_k_vals)\n",
    "\n",
    "print(error_effect)\n",
    "\n",
    "# tensor([ 4393, 15242,  9049,  3959, 11802, 14960,   428,  9920,  2715,  3509,\n",
    "#          9444, 12979,  9319,  8910, 12243,  7781, 11637, 10283,  4204,  2557],\n",
    "#        device='cuda:0')\n",
    "# tensor([0.0074, 0.0058, 0.0033, 0.0016, 0.0016, 0.0013, 0.0011, 0.0009, 0.0008,\n",
    "#         0.0008, 0.0008, 0.0008, 0.0007, 0.0007, 0.0007, 0.0007, 0.0006, 0.0006,\n",
    "#         0.0006, 0.0005], device='cuda:0')\n",
    "\n",
    "# tensor([ 4393, 15242,  9049, 13855, 11802,  3959,  2039, 14960,  1683,  3509,\n",
    "#           428,  4794,  3645,  9920,  5911,  1160,  1656, 16078,  9319,   394],\n",
    "#        device='cuda:0')\n",
    "# tensor([ 0.0305,  0.0242,  0.0133, -0.0078,  0.0064,  0.0063, -0.0058,  0.0052,\n",
    "#         -0.0051,  0.0050,  0.0045, -0.0043, -0.0042,  0.0040, -0.0039, -0.0034,\n",
    "#         -0.0033, -0.0033,  0.0033, -0.0031], device='cuda:0')\n",
    "# tensor(-0.0181, device='cuda:0')\n",
    "\n",
    "# tensor([ 4393, 15242,  9049, 13855, 11802,  3645,  2039,  3959,  1683,  3509,\n",
    "#         14960,  9920,  4794,  1656,  5911, 16078,  9319,  1160,  5286,   394],\n",
    "#        device='cuda:0')\n",
    "# tensor([ 0.0276,  0.0227,  0.0114, -0.0077,  0.0062, -0.0058, -0.0056,  0.0055,\n",
    "#         -0.0049,  0.0047,  0.0046,  0.0038, -0.0036, -0.0035, -0.0034, -0.0034,\n",
    "#          0.0033, -0.0032, -0.0030, -0.0029], device='cuda:0')\n",
    "# tensor(-0.0127, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sae.W_dec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts_dir = \"max_acts\"\n",
    "acts_filename = f\"acts_{model_name}_layer_{chosen_layers[0]}_trainer_{trainer_id}_layer_percent_{chosen_layer_percentage[0]}.pt\".replace(\"/\", \"_\")\n",
    "if not os.path.exists(acts_filename):\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    path_to_config = hf_hub_download(\n",
    "        repo_id=\"adamkarvonen/sae_max_acts\",\n",
    "        filename=acts_filename,\n",
    "        force_download=False,\n",
    "        local_dir=acts_dir,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "\n",
    "    acts_path = os.path.join(acts_dir, acts_filename)\n",
    "    acts_data = torch.load(acts_path)\n",
    "    \n",
    "    # max_tokens, max_acts = interp_utils.get_interp_prompts(\n",
    "    #     model,\n",
    "    #     submodules[0],\n",
    "    #     sae,\n",
    "    #     torch.tensor(list(range(sae.W_dec.shape[0]))),\n",
    "    #     context_length=128,\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     batch_size=batch_size * 32,\n",
    "    #     num_tokens=30_000_000,\n",
    "    # )\n",
    "    # acts_data = {\n",
    "    #     \"max_tokens\": max_tokens,\n",
    "    #     \"max_acts\": max_acts,\n",
    "    # }\n",
    "    # torch.save(acts_data, acts_filename)\n",
    "else:\n",
    "    acts_path = os.path.join(acts_dir, acts_filename)\n",
    "    acts_data = torch.load(acts_path)\n",
    "max_tokens = acts_data[\"max_tokens\"].cpu()\n",
    "max_acts = acts_data[\"max_acts\"].cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "import gc\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "def _list_decode(x):\n",
    "    if len(x.shape) == 0:\n",
    "        return tokenizer.decode(x, skip_special_tokens=False)\n",
    "    else:\n",
    "        return [_list_decode(y) for y in x]\n",
    "    \n",
    "\n",
    "clear_output(wait=True)\n",
    "gc.collect()\n",
    "\n",
    "for i in range(20):\n",
    "    feature_idx = top_k_ids[i]\n",
    "    feature_val = top_k_vals[i]\n",
    "    print(f\"Feature {i}, value: {feature_val}\")\n",
    "    selected_token_KL = max_tokens[feature_idx]\n",
    "\n",
    "    selected_activations_KL11 = [max_acts[feature_idx, k, :, None, None] for k in range(5)]\n",
    "    selected_token_strs_KL = _list_decode(selected_token_KL)\n",
    "\n",
    "    for k in range(len(selected_token_strs_KL)):\n",
    "        if \"<s>\" in selected_token_strs_KL[k][0] or \"<bos>\" in selected_token_strs_KL[k][0]:\n",
    "            selected_token_strs_KL[k][0] = \"BOS>\"\n",
    "\n",
    "    # selected_token_strs_KL = tokenizer.batch_decode(selected_token_KL, skip_special_tokens=False)\n",
    "    # for k in range(len(selected_token_strs_KL)):\n",
    "    #     string = selected_token_strs_KL[k]\n",
    "    #     print(string[:10])\n",
    "        # print(\"\".join(string))\n",
    "\n",
    "    html_activations = text_neuron_activations(selected_token_strs_KL, selected_activations_KL11)\n",
    "    display(html_activations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
