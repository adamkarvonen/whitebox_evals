{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/whitebox_evals/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 01:43:41 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 01:43:42,223\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import einops\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import random\n",
    "\n",
    "import mypkg.whitebox_infra.attribution as attribution\n",
    "import mypkg.whitebox_infra.dictionaries.batch_topk_sae as batch_topk_sae\n",
    "import mypkg.whitebox_infra.data_utils as data_utils\n",
    "import mypkg.whitebox_infra.model_utils as model_utils\n",
    "import mypkg.whitebox_infra.interp_utils as interp_utils\n",
    "import mypkg.pipeline.setup.dataset as dataset_setup\n",
    "import mypkg.pipeline.infra.hiring_bias_prompts as hiring_bias_prompts\n",
    "from mypkg.eval_config import EvalConfig\n",
    "import mypkg.pipeline.infra.model_inference as model_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 10/10 [00:07<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "model_name = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n",
    "# model_name = \"google/gemma-2-9b-it\"\n",
    "# model_name = \"google/gemma-2-27b-it\"\n",
    "\n",
    "args = hiring_bias_prompts.HiringBiasArgs(\n",
    "    political_orientation=True,\n",
    "    employment_gap=False,\n",
    "    pregnancy=False,\n",
    "    race=False,\n",
    "    gender=False,\n",
    ")\n",
    "\n",
    "\n",
    "args = hiring_bias_prompts.HiringBiasArgs(\n",
    "    political_orientation=False,\n",
    "    employment_gap=False,\n",
    "    pregnancy=False,\n",
    "    race=True,\n",
    "    gender=False,\n",
    ")\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=dtype, device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "gradient_checkpointing = False\n",
    "\n",
    "if model_name == \"google/gemma-2-27b-it\":\n",
    "    gradient_checkpointing = True\n",
    "    batch_size = 1\n",
    "elif model_name == \"mistralai/Mistral-Small-24B-Instruct-2501\":\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 3\n",
    "\n",
    "if gradient_checkpointing:\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "chosen_layer_percentage = [25]\n",
    "# chosen_layer_percentage = [50]\n",
    "\n",
    "system_prompt = \"yes_no.txt\"\n",
    "\n",
    "chosen_layers = []\n",
    "for layer_percent in chosen_layer_percentage:\n",
    "    chosen_layers.append(model_utils.MODEL_CONFIGS[model_name][\"layer_mappings\"][layer_percent][\"layer\"])\n",
    "\n",
    "eval_config = EvalConfig(\n",
    "        model_name=model_name,\n",
    "        political_orientation=True,\n",
    "        pregnancy=False,\n",
    "        employment_gap=False,\n",
    "        anthropic_dataset=False,\n",
    "        downsample=50,\n",
    "        # downsample=10,\n",
    "        gpu_inference=True,\n",
    "        anti_bias_statement_file=\"v1.txt\",\n",
    "        # anti_bias_statement_file=\"v3.txt\",\n",
    "        # anti_bias_statement_file=\"v17.txt\",\n",
    "        job_description_file=\"short_meta_job_description.txt\",\n",
    "        system_prompt_filename=system_prompt,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original keys in state_dict: dict_keys(['b_dec', 'k', 'threshold', 'decoder.weight', 'encoder.weight', 'encoder.bias'])\n",
      "Renamed keys in state_dict: dict_keys(['b_dec', 'k', 'threshold', 'W_dec', 'W_enc', 'b_enc'])\n"
     ]
    }
   ],
   "source": [
    "# sae_repo = \"adamkarvonen/ministral_saes\"\n",
    "# sae_path = f\"mistralai_Ministral-8B-Instruct-2410_batch_top_k/resid_post_layer_{chosen_layers[0]}/trainer_1/ae.pt\"\n",
    "\n",
    "# sae = batch_topk_sae.load_dictionary_learning_batch_topk_sae(\n",
    "#     repo_id=sae_repo,\n",
    "#     filename=sae_path,\n",
    "#     model_name=model_name,\n",
    "#     device=device,\n",
    "#     dtype=dtype,\n",
    "#     layer=chosen_layers[0],\n",
    "#     local_dir=\"downloaded_saes\",\n",
    "# )\n",
    "trainer_id = 2\n",
    "\n",
    "if \"gemma\" in model_name:\n",
    "    trainer_id = 0\n",
    "    \n",
    "sae = model_utils.load_model_sae(model_name, device, dtype, chosen_layer_percentage[0], trainer_id=trainer_id)\n",
    "\n",
    "submodules = [model_utils.get_submodule(model, chosen_layers[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled to 50 unique resumes\n",
      "Total samples after maintaining demographic variations: 200\n"
     ]
    }
   ],
   "source": [
    "df = dataset_setup.load_raw_dataset()\n",
    "\n",
    "industry = \"INFORMATION-TECHNOLOGY\"\n",
    "downsample = eval_config.downsample\n",
    "random_seed = eval_config.random_seed\n",
    "\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "df = dataset_setup.filter_by_industry(df, industry)\n",
    "\n",
    "df = dataset_setup.balanced_downsample(df, downsample, random_seed)\n",
    "\n",
    "\n",
    "\n",
    "prompts = hiring_bias_prompts.create_all_prompts_hiring_bias(df, args, eval_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pad token found, setting eos token as pad token\n",
      "Filtered out 4 samples that exceeded max_length (2500)\n",
      "Original dataset size: 200, new size: 196\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_labels = hiring_bias_prompts.process_hiring_bias_resumes_prompts(prompts, args)\n",
    "\n",
    "train_texts = model_utils.add_chat_template(train_texts, model_name)\n",
    "\n",
    "dataloader = data_utils.create_simple_dataloader(\n",
    "    train_texts, train_labels, model_name, device, batch_size=batch_size, max_length=2500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_answers = model_inference.run_single_forward_pass_transformers(\n",
    "#     prompts, model_name, batch_size=batch_size * 6, model=model\n",
    "# )\n",
    "\n",
    "# bias_scores = hiring_bias_prompts.evaluate_bias(\n",
    "#     model_answers,\n",
    "#     system_prompt\n",
    "# )\n",
    "# print(bias_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:08<00:00, 23.61it/s]\n"
     ]
    }
   ],
   "source": [
    "diff_acts_F = attribution.get_activations(model, sae, dataloader, submodules, chosen_layers, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([61599, 50902, 33592, 47196,  6638, 41619, 39383, 61307, 17161, 57264,\n",
      "        53451, 29812, 46652, 49757, 46425, 44249,   574, 43212, 24257, 40698],\n",
      "       device='cuda:0')\n",
      "tensor([-0.0014,  0.0014,  0.0013, -0.0011,  0.0008, -0.0008,  0.0008,  0.0007,\n",
      "        -0.0006,  0.0006,  0.0006, -0.0006, -0.0005, -0.0005, -0.0005,  0.0004,\n",
      "         0.0004,  0.0004,  0.0004,  0.0004], device='cuda:0')\n",
      "diff_acts/v1_trainer_2_model_mistralai_Mistral-Small-24B-Instruct-2501_layer_25_attrib_data.pt\n"
     ]
    }
   ],
   "source": [
    "acts_top_k_ids = diff_acts_F.abs().topk(20).indices\n",
    "print(acts_top_k_ids)\n",
    "\n",
    "acts_top_k_vals = diff_acts_F[acts_top_k_ids]\n",
    "print(acts_top_k_vals)\n",
    "\n",
    "output_filename = os.path.join(\n",
    "    \"diff_acts\",\n",
    "    f\"{eval_config.anti_bias_statement_file.replace('.txt', '')}_trainer_{trainer_id}_model_{model_name.replace('/', '_')}_layer_{chosen_layer_percentage[0]}_attrib_data.pt\",\n",
    ")\n",
    "print(output_filename)\n",
    "\n",
    "os.makedirs(\"diff_acts\", exist_ok=True)\n",
    "\n",
    "diff_acts = {\"diff_acts_F\": diff_acts_F}\n",
    "diff_acts[\"config\"] = {\n",
    "        \"model_name\": model_name,\n",
    "        \"layer\": chosen_layers[0],\n",
    "        \"bias_categories\": \"N\\A\",\n",
    "        \"anti_bias_statement_file\": eval_config.anti_bias_statement_file,\n",
    "        \"downsample\": eval_config.downsample,\n",
    "        \"random_seed\": eval_config.random_seed,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"chosen_layer_percentage\": chosen_layer_percentage,\n",
    "        \"trainer_id\": trainer_id,\n",
    "    }\n",
    "\n",
    "torch.save(diff_acts, output_filename)\n",
    "\n",
    "# tensor([23759, 42925, 33394, 45780, 10085, 23574, 30460, 61472,   521, 59020,\n",
    "#           775,  4261, 41205, 29588, 44488,   983, 55773, 44225, 42612, 30063],\n",
    "#        device='cuda:0')\n",
    "# tensor([-2.2821e-05, -1.8463e-05, -1.8400e-05,  1.6533e-05, -1.6091e-05,\n",
    "#          1.5563e-05,  1.5402e-05,  1.4945e-05, -1.4770e-05, -1.4396e-05,\n",
    "#         -1.4326e-05, -1.4277e-05, -1.3993e-05,  1.3824e-05, -1.3621e-05,\n",
    "#         -1.2731e-05,  1.2631e-05,  1.2598e-05, -1.2597e-05, -1.2533e-05],\n",
    "#        device='cuda:0')\n",
    "# diff_acts/v1_trainer_3_model_mistralai_Ministral-8B-Instruct-2410_layer_50_attrib_data.pt\n",
    "\n",
    "# tensor([24895, 37973, 29596, 35104, 19677, 64690, 23575, 14460, 54626, 60262,\n",
    "#         36264, 10894,  2577,  6381, 25218, 17486, 50206,  1279,  9861, 26352],\n",
    "#        device='cuda:0')\n",
    "# tensor([-0.0399, -0.0257,  0.0186, -0.0184,  0.0138,  0.0096,  0.0089, -0.0089,\n",
    "#         -0.0082, -0.0079, -0.0077,  0.0074,  0.0067, -0.0066, -0.0062, -0.0060,\n",
    "#          0.0056,  0.0050,  0.0046,  0.0046], device='cuda:0')\n",
    "# diff_acts/v17_trainer_2_model_mistralai_Mistral-Small-24B-Instruct-2501_layer_50_attrib_data.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:51<00:00,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label 1:\n",
      "Total samples: 98\n",
      "Yes rate: 70.4%\n",
      "No rate: 29.6%\n",
      "Invalid rate: 0.0%\n",
      "\n",
      "Label 0:\n",
      "Total samples: 98\n",
      "Yes rate: 73.5%\n",
      "No rate: 26.5%\n",
      "Invalid rate: 0.0%\n",
      "Peak CUDA memory usage: 67290.98 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the custom loss function\n",
    "yes_vs_no_loss_fn = attribution.make_yes_no_loss_fn(\n",
    "    tokenizer,\n",
    "    yes_candidates=[\"yes\", \" yes\", \"Yes\", \" Yes\", \"YES\", \" YES\"],\n",
    "    no_candidates=[\"no\", \" no\", \"No\", \" No\", \"NO\", \" NO\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "effects_F, error_effect, predicted_tokens = attribution.get_effects(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    sae,\n",
    "    dataloader,\n",
    "    yes_vs_no_loss_fn,\n",
    "    submodules,\n",
    "    chosen_layers,\n",
    "    device,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Print peak memory usage\n",
    "if torch.cuda.is_available():\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # Convert to MB\n",
    "    print(f\"Peak CUDA memory usage: {peak_memory:.2f} MB\")\n",
    "\n",
    "# Peak CUDA memory usage: 33432.72 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([33592,  3662, 47196, 52209, 18947,  9593, 64744, 50582, 57031,   574,\n",
      "        45795, 61625, 33006, 53980,  4975, 46136, 12936, 32874, 56610, 46957],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.1536, -0.1528, -0.1309,  0.1148,  0.1071,  0.1047, -0.0910, -0.0882,\n",
      "        -0.0862, -0.0824, -0.0781,  0.0728,  0.0669,  0.0657, -0.0587, -0.0573,\n",
      "         0.0556,  0.0549, -0.0548,  0.0543], device='cuda:0')\n",
      "tensor(0.0866, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "top_k_ids = effects_F.abs().topk(20).indices\n",
    "print(top_k_ids)\n",
    "\n",
    "top_k_vals = effects_F[top_k_ids]\n",
    "print(top_k_vals)\n",
    "\n",
    "print(error_effect)\n",
    "\n",
    "# tensor([ 4393, 15242,  9049,  3959, 11802, 14960,   428,  9920,  2715,  3509,\n",
    "#          9444, 12979,  9319,  8910, 12243,  7781, 11637, 10283,  4204,  2557],\n",
    "#        device='cuda:0')\n",
    "# tensor([0.0074, 0.0058, 0.0033, 0.0016, 0.0016, 0.0013, 0.0011, 0.0009, 0.0008,\n",
    "#         0.0008, 0.0008, 0.0008, 0.0007, 0.0007, 0.0007, 0.0007, 0.0006, 0.0006,\n",
    "#         0.0006, 0.0005], device='cuda:0')\n",
    "\n",
    "# tensor([ 4393, 15242,  9049, 13855, 11802,  3959,  2039, 14960,  1683,  3509,\n",
    "#           428,  4794,  3645,  9920,  5911,  1160,  1656, 16078,  9319,   394],\n",
    "#        device='cuda:0')\n",
    "# tensor([ 0.0305,  0.0242,  0.0133, -0.0078,  0.0064,  0.0063, -0.0058,  0.0052,\n",
    "#         -0.0051,  0.0050,  0.0045, -0.0043, -0.0042,  0.0040, -0.0039, -0.0034,\n",
    "#         -0.0033, -0.0033,  0.0033, -0.0031], device='cuda:0')\n",
    "# tensor(-0.0181, device='cuda:0')\n",
    "\n",
    "# tensor([ 4393, 15242,  9049, 13855, 11802,  3645,  2039,  3959,  1683,  3509,\n",
    "#         14960,  9920,  4794,  1656,  5911, 16078,  9319,  1160,  5286,   394],\n",
    "#        device='cuda:0')\n",
    "# tensor([ 0.0276,  0.0227,  0.0114, -0.0077,  0.0062, -0.0058, -0.0056,  0.0055,\n",
    "#         -0.0049,  0.0047,  0.0046,  0.0038, -0.0036, -0.0035, -0.0034, -0.0034,\n",
    "#          0.0033, -0.0032, -0.0030, -0.0029], device='cuda:0')\n",
    "# tensor(-0.0127, device='cuda:0')\n",
    "\n",
    "# tensor([15658, 54626, 17662, 37447, 26352, 13920, 47911,   413,   204, 39717,\n",
    "#         52330, 24031, 62241, 53133, 14038,  4682, 13032, 61127, 46104,  9405],\n",
    "#        device='cuda:0')\n",
    "# tensor([ 0.3070,  0.1780,  0.1460,  0.1291,  0.1048, -0.0993, -0.0823,  0.0779,\n",
    "#          0.0748,  0.0666,  0.0586,  0.0581, -0.0442, -0.0442,  0.0392,  0.0377,\n",
    "#         -0.0370,  0.0332, -0.0313, -0.0293], device='cuda:0')\n",
    "# tensor(-0.0644, device='cuda:0')\n",
    "\n",
    "# tensor([33592,  3662, 47196, 52209,  9593, 18947, 64744, 50582, 45795,   574,\n",
    "#         57031, 61625, 33006, 53980, 46136,  4975, 12936, 32874,  6954, 38436],\n",
    "#        device='cuda:0')\n",
    "# tensor([ 0.1563, -0.1505, -0.1254,  0.1122,  0.1059,  0.1016, -0.0913, -0.0874,\n",
    "#         -0.0841, -0.0828, -0.0805,  0.0714,  0.0673,  0.0635, -0.0592, -0.0583,\n",
    "#          0.0563,  0.0545, -0.0545, -0.0544], device='cuda:0')\n",
    "# tensor(0.0565, device='cuda:0')\n",
    "\n",
    "# downsample 10\n",
    "# tensor([63073,  3662, 45795, 33006, 58683, 50582, 45304, 42524, 33592, 27866,\n",
    "#         21772, 12936, 25266, 57031, 15390, 60822, 64744, 44535, 60211,   169],\n",
    "#        device='cuda:0')\n",
    "# tensor([-0.5895, -0.2480, -0.2339,  0.2267,  0.2038, -0.1890, -0.1808,  0.1536,\n",
    "#          0.1423, -0.1383, -0.1340,  0.1250, -0.1238, -0.1221,  0.1199,  0.1138,\n",
    "#         -0.1091, -0.1051,  0.1040,  0.0992], device='cuda:0')\n",
    "# tensor(0.6601, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{17161, 41619, 61599, 57264, 33592, 46652, 574, 24257, 53451, 43212, 50902, 39383, 44249, 46425, 47196, 49757, 6638, 29812, 40698, 61307}\n",
      "{18947, 12936, 50582, 56610, 33592, 61625, 46136, 574, 57031, 3662, 53980, 47196, 45795, 64744, 32874, 46957, 33006, 4975, 52209, 9593}\n",
      "{33592, 47196, 574}\n"
     ]
    }
   ],
   "source": [
    "set1 = set(acts_top_k_ids.cpu().tolist())\n",
    "set2 = set(top_k_ids.cpu().tolist())\n",
    "\n",
    "print(set1)\n",
    "print(set2)\n",
    "\n",
    "print(set1.intersection(set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65536\n"
     ]
    }
   ],
   "source": [
    "print(sae.W_dec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts_dir = \"max_acts\"\n",
    "acts_filename = f\"acts_{model_name}_layer_{chosen_layers[0]}_trainer_{trainer_id}_layer_percent_{chosen_layer_percentage[0]}.pt\".replace(\"/\", \"_\")\n",
    "if not os.path.exists(acts_filename):\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    path_to_config = hf_hub_download(\n",
    "        repo_id=\"adamkarvonen/sae_max_acts\",\n",
    "        filename=acts_filename,\n",
    "        force_download=False,\n",
    "        local_dir=acts_dir,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "\n",
    "    acts_path = os.path.join(acts_dir, acts_filename)\n",
    "    acts_data = torch.load(acts_path)\n",
    "    \n",
    "    # max_tokens, max_acts = interp_utils.get_interp_prompts(\n",
    "    #     model,\n",
    "    #     submodules[0],\n",
    "    #     sae,\n",
    "    #     torch.tensor(list(range(sae.W_dec.shape[0]))),\n",
    "    #     context_length=128,\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     batch_size=batch_size * 32,\n",
    "    #     num_tokens=30_000_000,\n",
    "    # )\n",
    "    # acts_data = {\n",
    "    #     \"max_tokens\": max_tokens,\n",
    "    #     \"max_acts\": max_acts,\n",
    "    # }\n",
    "    # torch.save(acts_data, acts_filename)\n",
    "else:\n",
    "    acts_path = os.path.join(acts_dir, acts_filename)\n",
    "    acts_data = torch.load(acts_path)\n",
    "max_tokens = acts_data[\"max_tokens\"].cpu()\n",
    "max_acts = acts_data[\"max_acts\"].cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0, feature idx: 33592, value: 0.15362504124641418\n",
      "Feature 1, feature idx: 3662, value: -0.15279383957386017\n"
     ]
    }
   ],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "import gc\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "def _list_decode(x):\n",
    "    if len(x.shape) == 0:\n",
    "        return tokenizer.decode(x, skip_special_tokens=False)\n",
    "    else:\n",
    "        return [_list_decode(y) for y in x]\n",
    "\n",
    "\n",
    "def display_html_activations(\n",
    "    selected_tokens_FKL: list[str],\n",
    "    selected_activations_FKL: list[torch.Tensor],\n",
    "    num_display: int = 10,\n",
    "    k: int = 5,\n",
    "):\n",
    "\n",
    "    all_html_activations = []\n",
    "\n",
    "    for i in range(num_display):\n",
    "\n",
    "        selected_activations_KL11 = [\n",
    "            selected_activations_FKL[i, k, :, None, None] for k in range(k)\n",
    "        ]\n",
    "        selected_tokens_KL = selected_tokens_FKL[i]\n",
    "        selected_token_strs_KL = _list_decode(selected_tokens_KL)\n",
    "\n",
    "        for k in range(len(selected_token_strs_KL)):\n",
    "            if (\n",
    "                \"<s>\" in selected_token_strs_KL[k][0]\n",
    "                or \"<bos>\" in selected_token_strs_KL[k][0]\n",
    "            ):\n",
    "                selected_token_strs_KL[k][0] = \"BOS>\"\n",
    "\n",
    "        # selected_token_strs_KL = tokenizer.batch_decode(selected_token_KL, skip_special_tokens=False)\n",
    "        # for k in range(len(selected_token_strs_KL)):\n",
    "        #     string = selected_token_strs_KL[k]\n",
    "        #     print(string[:10])\n",
    "        # print(\"\".join(string))\n",
    "\n",
    "        html_activations = text_neuron_activations(\n",
    "            selected_token_strs_KL, selected_activations_KL11\n",
    "        )\n",
    "\n",
    "        all_html_activations.append(html_activations)\n",
    "    \n",
    "    return all_html_activations\n",
    "\n",
    "clear_output(wait=True)\n",
    "gc.collect()\n",
    "html_activations = display_html_activations(max_tokens[top_k_ids.cpu()], max_acts[top_k_ids.cpu()], num_display=2)\n",
    "for i, html_activation in enumerate(html_activations):\n",
    "    feature_idx = top_k_ids[i]\n",
    "    feature_val = top_k_vals[i]\n",
    "    print(f\"Feature {i}, feature idx: {feature_idx}, value: {feature_val}\")\n",
    "    # display(html_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled to 3 unique resumes\n",
      "Total samples after maintaining demographic variations: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 2/2 [00:00<00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 12, 2357])\n",
      "torch.Size([20, 12, 2357])\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "tensor([False,  True, False,  True, False,  True, False,  True, False,  True,\n",
      "        False,  True])\n",
      "tensor([ True, False,  True, False,  True, False,  True, False,  True, False,\n",
      "         True, False])\n",
      "torch.Size([20, 12, 2357])\n",
      "tensor([0.0022, 0.0047, 0.3281, 0.0115, 0.0154, 0.0039, 0.0019, 0.0026, 0.0330,\n",
      "        0.0334, 0.0786, 0.0038, 0.0183, 0.0034, 0.0017, 0.0151, 0.0038, 0.0029,\n",
      "        0.0040, 0.0195], dtype=torch.bfloat16)\n",
      "tensor([    0.0003,     0.0047,     0.3301,     0.0117,     0.0154,     0.0040,\n",
      "            0.0018,     0.0025,     0.0327,     0.0330,     0.0786,     0.0038,\n",
      "            0.0183,     0.0035,     0.0014,     0.0151,     0.0034,     0.0032,\n",
      "            0.0040,     0.0195], dtype=torch.bfloat16)\n",
      "tensor([6.8125, 0.9883, 0.9922, 0.9805, 0.9961, 0.9922, 1.0312, 1.0234, 1.0078,\n",
      "        1.0156, 1.0000, 0.9883, 1.0000, 0.9883, 1.2266, 1.0078, 1.1172, 0.9023,\n",
      "        0.9922, 1.0000], dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "inspection_eval_config = deepcopy(eval_config)\n",
    "inspection_eval_config.downsample = 3\n",
    "\n",
    "inspect_df = dataset_setup.balanced_downsample(df, inspection_eval_config.downsample, random_seed)\n",
    "\n",
    "inspect_prompts = hiring_bias_prompts.create_all_prompts_hiring_bias(inspect_df, args, inspection_eval_config)\n",
    "\n",
    "inspect_texts, inspect_labels = hiring_bias_prompts.process_hiring_bias_resumes_prompts(inspect_prompts, args)\n",
    "\n",
    "inspect_texts = model_utils.add_chat_template(inspect_texts, model_name)\n",
    "\n",
    "inspect_tokens_FBL, inspect_activations_FBL = interp_utils.get_interp_prompts_user_inputs(\n",
    "    model,\n",
    "    submodules[0],\n",
    "    sae,\n",
    "    top_k_ids,\n",
    "    inspect_texts,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size * 6,\n",
    "    k=len(inspect_texts),\n",
    "    sort_by_activation=False,\n",
    ")\n",
    "\n",
    "print(inspect_tokens_FBL.shape)\n",
    "print(inspect_activations_FBL.shape)\n",
    "print(inspect_labels)\n",
    "\n",
    "labels_tensor = torch.tensor(inspect_labels)\n",
    "\n",
    "pos_mask_K = labels_tensor == 1\n",
    "neg_mask_K = labels_tensor == 0\n",
    "\n",
    "print(pos_mask_K)\n",
    "print(neg_mask_K)\n",
    "\n",
    "print(inspect_activations_FBL.shape)\n",
    "\n",
    "pos_acts_FKL = inspect_activations_FBL[:, pos_mask_K, :]\n",
    "neg_acts_FKL = inspect_activations_FBL[:, neg_mask_K, :]\n",
    "\n",
    "mean_pos_acts_F = pos_acts_FKL.mean(dim=(1,2))\n",
    "mean_neg_acts_F = neg_acts_FKL.mean(dim=(1,2))\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "\n",
    "print(mean_pos_acts_F)\n",
    "print(mean_neg_acts_F)\n",
    "\n",
    "ratios = mean_pos_acts_F / mean_neg_acts_F\n",
    "\n",
    "print(ratios)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "\n",
    "# inspection_eval_config = deepcopy(eval_config)\n",
    "# inspection_eval_config.downsample = 3\n",
    "\n",
    "# inspect_df = dataset_setup.balanced_downsample(df, inspection_eval_config.downsample, random_seed)\n",
    "\n",
    "# inspect_prompts = hiring_bias_prompts.create_all_prompts_hiring_bias(inspect_df, args, inspection_eval_config)\n",
    "\n",
    "# inspect_texts, inspect_labels = hiring_bias_prompts.process_hiring_bias_resumes_prompts(inspect_prompts, args)\n",
    "\n",
    "# inspect_texts = model_utils.add_chat_template(inspect_texts, model_name)\n",
    "\n",
    "# inspect_tokens_FBL, inspect_activations_FBL = interp_utils.get_interp_prompts_user_inputs(\n",
    "#     model,\n",
    "#     submodules[0],\n",
    "#     sae,\n",
    "#     top_k_ids,\n",
    "#     inspect_texts,\n",
    "#     tokenizer=tokenizer,\n",
    "#     batch_size=batch_size * 6,\n",
    "#     k=len(inspect_texts),\n",
    "# )\n",
    "\n",
    "# num_display = 2\n",
    "\n",
    "# html_activations = display_html_activations(inspect_tokens_FBL, inspect_activations_FBL, num_display=num_display, k=len(inspect_texts))\n",
    "\n",
    "# for i, html_activation in enumerate(html_activations):\n",
    "#     feature_idx = top_k_ids[i]\n",
    "#     feature_val = top_k_vals[i]\n",
    "#     print(f\"Feature {i}, feature idx: {feature_idx}, value: {feature_val}\")\n",
    "#     display(html_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
