{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/whitebox_evals/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import einops\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Callable, Optional\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import mypkg.whitebox_infra.attribution as attribution\n",
    "import mypkg.whitebox_infra.dictionaries.batch_topk_sae as batch_topk_sae\n",
    "import mypkg.whitebox_infra.data_utils as data_utils\n",
    "import mypkg.whitebox_infra.model_utils as model_utils\n",
    "import mypkg.whitebox_infra.interp_utils as interp_utils\n",
    "import mypkg.pipeline.setup.dataset as dataset_setup\n",
    "import mypkg.pipeline.infra.hiring_bias_prompts as hiring_bias_prompts\n",
    "from mypkg.eval_config import EvalConfig\n",
    "import mypkg.pipeline.infra.model_inference as model_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "model_name = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "# model_name = \"google/gemma-2-27b-it\"\n",
    "\n",
    "bias_type = \"gender\"\n",
    "bias_type = \"race\"\n",
    "# bias_type = \"political_orientation\"\n",
    "\n",
    "anti_bias_statement_file = \"v1.txt\"\n",
    "anti_bias_statement_file = \"v3.txt\"\n",
    "# anti_bias_statement_file = \"v17.txt\"\n",
    "\n",
    "args = hiring_bias_prompts.HiringBiasArgs(\n",
    "    political_orientation=bias_type == \"political_orientation\",\n",
    "    employment_gap=bias_type == \"employment_gap\",\n",
    "    pregnancy=bias_type == \"pregnancy\",\n",
    "    race=bias_type == \"race\",\n",
    "    gender=bias_type == \"gender\",\n",
    "    misc=bias_type == \"misc\",\n",
    ")\n",
    "\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=dtype, device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "gradient_checkpointing = False\n",
    "\n",
    "if model_name == \"google/gemma-2-27b-it\":\n",
    "    gradient_checkpointing = True\n",
    "    batch_size = 1\n",
    "elif model_name == \"mistralai/Mistral-Small-24B-Instruct-2501\" or model_name == \"google/gemma-2-2b-it\":\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 3\n",
    "\n",
    "if gradient_checkpointing:\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "chosen_layer_percentage = [25]\n",
    "# chosen_layer_percentage = [50]\n",
    "\n",
    "system_prompt = \"yes_no.txt\"\n",
    "\n",
    "chosen_layers = []\n",
    "for layer_percent in chosen_layer_percentage:\n",
    "    chosen_layers.append(model_utils.MODEL_CONFIGS[model_name][\"layer_mappings\"][layer_percent][\"layer\"])\n",
    "\n",
    "eval_config = EvalConfig(\n",
    "        model_name=model_name,\n",
    "        political_orientation=True,\n",
    "        pregnancy=False,\n",
    "        employment_gap=False,\n",
    "        anthropic_dataset=False,\n",
    "        downsample=50,\n",
    "        # downsample=10,\n",
    "        gpu_inference=True,\n",
    "        anti_bias_statement_file=anti_bias_statement_file,\n",
    "        job_description_file=\"short_meta_job_description.txt\",\n",
    "        system_prompt_filename=system_prompt,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sae_repo = \"adamkarvonen/ministral_saes\"\n",
    "# sae_path = f\"mistralai_Ministral-8B-Instruct-2410_batch_top_k/resid_post_layer_{chosen_layers[0]}/trainer_1/ae.pt\"\n",
    "\n",
    "# sae = batch_topk_sae.load_dictionary_learning_batch_topk_sae(\n",
    "#     repo_id=sae_repo,\n",
    "#     filename=sae_path,\n",
    "#     model_name=model_name,\n",
    "#     device=device,\n",
    "#     dtype=dtype,\n",
    "#     layer=chosen_layers[0],\n",
    "#     local_dir=\"downloaded_saes\",\n",
    "# )\n",
    "trainer_id = 2\n",
    "\n",
    "if \"gemma\" in model_name:\n",
    "    trainer_id = 0\n",
    "    \n",
    "sae = model_utils.load_model_sae(model_name, device, dtype, chosen_layer_percentage[0], trainer_id=trainer_id)\n",
    "\n",
    "submodules = [model_utils.get_submodule(model, chosen_layers[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled to 50 unique resumes\n",
      "Total samples after maintaining demographic variations: 200\n"
     ]
    }
   ],
   "source": [
    "df = dataset_setup.load_raw_dataset()\n",
    "\n",
    "industry = \"INFORMATION-TECHNOLOGY\"\n",
    "downsample = eval_config.downsample\n",
    "random_seed = eval_config.random_seed\n",
    "\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "df = dataset_setup.filter_by_industry(df, industry)\n",
    "\n",
    "df = dataset_setup.balanced_downsample(df, downsample, random_seed)\n",
    "\n",
    "\n",
    "\n",
    "prompts = hiring_bias_prompts.create_all_prompts_hiring_bias(df, args, eval_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 4 samples that exceeded max_length (2500)\n",
      "Original dataset size: 200, new size: 196\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_labels, train_prompt_details = hiring_bias_prompts.process_hiring_bias_resumes_prompts(prompts, args)\n",
    "\n",
    "train_texts = model_utils.add_chat_template(train_texts, model_name)\n",
    "\n",
    "dataloader = data_utils.create_simple_dataloader(\n",
    "    train_texts, train_labels, model_name, device, batch_size=batch_size, max_length=2500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_answers = model_inference.run_single_forward_pass_transformers(\n",
    "#     prompts, model_name, batch_size=batch_size * 6, model=model\n",
    "# )\n",
    "\n",
    "# bias_scores = hiring_bias_prompts.evaluate_bias(\n",
    "#     model_answers,\n",
    "#     system_prompt\n",
    "# )\n",
    "# print(bias_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:12<00:00, 15.14it/s]\n"
     ]
    }
   ],
   "source": [
    "diff_acts_F = attribution.get_activations(model, sae, dataloader, submodules, chosen_layers, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3513,  2841,   300,  4559, 15884,   461,  6517,  5023,   384,  5464,\n",
      "         7549,  4255, 14280, 16038, 13324, 13715, 11782,  7574,  3844, 14850],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.0163,  0.0116,  0.0108, -0.0083, -0.0082, -0.0075,  0.0067,  0.0065,\n",
      "        -0.0064,  0.0064,  0.0062,  0.0056,  0.0056,  0.0055,  0.0054, -0.0052,\n",
      "        -0.0049, -0.0048,  0.0048, -0.0047], device='cuda:0')\n",
      "diff_acts/v3_trainer_0_model_google_gemma-2-2b-it_layer_25_attrib_data.pt\n"
     ]
    }
   ],
   "source": [
    "acts_top_k_ids = diff_acts_F.abs().topk(20).indices\n",
    "print(acts_top_k_ids)\n",
    "\n",
    "acts_top_k_vals = diff_acts_F[acts_top_k_ids]\n",
    "print(acts_top_k_vals)\n",
    "\n",
    "output_filename = os.path.join(\n",
    "    \"diff_acts\",\n",
    "    f\"{eval_config.anti_bias_statement_file.replace('.txt', '')}_trainer_{trainer_id}_model_{model_name.replace('/', '_')}_layer_{chosen_layer_percentage[0]}_attrib_data.pt\",\n",
    ")\n",
    "print(output_filename)\n",
    "\n",
    "os.makedirs(\"diff_acts\", exist_ok=True)\n",
    "\n",
    "diff_acts = {\"diff_acts_F\": diff_acts_F}\n",
    "diff_acts[\"config\"] = {\n",
    "        \"model_name\": model_name,\n",
    "        \"layer\": chosen_layers[0],\n",
    "        \"bias_categories\": \"N\\A\",\n",
    "        \"anti_bias_statement_file\": eval_config.anti_bias_statement_file,\n",
    "        \"downsample\": eval_config.downsample,\n",
    "        \"random_seed\": eval_config.random_seed,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"chosen_layer_percentage\": chosen_layer_percentage,\n",
    "        \"trainer_id\": trainer_id,\n",
    "    }\n",
    "\n",
    "torch.save(diff_acts, output_filename)\n",
    "\n",
    "# tensor([23759, 42925, 33394, 45780, 10085, 23574, 30460, 61472,   521, 59020,\n",
    "#           775,  4261, 41205, 29588, 44488,   983, 55773, 44225, 42612, 30063],\n",
    "#        device='cuda:0')\n",
    "# tensor([-2.2821e-05, -1.8463e-05, -1.8400e-05,  1.6533e-05, -1.6091e-05,\n",
    "#          1.5563e-05,  1.5402e-05,  1.4945e-05, -1.4770e-05, -1.4396e-05,\n",
    "#         -1.4326e-05, -1.4277e-05, -1.3993e-05,  1.3824e-05, -1.3621e-05,\n",
    "#         -1.2731e-05,  1.2631e-05,  1.2598e-05, -1.2597e-05, -1.2533e-05],\n",
    "#        device='cuda:0')\n",
    "# diff_acts/v1_trainer_3_model_mistralai_Ministral-8B-Instruct-2410_layer_50_attrib_data.pt\n",
    "\n",
    "# tensor([24895, 37973, 29596, 35104, 19677, 64690, 23575, 14460, 54626, 60262,\n",
    "#         36264, 10894,  2577,  6381, 25218, 17486, 50206,  1279,  9861, 26352],\n",
    "#        device='cuda:0')\n",
    "# tensor([-0.0399, -0.0257,  0.0186, -0.0184,  0.0138,  0.0096,  0.0089, -0.0089,\n",
    "#         -0.0082, -0.0079, -0.0077,  0.0074,  0.0067, -0.0066, -0.0062, -0.0060,\n",
    "#          0.0056,  0.0050,  0.0046,  0.0046], device='cuda:0')\n",
    "# diff_acts/v17_trainer_2_model_mistralai_Mistral-Small-24B-Instruct-2501_layer_50_attrib_data.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [01:29<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label 1:\n",
      "Total samples: 98\n",
      "Yes rate: 67.3%\n",
      "No rate: 32.7%\n",
      "Invalid rate: 0.0%\n",
      "\n",
      "Label 0:\n",
      "Total samples: 98\n",
      "Yes rate: 66.3%\n",
      "No rate: 33.7%\n",
      "Invalid rate: 0.0%\n",
      "Peak CUDA memory usage: 15365.44 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the custom loss function\n",
    "yes_vs_no_loss_fn = attribution.make_yes_no_loss_fn(\n",
    "    tokenizer,\n",
    "    yes_candidates=[\"yes\", \" yes\", \"Yes\", \" Yes\", \"YES\", \" YES\"],\n",
    "    no_candidates=[\"no\", \" no\", \"No\", \" No\", \"NO\", \" NO\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "effects_F, error_effect, predicted_tokens = attribution.get_effects(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    sae,\n",
    "    dataloader,\n",
    "    yes_vs_no_loss_fn,\n",
    "    submodules,\n",
    "    chosen_layers,\n",
    "    device,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Print peak memory usage\n",
    "if torch.cuda.is_available():\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # Convert to MB\n",
    "    print(f\"Peak CUDA memory usage: {peak_memory:.2f} MB\")\n",
    "\n",
    "# Peak CUDA memory usage: 33432.72 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2870,  4048, 13789,  3513, 15263,   949, 15216,  2908,  6482,  4235,\n",
      "        14392, 14217, 10273,  3235,  2841,  6652, 10847, 12553,  3587,  5859],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.6280, -1.8414,  1.4296,  0.6981,  0.6451, -0.5354, -0.5200, -0.5059,\n",
      "        -0.5012, -0.4925, -0.4562, -0.4433,  0.4365, -0.4199,  0.4192,  0.4149,\n",
      "        -0.3746, -0.3451, -0.3224, -0.3199], device='cuda:0')\n",
      "tensor(0.8407, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "top_k_ids = effects_F.abs().topk(20).indices\n",
    "print(top_k_ids)\n",
    "\n",
    "top_k_vals = effects_F[top_k_ids]\n",
    "print(top_k_vals)\n",
    "\n",
    "print(error_effect)\n",
    "\n",
    "# tensor([ 4393, 15242,  9049,  3959, 11802, 14960,   428,  9920,  2715,  3509,\n",
    "#          9444, 12979,  9319,  8910, 12243,  7781, 11637, 10283,  4204,  2557],\n",
    "#        device='cuda:0')\n",
    "# tensor([0.0074, 0.0058, 0.0033, 0.0016, 0.0016, 0.0013, 0.0011, 0.0009, 0.0008,\n",
    "#         0.0008, 0.0008, 0.0008, 0.0007, 0.0007, 0.0007, 0.0007, 0.0006, 0.0006,\n",
    "#         0.0006, 0.0005], device='cuda:0')\n",
    "\n",
    "# tensor([ 4393, 15242,  9049, 13855, 11802,  3959,  2039, 14960,  1683,  3509,\n",
    "#           428,  4794,  3645,  9920,  5911,  1160,  1656, 16078,  9319,   394],\n",
    "#        device='cuda:0')\n",
    "# tensor([ 0.0305,  0.0242,  0.0133, -0.0078,  0.0064,  0.0063, -0.0058,  0.0052,\n",
    "#         -0.0051,  0.0050,  0.0045, -0.0043, -0.0042,  0.0040, -0.0039, -0.0034,\n",
    "#         -0.0033, -0.0033,  0.0033, -0.0031], device='cuda:0')\n",
    "# tensor(-0.0181, device='cuda:0')\n",
    "\n",
    "# tensor([ 4393, 15242,  9049, 13855, 11802,  3645,  2039,  3959,  1683,  3509,\n",
    "#         14960,  9920,  4794,  1656,  5911, 16078,  9319,  1160,  5286,   394],\n",
    "#        device='cuda:0')\n",
    "# tensor([ 0.0276,  0.0227,  0.0114, -0.0077,  0.0062, -0.0058, -0.0056,  0.0055,\n",
    "#         -0.0049,  0.0047,  0.0046,  0.0038, -0.0036, -0.0035, -0.0034, -0.0034,\n",
    "#          0.0033, -0.0032, -0.0030, -0.0029], device='cuda:0')\n",
    "# tensor(-0.0127, device='cuda:0')\n",
    "\n",
    "# tensor([15658, 54626, 17662, 37447, 26352, 13920, 47911,   413,   204, 39717,\n",
    "#         52330, 24031, 62241, 53133, 14038,  4682, 13032, 61127, 46104,  9405],\n",
    "#        device='cuda:0')\n",
    "# tensor([ 0.3070,  0.1780,  0.1460,  0.1291,  0.1048, -0.0993, -0.0823,  0.0779,\n",
    "#          0.0748,  0.0666,  0.0586,  0.0581, -0.0442, -0.0442,  0.0392,  0.0377,\n",
    "#         -0.0370,  0.0332, -0.0313, -0.0293], device='cuda:0')\n",
    "# tensor(-0.0644, device='cuda:0')\n",
    "\n",
    "# tensor([33592,  3662, 47196, 52209,  9593, 18947, 64744, 50582, 45795,   574,\n",
    "#         57031, 61625, 33006, 53980, 46136,  4975, 12936, 32874,  6954, 38436],\n",
    "#        device='cuda:0')\n",
    "# tensor([ 0.1563, -0.1505, -0.1254,  0.1122,  0.1059,  0.1016, -0.0913, -0.0874,\n",
    "#         -0.0841, -0.0828, -0.0805,  0.0714,  0.0673,  0.0635, -0.0592, -0.0583,\n",
    "#          0.0563,  0.0545, -0.0545, -0.0544], device='cuda:0')\n",
    "# tensor(0.0565, device='cuda:0')\n",
    "\n",
    "# downsample 10\n",
    "# tensor([63073,  3662, 45795, 33006, 58683, 50582, 45304, 42524, 33592, 27866,\n",
    "#         21772, 12936, 25266, 57031, 15390, 60822, 64744, 44535, 60211,   169],\n",
    "#        device='cuda:0')\n",
    "# tensor([-0.5895, -0.2480, -0.2339,  0.2267,  0.2038, -0.1890, -0.1808,  0.1536,\n",
    "#          0.1423, -0.1383, -0.1340,  0.1250, -0.1238, -0.1221,  0.1199,  0.1138,\n",
    "#         -0.1091, -0.1051,  0.1040,  0.0992], device='cuda:0')\n",
    "# tensor(0.6601, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{384, 14850, 3844, 11782, 15884, 13324, 13715, 7574, 2841, 4255, 5023, 16038, 300, 3513, 14280, 461, 4559, 5464, 6517, 7549}\n",
      "{3587, 14217, 12553, 4235, 2841, 15263, 10273, 3235, 949, 2870, 14392, 3513, 4048, 6482, 2908, 13789, 10847, 5859, 15216, 6652}\n",
      "{2841, 3513}\n"
     ]
    }
   ],
   "source": [
    "set1 = set(acts_top_k_ids.cpu().tolist())\n",
    "set2 = set(top_k_ids.cpu().tolist())\n",
    "\n",
    "print(set1)\n",
    "print(set2)\n",
    "\n",
    "print(set1.intersection(set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384\n"
     ]
    }
   ],
   "source": [
    "print(sae.W_dec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acts_google_gemma-2-2b-it_layer_5_trainer_0_layer_percent_25.pt\n"
     ]
    }
   ],
   "source": [
    "acts_dir = \"max_acts\"\n",
    "acts_filename = f\"acts_{model_name}_layer_{chosen_layers[0]}_trainer_{trainer_id}_layer_percent_{chosen_layer_percentage[0]}.pt\".replace(\"/\", \"_\")\n",
    "print(acts_filename)\n",
    "acts_path = os.path.join(acts_dir, acts_filename)\n",
    "if not os.path.exists(acts_path):\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    path_to_config = hf_hub_download(\n",
    "        repo_id=\"adamkarvonen/sae_max_acts\",\n",
    "        filename=acts_filename,\n",
    "        force_download=False,\n",
    "        local_dir=acts_dir,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    acts_data = torch.load(acts_path)\n",
    "    \n",
    "    # max_tokens, max_acts = interp_utils.get_interp_prompts(\n",
    "    #     model,\n",
    "    #     submodules[0],\n",
    "    #     sae,\n",
    "    #     torch.tensor(list(range(sae.W_dec.shape[0]))),\n",
    "    #     context_length=128,\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     batch_size=batch_size * 32,\n",
    "    #     num_tokens=30_000_000,\n",
    "    # )\n",
    "    # acts_data = {\n",
    "    #     \"max_tokens\": max_tokens,\n",
    "    #     \"max_acts\": max_acts,\n",
    "    # }\n",
    "    # torch.save(acts_data, acts_filename)\n",
    "else:\n",
    "    acts_data = torch.load(acts_path)\n",
    "max_tokens = acts_data[\"max_tokens\"].cpu()\n",
    "max_acts = acts_data[\"max_acts\"].cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0, feature idx: 2870, value: 3.62801194190979\n",
      "Feature 1, feature idx: 4048, value: -1.8413792848587036\n",
      "Feature 2, feature idx: 13789, value: 1.429613471031189\n",
      "Feature 3, feature idx: 3513, value: 0.6981467008590698\n",
      "Feature 4, feature idx: 15263, value: 0.6450632214546204\n",
      "Feature 5, feature idx: 949, value: -0.5353862643241882\n",
      "Feature 6, feature idx: 15216, value: -0.5199776887893677\n",
      "Feature 7, feature idx: 2908, value: -0.5058706402778625\n",
      "Feature 8, feature idx: 6482, value: -0.5011627078056335\n",
      "Feature 9, feature idx: 4235, value: -0.4924779534339905\n",
      "Feature 10, feature idx: 14392, value: -0.45618849992752075\n",
      "Feature 11, feature idx: 14217, value: -0.44330069422721863\n",
      "Feature 12, feature idx: 10273, value: 0.43647849559783936\n",
      "Feature 13, feature idx: 3235, value: -0.4198872148990631\n",
      "Feature 14, feature idx: 2841, value: 0.4192456603050232\n",
      "Feature 15, feature idx: 6652, value: 0.4148939251899719\n",
      "Feature 16, feature idx: 10847, value: -0.3745908737182617\n",
      "Feature 17, feature idx: 12553, value: -0.3451492190361023\n",
      "Feature 18, feature idx: 3587, value: -0.3223767578601837\n",
      "Feature 19, feature idx: 5859, value: -0.3198758363723755\n"
     ]
    }
   ],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "import gc\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "# def _list_decode(x):\n",
    "#     if len(x.shape) == 0:\n",
    "#         return tokenizer.decode(x, skip_special_tokens=False)\n",
    "#     else:\n",
    "#         return [_list_decode(y) for y in x]\n",
    "\n",
    "def _list_decode(x: torch.Tensor):\n",
    "    assert len(x.shape) == 1 or len(x.shape) == 2\n",
    "    # Convert to list of lists, even if x is 1D\n",
    "    if len(x.shape) == 1:\n",
    "        x = x.unsqueeze(0)  # Make it 2D for consistent handling\n",
    "\n",
    "    # Convert tensor to list of list of ints\n",
    "    token_ids = x.tolist()\n",
    "    \n",
    "    # Convert token ids to token strings\n",
    "    return [tokenizer.batch_decode(seq, skip_special_tokens=False) for seq in token_ids]\n",
    "\n",
    "\n",
    "def create_html_activations(\n",
    "    selected_tokens_FKL: list[str],\n",
    "    selected_activations_FKL: list[torch.Tensor],\n",
    "    num_display: int = 10,\n",
    "    k: int = 5,\n",
    "):\n",
    "\n",
    "    all_html_activations = []\n",
    "\n",
    "    for i in range(num_display):\n",
    "\n",
    "        selected_activations_KL11 = [\n",
    "            selected_activations_FKL[i, k, :, None, None] for k in range(k)\n",
    "        ]\n",
    "        selected_tokens_KL = selected_tokens_FKL[i]\n",
    "        selected_token_strs_KL = _list_decode(selected_tokens_KL)\n",
    "\n",
    "        # print(selected_token_strs_KL)\n",
    "\n",
    "        for k in range(len(selected_token_strs_KL)):\n",
    "            if (\n",
    "                \"<s>\" in selected_token_strs_KL[k][0]\n",
    "                or \"<bos>\" in selected_token_strs_KL[k][0]\n",
    "            ):\n",
    "                selected_token_strs_KL[k][0] = \"BOS>\"\n",
    "\n",
    "        # selected_token_strs_KL = tokenizer.batch_decode(selected_token_KL, skip_special_tokens=False)\n",
    "        # for k in range(len(selected_token_strs_KL)):\n",
    "        #     string = selected_token_strs_KL[k]\n",
    "        #     print(string[:10])\n",
    "        # print(\"\".join(string))\n",
    "\n",
    "        html_activations = text_neuron_activations(\n",
    "            selected_token_strs_KL, selected_activations_KL11\n",
    "        )\n",
    "\n",
    "        all_html_activations.append(html_activations)\n",
    "    \n",
    "    return all_html_activations\n",
    "\n",
    "clear_output(wait=True)\n",
    "gc.collect()\n",
    "html_activations = create_html_activations(max_tokens[top_k_ids.cpu()], max_acts[top_k_ids.cpu()], num_display=20)\n",
    "\n",
    "with open(\"autointerp_html_activations.pkl\", \"wb\") as f:\n",
    "    pickle.dump(html_activations, f)\n",
    "\n",
    "for i, html_activation in enumerate(html_activations):\n",
    "    feature_idx = top_k_ids[i]\n",
    "    feature_val = top_k_vals[i]\n",
    "    print(f\"Feature {i}, feature idx: {feature_idx}, value: {feature_val}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "\n",
    "# random.seed(random_seed)\n",
    "# torch.manual_seed(random_seed)\n",
    "\n",
    "# inspection_eval_config = deepcopy(eval_config)\n",
    "# inspection_eval_config.downsample = 3\n",
    "\n",
    "# inspect_df = dataset_setup.balanced_downsample(df, inspection_eval_config.downsample, random_seed)\n",
    "\n",
    "# inspect_prompts = hiring_bias_prompts.create_all_prompts_hiring_bias(inspect_df, args, inspection_eval_config)\n",
    "\n",
    "# inspect_texts, inspect_labels = hiring_bias_prompts.process_hiring_bias_resumes_prompts(inspect_prompts, args)\n",
    "\n",
    "# inspect_texts = model_utils.add_chat_template(inspect_texts, model_name)\n",
    "\n",
    "# inspect_tokens_FBL, inspect_activations_FBL = interp_utils.get_interp_prompts_user_inputs(\n",
    "#     model,\n",
    "#     submodules[0],\n",
    "#     sae,\n",
    "#     top_k_ids,\n",
    "#     inspect_texts,\n",
    "#     tokenizer=tokenizer,\n",
    "#     batch_size=batch_size * 6,\n",
    "#     k=len(inspect_texts),\n",
    "#     sort_by_activation=False,\n",
    "# )\n",
    "\n",
    "# print(inspect_tokens_FBL.shape)\n",
    "# print(inspect_activations_FBL.shape)\n",
    "# print(inspect_labels)\n",
    "\n",
    "# labels_tensor = torch.tensor(inspect_labels)\n",
    "\n",
    "# pos_mask_K = labels_tensor == 1\n",
    "# neg_mask_K = labels_tensor == 0\n",
    "\n",
    "# print(pos_mask_K)\n",
    "# print(neg_mask_K)\n",
    "\n",
    "# print(inspect_activations_FBL.shape)\n",
    "\n",
    "# pos_acts_FKL = inspect_activations_FBL[:, pos_mask_K, :]\n",
    "# neg_acts_FKL = inspect_activations_FBL[:, neg_mask_K, :]\n",
    "\n",
    "# mean_pos_acts_F = pos_acts_FKL.mean(dim=(1,2))\n",
    "# mean_neg_acts_F = neg_acts_FKL.mean(dim=(1,2))\n",
    "\n",
    "# torch.set_printoptions(precision=4, sci_mode=False)\n",
    "\n",
    "# print(mean_pos_acts_F)\n",
    "# print(mean_neg_acts_F)\n",
    "\n",
    "# ratios = mean_pos_acts_F / mean_neg_acts_F\n",
    "\n",
    "# print(ratios)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled to 10 unique resumes\n",
      "Total samples after maintaining demographic variations: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:18<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_attrib_acts(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    sae: batch_topk_sae.BatchTopKSAE,\n",
    "    top_k_ids: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    device: torch.device,\n",
    "    chosen_layers: list[int],\n",
    "    submodules: list[torch.nn.Module],\n",
    "    yes_vs_no_loss_fn: Callable,\n",
    "    downsample: int\n",
    ") -> list[dict]:\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    inspection_eval_config = deepcopy(eval_config)\n",
    "    inspection_eval_config.downsample = downsample\n",
    "    inspection_eval_config.anti_bias_statement_file = anti_bias_statement_file\n",
    "    # inspection_eval_config.anti_bias_statement_file = \"v1.txt\"\n",
    "    # inspection_eval_config.anti_bias_statement_file = \"v17.txt\"\n",
    "    # inspection_eval_config.anti_bias_statement_file = \"v6.txt\"\n",
    "\n",
    "    inspect_df = dataset_setup.balanced_downsample(\n",
    "        df, inspection_eval_config.downsample, random_seed\n",
    "    )\n",
    "\n",
    "    inspect_prompts = hiring_bias_prompts.create_all_prompts_hiring_bias(\n",
    "        inspect_df, args, inspection_eval_config\n",
    "    )\n",
    "\n",
    "    inspect_texts, inspect_labels = (\n",
    "        hiring_bias_prompts.process_hiring_bias_resumes_prompts(inspect_prompts, args)\n",
    "    )\n",
    "\n",
    "    inspect_texts = model_utils.add_chat_template(inspect_texts, model_name)\n",
    "\n",
    "    inspect_dataloader = data_utils.create_simple_dataloader(\n",
    "        inspect_texts,\n",
    "        inspect_labels,\n",
    "        model_name,\n",
    "        device=device,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    all_batch_results = []\n",
    "\n",
    "    for batch in tqdm(inspect_dataloader):\n",
    "        input_ids, attention_mask, labels, idx_batch = batch\n",
    "\n",
    "        model_inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "        batch_results = attribution.compute_attributions(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            sae,\n",
    "            model_inputs,\n",
    "            labels,\n",
    "            chosen_layers,\n",
    "            submodules,\n",
    "            yes_vs_no_loss_fn,\n",
    "        )\n",
    "        batch_results = batch_results[chosen_layers[0]]\n",
    "\n",
    "        batch_results[\"encoded_acts_BLF\"] = batch_results[\"encoded_acts_BLF\"][\n",
    "            :, :, top_k_ids\n",
    "        ]\n",
    "        batch_results[\"effects_BLF\"] = batch_results[\"effects_BLF\"][:, :, top_k_ids]\n",
    "        batch_results[\"grad_x_dot_decoder_BLF\"] = batch_results[\n",
    "            \"grad_x_dot_decoder_BLF\"\n",
    "        ][:, :, top_k_ids]\n",
    "        all_batch_results.append(batch_results)\n",
    "    return all_batch_results\n",
    "\n",
    "all_attrib_batch_results = get_attrib_acts(model, tokenizer, sae, top_k_ids, batch_size, device, chosen_layers, submodules, yes_vs_no_loss_fn, downsample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attrib_html_activations = []\n",
    "\n",
    "\n",
    "# def reshape_attrib_acts(\n",
    "#     all_batch_results: dict, gradients: bool = False\n",
    "# ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#     max_len = 0\n",
    "#     for batch_results in all_batch_results:\n",
    "#         model_inputs = batch_results[\"model_inputs\"]\n",
    "#         input_tokens_BL = model_inputs[\"input_ids\"].cpu()\n",
    "#         max_len = max(max_len, input_tokens_BL.shape[1])\n",
    "\n",
    "#     padded_tokens_list = []\n",
    "#     padded_acts_list = []\n",
    "#     labels_list = []\n",
    "\n",
    "#     print(max_len)\n",
    "\n",
    "#     for batch_results in all_batch_results:\n",
    "#         # print(batch_results.keys())\n",
    "#         # print(batch_results)\n",
    "#         model_inputs = batch_results[\"model_inputs\"]\n",
    "#         predicted_tokens = batch_results[\"predicted_tokens\"]\n",
    "#         input_tokens_BL = model_inputs[\"input_ids\"].cpu()\n",
    "#         sae_acts_BLF = batch_results[\"encoded_acts_BLF\"]\n",
    "#         effects_BLF = batch_results[\"effects_BLF\"]\n",
    "#         grad_x_dot_decoder_BLF = batch_results[\"grad_x_dot_decoder_BLF\"]\n",
    "#         labels_B = batch_results[\"labels\"]\n",
    "#         labels_list.append(labels_B)\n",
    "\n",
    "#         # print(sae_acts_BLF.shape)\n",
    "#         # print(effects_BLF.shape)\n",
    "#         # print(grad_x_dot_decoder_BLF.shape)\n",
    "#         # print(predicted_tokens)\n",
    "#         # print(input_tokens_BL)\n",
    "\n",
    "#         B, current_L, F = sae_acts_BLF.shape\n",
    "\n",
    "#         input_tokens_FBL = einops.repeat(input_tokens_BL, \"B L -> F B L\", F=F)\n",
    "#         # print(input_tokens_FBL.shape)\n",
    "\n",
    "#         if gradients:\n",
    "#             chosen_acts_BLF = grad_x_dot_decoder_BLF.clone()\n",
    "#         else:\n",
    "#             chosen_acts_BLF = sae_acts_BLF.clone()\n",
    "#         chosen_acts_FBL = einops.rearrange(chosen_acts_BLF, \"B L F -> F B L\")\n",
    "#         chosen_acts_FBL = chosen_acts_FBL.to(dtype=torch.float32)\n",
    "#         # print(chosen_acts_FBL.shape)\n",
    "\n",
    "#         pad_len = max_len - current_L\n",
    "\n",
    "#         # Pad the last dimension (L)\n",
    "#         # The padding tuple is (pad_left, pad_right) for the last dimension\n",
    "#         padded_input_tokens_FBL = torch.nn.functional.pad(\n",
    "#             input_tokens_FBL,\n",
    "#             (0, pad_len),\n",
    "#             mode=\"constant\",\n",
    "#             value=tokenizer.pad_token_id,\n",
    "#         )\n",
    "#         padded_chosen_acts_FBL = torch.nn.functional.pad(\n",
    "#             chosen_acts_FBL, (0, pad_len), mode=\"constant\", value=0\n",
    "#         )\n",
    "#         # --- Padding End ---\n",
    "\n",
    "#         padded_tokens_list.append(padded_input_tokens_FBL)\n",
    "#         padded_acts_list.append(padded_chosen_acts_FBL)\n",
    "\n",
    "#     all_tokens_FBL = torch.cat(padded_tokens_list, dim=1)\n",
    "#     all_acts_FBL = torch.cat(padded_acts_list, dim=1)\n",
    "#     labels_B = torch.cat(labels_list, dim=0)\n",
    "\n",
    "#     print(all_tokens_FBL.shape)\n",
    "#     print(all_acts_FBL.shape)\n",
    "#     print(labels_B.shape)\n",
    "\n",
    "#     return all_tokens_FBL, all_acts_FBL, labels_B\n",
    "\n",
    "\n",
    "# def analyze_acts(\n",
    "#     acts_FBL: torch.Tensor,\n",
    "#     tokens_FBL: torch.Tensor,\n",
    "#     labels_B: torch.Tensor,\n",
    "#     top_k_ids: torch.Tensor,\n",
    "#     top_k_vals: torch.Tensor,\n",
    "#     gradients: bool = False\n",
    "# ):\n",
    "#     print(acts_FBL.shape)\n",
    "#     print(tokens_FBL.shape)\n",
    "#     print(labels_B)\n",
    "\n",
    "#     pos_mask_K = labels_B == 1\n",
    "#     neg_mask_K = labels_B == 0\n",
    "\n",
    "#     print(pos_mask_K)\n",
    "#     print(neg_mask_K)\n",
    "\n",
    "#     print(acts_FBL.shape)\n",
    "\n",
    "#     pos_acts_FKL = acts_FBL[:, pos_mask_K, :]\n",
    "#     neg_acts_FKL = acts_FBL[:, neg_mask_K, :]\n",
    "\n",
    "#     if gradients:\n",
    "#         pos_acts_FKL = pos_acts_FKL[:, :, -10:]\n",
    "#         neg_acts_FKL = neg_acts_FKL[:, :, -10:]\n",
    "#         neg_acts_FKL = neg_acts_FKL * -1\n",
    "\n",
    "#     mean_pos_acts_F = pos_acts_FKL.mean(dim=(1, 2))\n",
    "#     mean_neg_acts_F = neg_acts_FKL.mean(dim=(1, 2))\n",
    "\n",
    "#     torch.set_printoptions(precision=6, sci_mode=False)\n",
    "\n",
    "#     print(mean_pos_acts_F)\n",
    "#     print(mean_neg_acts_F)\n",
    "\n",
    "#     ratios = mean_pos_acts_F / mean_neg_acts_F\n",
    "\n",
    "#     print(ratios)\n",
    "\n",
    "#     for i in range(top_k_ids.shape[0]):\n",
    "#         print()\n",
    "#         print(f\"Feature {i}, feature idx: {top_k_ids[i]}, value: {top_k_vals[i]:.5f}\")\n",
    "#         print(f\"Pos acts: {mean_pos_acts_F[i]:.5f}, Neg acts: {mean_neg_acts_F[i]:.5f}, Ratio: {ratios[i]:.5f}\")\n",
    "\n",
    "\n",
    "\n",
    "# all_attrib_tokens_FBL, all_attrib_acts_FBL, all_attrib_labels_B = reshape_attrib_acts(\n",
    "#     all_attrib_batch_results, gradients=True\n",
    "# )\n",
    "\n",
    "# analyze_acts(\n",
    "#     all_attrib_acts_FBL,\n",
    "#     all_attrib_tokens_FBL,\n",
    "#     all_attrib_labels_B,\n",
    "#     top_k_ids,\n",
    "#     top_k_vals,\n",
    "#     gradients=True\n",
    "# )\n",
    "\n",
    "# # html_activations = create_html_activations(all_attrib_tokens_FBL, all_attrib_acts_FBL, num_display=5, k=all_attrib_acts_FBL.shape[1])\n",
    "\n",
    "# # with open(\"prompts_html_activations.pkl\", \"wb\") as f:\n",
    "# #     pickle.dump(html_activations, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping standard activations...\n",
      "\n",
      "Reshaping gradient activations...\n",
      "\n",
      "Running analysis...\n",
      "Acts shape: torch.Size([20, 40, 2408])\n",
      "Grad Acts shape: torch.Size([20, 40, 2408])\n",
      "Tokens shape: torch.Size([20, 40, 2408])\n",
      "Labels: tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0], device='cuda:0')\n",
      "\n",
      "--- Analysis per Feature ---\n",
      "Feature Idx   Value      | Acts Pos   Acts Neg   Acts Ratio | Grad Pos   Grad Neg   Grad Ratio\n",
      "----------------------------------------------------------------------------------------------\n",
      "Feature 0 , idx: 2870 , value: 3.62801    | acts pos: 0.24207   , acts neg: 0.24221   , acts ratio: 0.99945    | \n",
      "Feature 1 , idx: 4048 , value: -1.84138   | acts pos: 0.13651   , acts neg: 0.13655   , acts ratio: 0.99968    | \n",
      "Feature 2 , idx: 13789, value: 1.42961    | acts pos: 5.41644   , acts neg: 5.41484   , acts ratio: 1.00030    | \n",
      "Feature 3 , idx: 3513 , value: 0.69815    | acts pos: 0.02527   , acts neg: 0.00657   , acts ratio: 3.84507    | \n",
      "Feature 4 , idx: 15263, value: 0.64506    | acts pos: 0.04849   , acts neg: 0.04844   , acts ratio: 1.00098    | \n",
      "Feature 5 , idx: 949  , value: -0.53539   | acts pos: 0.03416   , acts neg: 0.03410   , acts ratio: 1.00190    | \n",
      "Feature 6 , idx: 15216, value: -0.51998   | acts pos: 0.27614   , acts neg: 0.27502   , acts ratio: 1.00406    | \n",
      "Feature 7 , idx: 2908 , value: -0.50587   | acts pos: 0.01874   , acts neg: 0.01857   , acts ratio: 1.00926    | \n",
      "Feature 8 , idx: 6482 , value: -0.50116   | acts pos: 0.45314   , acts neg: 0.45387   , acts ratio: 0.99839    | \n",
      "Feature 9 , idx: 4235 , value: -0.49248   | acts pos: 2.57529   , acts neg: 2.57145   , acts ratio: 1.00149    | \n",
      "Feature 10, idx: 14392, value: -0.45619   | acts pos: 0.02312   , acts neg: 0.02274   , acts ratio: 1.01710    | \n",
      "Feature 11, idx: 14217, value: -0.44330   | acts pos: 6.53803   , acts neg: 6.53316   , acts ratio: 1.00075    | \n",
      "Feature 12, idx: 10273, value: 0.43648    | acts pos: 0.18592   , acts neg: 0.18598   , acts ratio: 0.99965    | \n",
      "Feature 13, idx: 3235 , value: -0.41989   | acts pos: 0.04781   , acts neg: 0.04781   , acts ratio: 1.00000    | \n",
      "Feature 14, idx: 2841 , value: 0.41925    | acts pos: 18.56936  , acts neg: 18.55108  , acts ratio: 1.00099    | \n",
      "Feature 15, idx: 6652 , value: 0.41489    | acts pos: 0.14132   , acts neg: 0.14146   , acts ratio: 0.99898    | \n",
      "Feature 16, idx: 10847, value: -0.37459   | acts pos: 0.01076   , acts neg: 0.01073   , acts ratio: 1.00283    | \n",
      "Feature 17, idx: 12553, value: -0.34515   | acts pos: 1.34037   , acts neg: 1.34011   , acts ratio: 1.00019    | \n",
      "Feature 18, idx: 3587 , value: -0.32238   | acts pos: 0.07661   , acts neg: 0.07652   , acts ratio: 1.00115    | \n",
      "Feature 19, idx: 5859 , value: -0.31988   | acts pos: 0.16740   , acts neg: 0.16717   , acts ratio: 1.00141    | \n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "\n",
    "def reshape_attrib_acts(\n",
    "    all_batch_results: dict, gradients: bool = False\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    # ... existing code in reshape_attrib_acts ...\n",
    "    # No changes needed inside this function itself\n",
    "    max_len = 0\n",
    "    for batch_results in all_batch_results:\n",
    "        model_inputs = batch_results[\"model_inputs\"]\n",
    "        input_tokens_BL = model_inputs[\"input_ids\"].cpu()\n",
    "        max_len = max(max_len, input_tokens_BL.shape[1])\n",
    "\n",
    "    padded_tokens_list = []\n",
    "    padded_acts_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # print(max_len)\n",
    "\n",
    "    for batch_results in all_batch_results:\n",
    "        # print(batch_results.keys())\n",
    "        # print(batch_results)\n",
    "        model_inputs = batch_results[\"model_inputs\"]\n",
    "        predicted_tokens = batch_results[\"predicted_tokens\"]\n",
    "        input_tokens_BL = model_inputs[\"input_ids\"].cpu()\n",
    "        sae_acts_BLF = batch_results[\"encoded_acts_BLF\"]\n",
    "        effects_BLF = batch_results[\"effects_BLF\"]\n",
    "        grad_x_dot_decoder_BLF = batch_results[\"grad_x_dot_decoder_BLF\"]\n",
    "        labels_B = batch_results[\"labels\"]\n",
    "        labels_list.append(labels_B)\n",
    "\n",
    "        # print(sae_acts_BLF.shape)\n",
    "        # print(effects_BLF.shape)\n",
    "        # print(grad_x_dot_decoder_BLF.shape)\n",
    "        # print(predicted_tokens)\n",
    "        # print(input_tokens_BL)\n",
    "\n",
    "        B, current_L, F = sae_acts_BLF.shape\n",
    "\n",
    "        input_tokens_FBL = einops.repeat(input_tokens_BL, \"B L -> F B L\", F=F)\n",
    "        # print(input_tokens_FBL.shape)\n",
    "\n",
    "        if gradients:\n",
    "            chosen_acts_BLF = grad_x_dot_decoder_BLF.clone()\n",
    "        else:\n",
    "            chosen_acts_BLF = sae_acts_BLF.clone()\n",
    "        chosen_acts_FBL = einops.rearrange(chosen_acts_BLF, \"B L F -> F B L\")\n",
    "        chosen_acts_FBL = chosen_acts_FBL.to(dtype=torch.float32)\n",
    "        # print(chosen_acts_FBL.shape)\n",
    "\n",
    "        pad_len = max_len - current_L\n",
    "\n",
    "        # Pad the last dimension (L)\n",
    "        # The padding tuple is (pad_left, pad_right) for the last dimension\n",
    "        padded_input_tokens_FBL = torch.nn.functional.pad(\n",
    "            input_tokens_FBL,\n",
    "            (0, pad_len),\n",
    "            mode=\"constant\",\n",
    "            value=tokenizer.pad_token_id,\n",
    "        )\n",
    "        padded_chosen_acts_FBL = torch.nn.functional.pad(\n",
    "            chosen_acts_FBL, (0, pad_len), mode=\"constant\", value=0\n",
    "        )\n",
    "        # --- Padding End ---\n",
    "\n",
    "        padded_tokens_list.append(padded_input_tokens_FBL)\n",
    "        padded_acts_list.append(padded_chosen_acts_FBL)\n",
    "\n",
    "    all_tokens_FBL = torch.cat(padded_tokens_list, dim=1)\n",
    "    all_acts_FBL = torch.cat(padded_acts_list, dim=1)\n",
    "    labels_B = torch.cat(labels_list, dim=0)\n",
    "\n",
    "    # print(all_tokens_FBL.shape)\n",
    "    # print(all_acts_FBL.shape)\n",
    "    # print(labels_B.shape)\n",
    "\n",
    "    return all_tokens_FBL, all_acts_FBL, labels_B\n",
    "\n",
    "\n",
    "def analyze_acts(\n",
    "    acts_FBL: torch.Tensor,\n",
    "    acts_grad_FBL: torch.Tensor,  # Added gradient activations\n",
    "    tokens_FBL: torch.Tensor,\n",
    "    labels_B: torch.Tensor,\n",
    "    top_k_ids: torch.Tensor,\n",
    "    top_k_vals: torch.Tensor,\n",
    "    output_filename: str,\n",
    "):\n",
    "    print(f\"Acts shape: {acts_FBL.shape}\")\n",
    "    print(f\"Grad Acts shape: {acts_grad_FBL.shape}\")\n",
    "    print(f\"Tokens shape: {tokens_FBL.shape}\")\n",
    "    print(f\"Labels: {labels_B}\")\n",
    "\n",
    "    pos_mask_K = labels_B == 1\n",
    "    neg_mask_K = labels_B == 0\n",
    "\n",
    "    # print(f\"Pos mask: {pos_mask_K}\")\n",
    "    # print(f\"Neg mask: {neg_mask_K}\")\n",
    "\n",
    "    # --- Regular Activations ---\n",
    "    pos_acts_FKL = acts_FBL[:, pos_mask_K, :]\n",
    "    neg_acts_FKL = acts_FBL[:, neg_mask_K, :]\n",
    "    mean_pos_acts_F = pos_acts_FKL.mean(dim=(1, 2))\n",
    "    mean_neg_acts_F = neg_acts_FKL.mean(dim=(1, 2))\n",
    "    ratios = mean_pos_acts_F / mean_neg_acts_F\n",
    "    ratios = torch.nan_to_num(ratios, nan=0.0)  # Handle potential division by zero\n",
    "\n",
    "    # --- Gradient Activations ---\n",
    "    pos_acts_grad_FKL = acts_grad_FBL[:, pos_mask_K, :]\n",
    "    neg_acts_grad_FKL = acts_grad_FBL[:, neg_mask_K, :]\n",
    "    # Take last 10 tokens for gradients and invert negative examples as before\n",
    "    # pos_acts_grad_FKL = pos_acts_grad_FKL[:, :, -40:]\n",
    "    # neg_acts_grad_FKL = neg_acts_grad_FKL[:, :, -40:]\n",
    "\n",
    "    pos_acts_grad_FKL = pos_acts_grad_FKL[:, :, -10:]\n",
    "    neg_acts_grad_FKL = neg_acts_grad_FKL[:, :, -10:]\n",
    "    neg_acts_grad_FKL = neg_acts_grad_FKL * -1\n",
    "    mean_pos_acts_grad_F = pos_acts_grad_FKL.mean(dim=(1, 2))\n",
    "    mean_neg_acts_grad_F = neg_acts_grad_FKL.mean(dim=(1, 2))\n",
    "    ratios_grad = mean_pos_acts_grad_F / mean_neg_acts_grad_F\n",
    "    ratios_grad = torch.nan_to_num(\n",
    "        ratios_grad, nan=0.0\n",
    "    )  # Handle potential division by zero\n",
    "\n",
    "    torch.set_printoptions(precision=5, sci_mode=False)\n",
    "\n",
    "    print(\"\\n--- Analysis per Feature ---\")\n",
    "    header = f\"{'Feature':<7} {'Idx':<5} {'Value':<10} | {'Acts Pos':<10} {'Acts Neg':<10} {'Acts Ratio':<10} | {'Grad Pos':<10} {'Grad Neg':<10} {'Grad Ratio':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for i in range(top_k_ids.shape[0]):\n",
    "        print(\n",
    "            f\"Feature {i:<2}, \"\n",
    "            f\"idx: {top_k_ids[i]:<5}, \"\n",
    "            f\"value: {top_k_vals[i]:<10.5f} | \"\n",
    "            f\"acts pos: {mean_pos_acts_F[i]:<10.5f}, \"\n",
    "            f\"acts neg: {mean_neg_acts_F[i]:<10.5f}, \"\n",
    "            f\"acts ratio: {ratios[i]:<10.5f} | \"\n",
    "            # f\"grad pos: {mean_pos_acts_grad_F[i]:<10.5f}, \"\n",
    "            # f\"grad neg: {mean_neg_acts_grad_F[i]:<10.5f}, \"\n",
    "            # f\"grad ratio: {ratios_grad[i]:<10.5f}\"\n",
    "        )\n",
    "\n",
    "    data = {\n",
    "        \"top_k_ids\": top_k_ids,\n",
    "        \"top_k_vals\": top_k_vals,\n",
    "        \"mean_pos_acts_F\": mean_pos_acts_F,\n",
    "        \"mean_neg_acts_F\": mean_neg_acts_F,\n",
    "        \"ratios\": ratios,\n",
    "        \"mean_pos_acts_grad_F\": mean_pos_acts_grad_F,\n",
    "        \"mean_neg_acts_grad_F\": mean_neg_acts_grad_F,\n",
    "        \"ratios_grad\": ratios_grad,\n",
    "    }\n",
    "\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "output_dir = \"bias_data_notebook\"\n",
    "output_filename = f\"{bias_type}_{model_name}_layer_{chosen_layers[0]}_downsample_{downsample}_trainer_id_{trainer_id}.pkl\".replace(\"/\", \"_\")\n",
    "output_filename = os.path.join(output_dir, output_filename)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Call reshape_attrib_acts twice ---\n",
    "print(\"Reshaping standard activations...\")\n",
    "all_attrib_tokens_FBL, all_attrib_acts_FBL, all_attrib_labels_B = reshape_attrib_acts(\n",
    "    all_attrib_batch_results, gradients=False\n",
    ")\n",
    "print(\"\\nReshaping gradient activations...\")\n",
    "_, all_attrib_acts_grad_FBL, _ = reshape_attrib_acts(  # Tokens and labels are the same\n",
    "    all_attrib_batch_results, gradients=True\n",
    ")\n",
    "print(\"\\nRunning analysis...\")\n",
    "analyze_acts(\n",
    "    all_attrib_acts_FBL,\n",
    "    all_attrib_acts_grad_FBL,  # Pass gradient acts\n",
    "    all_attrib_tokens_FBL,\n",
    "    all_attrib_labels_B,\n",
    "    top_k_ids,\n",
    "    top_k_vals,\n",
    "    output_filename,\n",
    ")\n",
    "\n",
    "html_activations = create_html_activations(\n",
    "    all_attrib_tokens_FBL,\n",
    "    # all_attrib_acts_grad_FBL,\n",
    "    all_attrib_acts_FBL,\n",
    "    num_display=12,\n",
    "    k=all_attrib_acts_FBL.shape[1],\n",
    ")\n",
    "\n",
    "with open(\"prompts_html_activations.pkl\", \"wb\") as f:\n",
    "    pickle.dump(html_activations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attrib_html_activations = []\n",
    "\n",
    "\n",
    "# def reshape_attrib_acts(\n",
    "#     all_batch_results: dict, gradients: bool = False\n",
    "# ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#     max_len = 0\n",
    "#     for batch_results in all_batch_results:\n",
    "#         model_inputs = batch_results[\"model_inputs\"]\n",
    "#         input_tokens_BL = model_inputs[\"input_ids\"].cpu()\n",
    "#         max_len = max(max_len, input_tokens_BL.shape[1])\n",
    "\n",
    "#     padded_tokens_list = []\n",
    "#     padded_acts_list = []\n",
    "#     labels_list = []\n",
    "\n",
    "#     print(max_len)\n",
    "\n",
    "#     for batch_results in all_batch_results:\n",
    "#         # print(batch_results.keys())\n",
    "#         # print(batch_results)\n",
    "#         model_inputs = batch_results[\"model_inputs\"]\n",
    "#         predicted_tokens = batch_results[\"predicted_tokens\"]\n",
    "#         input_tokens_BL = model_inputs[\"input_ids\"].cpu()\n",
    "#         sae_acts_BLF = batch_results[\"encoded_acts_BLF\"]\n",
    "#         effects_BLF = batch_results[\"effects_BLF\"]\n",
    "#         grad_x_dot_decoder_BLF = batch_results[\"grad_x_dot_decoder_BLF\"]\n",
    "#         labels_B = batch_results[\"labels\"]\n",
    "#         labels_list.append(labels_B)\n",
    "\n",
    "#         # print(sae_acts_BLF.shape)\n",
    "#         # print(effects_BLF.shape)\n",
    "#         # print(grad_x_dot_decoder_BLF.shape)\n",
    "#         # print(predicted_tokens)\n",
    "#         # print(input_tokens_BL)\n",
    "\n",
    "#         B, current_L, F = sae_acts_BLF.shape\n",
    "\n",
    "#         input_tokens_FBL = einops.repeat(input_tokens_BL, \"B L -> F B L\", F=F)\n",
    "#         # print(input_tokens_FBL.shape)\n",
    "\n",
    "#         if gradients:\n",
    "#             chosen_acts_BLF = grad_x_dot_decoder_BLF.clone()\n",
    "#         else:\n",
    "#             chosen_acts_BLF = sae_acts_BLF.clone()\n",
    "#         chosen_acts_FBL = einops.rearrange(chosen_acts_BLF, \"B L F -> F B L\")\n",
    "#         chosen_acts_FBL = chosen_acts_FBL.to(dtype=torch.float32)\n",
    "#         # print(chosen_acts_FBL.shape)\n",
    "\n",
    "#         pad_len = max_len - current_L\n",
    "\n",
    "#         # Pad the last dimension (L)\n",
    "#         # The padding tuple is (pad_left, pad_right) for the last dimension\n",
    "#         padded_input_tokens_FBL = torch.nn.functional.pad(\n",
    "#             input_tokens_FBL,\n",
    "#             (0, pad_len),\n",
    "#             mode=\"constant\",\n",
    "#             value=tokenizer.pad_token_id,\n",
    "#         )\n",
    "#         padded_chosen_acts_FBL = torch.nn.functional.pad(\n",
    "#             chosen_acts_FBL, (0, pad_len), mode=\"constant\", value=0\n",
    "#         )\n",
    "#         # --- Padding End ---\n",
    "\n",
    "#         padded_tokens_list.append(padded_input_tokens_FBL)\n",
    "#         padded_acts_list.append(padded_chosen_acts_FBL)\n",
    "\n",
    "#     all_tokens_FBL = torch.cat(padded_tokens_list, dim=1)\n",
    "#     all_acts_FBL = torch.cat(padded_acts_list, dim=1)\n",
    "#     labels_B = torch.cat(labels_list, dim=0)\n",
    "\n",
    "#     print(all_tokens_FBL.shape)\n",
    "#     print(all_acts_FBL.shape)\n",
    "#     print(labels_B.shape)\n",
    "\n",
    "#     return all_tokens_FBL, all_acts_FBL, labels_B\n",
    "\n",
    "\n",
    "# def analyze_acts(\n",
    "#     acts_FBL: torch.Tensor,\n",
    "#     tokens_FBL: torch.Tensor,\n",
    "#     labels_B: torch.Tensor,\n",
    "#     top_k_ids: torch.Tensor,\n",
    "#     top_k_vals: torch.Tensor,\n",
    "#     gradients: bool = False\n",
    "# ):\n",
    "#     print(acts_FBL.shape)\n",
    "#     print(tokens_FBL.shape)\n",
    "#     print(labels_B)\n",
    "\n",
    "#     pos_mask_K = labels_B == 1\n",
    "#     neg_mask_K = labels_B == 0\n",
    "\n",
    "#     print(pos_mask_K)\n",
    "#     print(neg_mask_K)\n",
    "\n",
    "#     print(acts_FBL.shape)\n",
    "\n",
    "#     pos_acts_FKL = acts_FBL[:, pos_mask_K, :]\n",
    "#     neg_acts_FKL = acts_FBL[:, neg_mask_K, :]\n",
    "\n",
    "#     if gradients:\n",
    "#         pos_acts_FKL = pos_acts_FKL[:, :, -50:]\n",
    "#         neg_acts_FKL = neg_acts_FKL[:, :, -50:]\n",
    "#         neg_acts_FKL = neg_acts_FKL * -1\n",
    "\n",
    "#     mean_pos_acts_F = pos_acts_FKL.mean(dim=(1, 2))\n",
    "#     mean_neg_acts_F = neg_acts_FKL.mean(dim=(1, 2))\n",
    "\n",
    "#     torch.set_printoptions(precision=6, sci_mode=False)\n",
    "\n",
    "#     print(mean_pos_acts_F)\n",
    "#     print(mean_neg_acts_F)\n",
    "\n",
    "#     ratios = mean_pos_acts_F / mean_neg_acts_F\n",
    "\n",
    "#     print(ratios)\n",
    "\n",
    "#     for i in range(top_k_ids.shape[0]):\n",
    "#         print()\n",
    "#         print(f\"Feature {i}, feature idx: {top_k_ids[i]}, value: {top_k_vals[i]:.5f}\")\n",
    "#         print(f\"Pos acts: {mean_pos_acts_F[i]:.5f}, Neg acts: {mean_neg_acts_F[i]:.5f}, Ratio: {ratios[i]:.5f}\")\n",
    "\n",
    "\n",
    "\n",
    "# all_attrib_tokens_FBL, all_attrib_acts_FBL, all_attrib_labels_B = reshape_attrib_acts(\n",
    "#     all_attrib_batch_results, gradients=True\n",
    "# )\n",
    "\n",
    "# analyze_acts(\n",
    "#     all_attrib_acts_FBL,\n",
    "#     all_attrib_tokens_FBL,\n",
    "#     all_attrib_labels_B,\n",
    "#     top_k_ids,\n",
    "#     top_k_vals,\n",
    "#     gradients=True\n",
    "# )\n",
    "\n",
    "# html_activations = create_html_activations(all_attrib_tokens_FBL, all_attrib_acts_FBL, num_display=5, k=all_attrib_acts_FBL.shape[1])\n",
    "\n",
    "# with open(\"prompts_html_activations.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(html_activations, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_output(wait=True)\n",
    "# gc.collect()\n",
    "# html_activations = create_html_activations(all_attrib_tokens_FBL, all_attrib_acts_FBL, num_display=2, k=all_attrib_acts_FBL.shape[1])\n",
    "# for i, html_activation in enumerate(html_activations):\n",
    "#     feature_idx = top_k_ids[i]\n",
    "#     feature_val = top_k_vals[i]\n",
    "#     print(f\"Feature {i}, feature idx: {feature_idx}, value: {feature_val}\")\n",
    "#     display(html_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "\n",
    "# inspection_eval_config = deepcopy(eval_config)\n",
    "# inspection_eval_config.downsample = 3\n",
    "\n",
    "# inspect_df = dataset_setup.balanced_downsample(df, inspection_eval_config.downsample, random_seed)\n",
    "\n",
    "# inspect_prompts = hiring_bias_prompts.create_all_prompts_hiring_bias(inspect_df, args, inspection_eval_config)\n",
    "\n",
    "# inspect_texts, inspect_labels = hiring_bias_prompts.process_hiring_bias_resumes_prompts(inspect_prompts, args)\n",
    "\n",
    "# inspect_texts = model_utils.add_chat_template(inspect_texts, model_name)\n",
    "\n",
    "# inspect_tokens_FBL, inspect_activations_FBL = interp_utils.get_interp_prompts_user_inputs(\n",
    "#     model,\n",
    "#     submodules[0],\n",
    "#     sae,\n",
    "#     top_k_ids,\n",
    "#     inspect_texts,\n",
    "#     tokenizer=tokenizer,\n",
    "#     batch_size=batch_size * 6,\n",
    "#     k=len(inspect_texts),\n",
    "# )\n",
    "\n",
    "# num_display = 2\n",
    "\n",
    "# html_activations = display_html_activations(inspect_tokens_FBL, inspect_activations_FBL, num_display=num_display, k=len(inspect_texts))\n",
    "\n",
    "# for i, html_activation in enumerate(html_activations):\n",
    "#     feature_idx = top_k_ids[i]\n",
    "#     feature_val = top_k_vals[i]\n",
    "#     print(f\"Feature {i}, feature idx: {feature_idx}, value: {feature_val}\")\n",
    "#     display(html_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
