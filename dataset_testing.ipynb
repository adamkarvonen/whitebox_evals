{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/whitebox_evals/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import sys\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "from mypkg.pipeline.setup.dataset import (\n",
    "    load_raw_dataset,\n",
    "    filter_by_industry,\n",
    "    filter_by_demographics,\n",
    "    prepare_dataset_for_model,\n",
    ")\n",
    "from mypkg.main_paper_dataset import balanced_downsample\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "explicit_dataset = load_dataset(\"Anthropic/discrim-eval\", \"explicit\")\n",
    "implicit_dataset = load_dataset(\"Anthropic/discrim-eval\", \"implicit\")\n",
    "\n",
    "explicit_df = pd.DataFrame(explicit_dataset[\"train\"])\n",
    "implicit_df = pd.DataFrame(implicit_dataset[\"train\"])\n",
    "\n",
    "df = pd.concat([explicit_df, implicit_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     filled_template  decision_question_id  \\\n",
      "0  The patient is a 20-year-old white female suff...                     0   \n",
      "1  The patient is a 20-year-old Black female suff...                     0   \n",
      "2  The patient is a 20-year-old Asian female suff...                     0   \n",
      "3  The patient is a 20-year-old Hispanic female s...                     0   \n",
      "4  The patient is a 20-year-old Native American f...                     0   \n",
      "\n",
      "    age  gender             race fill_type  \n",
      "0  20.0  female            white  explicit  \n",
      "1  20.0  female            Black  explicit  \n",
      "2  20.0  female            Asian  explicit  \n",
      "3  20.0  female         Hispanic  explicit  \n",
      "4  20.0  female  Native American  explicit  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "372\n",
      "                                      filled_template  decision_question_id  \\\n",
      "0   The patient is a 20-year-old white female suff...                     0   \n",
      "5   The patient is a(n) 20-year-old white male per...                     0   \n",
      "15  The patient is a(n) 30-year-old white female p...                     0   \n",
      "20  The patient is a 30-year-old white male suffer...                     0   \n",
      "30  The patient is a  40-year-old female white per...                     0   \n",
      "\n",
      "     age  gender   race fill_type  \n",
      "0   20.0  female  white  explicit  \n",
      "5   20.0    male  white  explicit  \n",
      "15  30.0  female  white  explicit  \n",
      "20  30.0    male  white  explicit  \n",
      "30  40.0  female  white  explicit  \n"
     ]
    }
   ],
   "source": [
    "retain_races = [\"white\", \"black\"]\n",
    "retain_ages = [20, 30, 40]\n",
    "retain_questions = list(range(0,41))\n",
    "retain_genders = [\"male\", \"female\"]\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "df = df[df[\"race\"].isin(retain_races)]\n",
    "df = df[df[\"age\"].isin(retain_ages)]\n",
    "df = df[df[\"decision_question_id\"].isin(retain_questions)]\n",
    "df = df[df[\"gender\"].isin(retain_genders)]\n",
    "print(len(df))\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 9450/9450 [00:00<00:00, 180166.88 examples/s]\n",
      "Generating train split: 100%|██████████| 9450/9450 [00:00<00:00, 374519.74 examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'datasets.arrow_dataset.Dataset'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([explicit_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], implicit_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_full_anthropic_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mload_full_anthropic_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m explicit_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnthropic/discrim-eval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplicit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m implicit_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnthropic/discrim-eval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimplicit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexplicit_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimplicit_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/workspace/whitebox_evals/.venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/workspace/whitebox_evals/.venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:448\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ndims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m sample, objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sample_object(objs, ndims, keys, names, levels)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Standardize axis parameter to int\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/whitebox_evals/.venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:489\u001b[0m, in \u001b[0;36m_Concatenator._get_ndims\u001b[0;34m(self, objs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[1;32m    485\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot concatenate object of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly Series and DataFrame objs are valid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m         )\n\u001b[0;32m--> 489\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    491\u001b[0m     ndims\u001b[38;5;241m.\u001b[39madd(obj\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ndims\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'datasets.arrow_dataset.Dataset'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "def load_full_anthropic_dataset(downsample_questions: int = 41):\n",
    "    explicit_dataset = load_dataset(\"Anthropic/discrim-eval\", \"explicit\")\n",
    "    implicit_dataset = load_dataset(\"Anthropic/discrim-eval\", \"implicit\")\n",
    "\n",
    "    explicit_df = pd.DataFrame(explicit_dataset[\"train\"])\n",
    "    implicit_df = pd.DataFrame(implicit_dataset[\"train\"])\n",
    "\n",
    "    df = pd.concat([explicit_df, implicit_df])\n",
    "\n",
    "    retain_races = [\"white\", \"black\"]\n",
    "    retain_ages = [20, 30, 40]\n",
    "    retain_questions = list(range(0,downsample_questions))\n",
    "    retain_genders = [\"male\", \"female\"]\n",
    "\n",
    "    print(len(df))\n",
    "\n",
    "    df = df[df[\"race\"].isin(retain_races)]\n",
    "    df = df[df[\"age\"].isin(retain_ages)]\n",
    "    df = df[df[\"decision_question_id\"].isin(retain_questions)]\n",
    "    df = df[df[\"gender\"].isin(retain_genders)]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_full_anthropic_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Function to load the datasets\n",
    "def load_datasets(industry=\"INFORMATION-TECHNOLOGY\", \n",
    "                 use_anthropic_dataset=False,\n",
    "                 dataset_type=\"implicit\",\n",
    "                 downsample_to=None):\n",
    "    \"\"\"\n",
    "    Load and prepare datasets for analysis\n",
    "    \n",
    "    Args:\n",
    "        industry: Industry to filter for (only for non-Anthropic dataset)\n",
    "        use_anthropic_dataset: Whether to use the Anthropic dataset\n",
    "        dataset_type: Type of Anthropic dataset to use\n",
    "        downsample_to: Number of samples to downsample to (if None, use all)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing the loaded and filtered dataset\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    \n",
    "    if use_anthropic_dataset:\n",
    "        print(f\"Loading Anthropic dataset ({dataset_type})...\")\n",
    "        dataset = load_dataset(\"Anthropic/discrim-eval\", dataset_type)\n",
    "        df = dataset[\"train\"]\n",
    "        filtered_df = [item for item in df if item[\"decision_question_id\"] == 16]\n",
    "        df = pd.DataFrame(filtered_df)\n",
    "    else:\n",
    "        print(\"Loading custom dataset...\")\n",
    "        df = load_raw_dataset()\n",
    "        \n",
    "        if industry:\n",
    "            print(f\"Filtering for industry: {industry}\")\n",
    "            df = filter_by_industry(df, industry)\n",
    "        else:\n",
    "            print(\"No industry filter applied.\")\n",
    "    \n",
    "    # Apply downsampling if requested\n",
    "    if downsample_to is not None and downsample_to > 0:\n",
    "        print(f\"Downsampling to {downsample_to} samples...\")\n",
    "        df = balanced_downsample(df, downsample_to, random_seed)\n",
    "    \n",
    "    print(f\"Dataset loaded with {len(df)} entries\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Cell 4: Example usage\n",
    "# You can run this to test loading the datasets\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Load standard dataset for IT industry\n",
    "    standard_df = load_datasets(industry=\"INFORMATION-TECHNOLOGY\", downsample_to=100)\n",
    "    \n",
    "    # Example: Load Anthropic dataset\n",
    "    # anthropic_df = load_datasets(use_anthropic_dataset=True, dataset_type=\"explicit\")\n",
    "    anthropic_df = load_datasets(use_anthropic_dataset=True, dataset_type=\"implicit\")\n",
    "    \n",
    "    # Display basic info about the datasets\n",
    "    print(\"\\nStandard Dataset Info:\")\n",
    "    print(f\"Shape: {standard_df.shape}\")\n",
    "    print(\"Columns:\", standard_df.columns.tolist())\n",
    "    \n",
    "    print(\"\\nAnthropic Dataset Info:\")\n",
    "    print(f\"Shape: {anthropic_df.shape}\")\n",
    "    print(\"Columns:\", anthropic_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_anthropic_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Convert to DataFrame if not already\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    # Filter for decision question ID\n",
    "    df = df[df[\"decision_question_id\"] == 16]\n",
    "    \n",
    "    # Filter for race (white or black only)\n",
    "    df = df[df[\"race\"].str.lower().isin([\"white\", \"black\"])]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def modify_anthropic_filled_templates(df):\n",
    "    \"\"\"Change the anthropic templates to match the hiring bias prompts\"\"\"\n",
    "    def modify(text):\n",
    "        if \"accountant\" not in text:\n",
    "            return text\n",
    "        # Replace role\n",
    "        text = text.replace(\"accounting\", \"\")\n",
    "        text = text.replace(\"accountant\", \"IT consultant\")\n",
    "        # Remove the word 'financial'\n",
    "        text = text.replace(\"financial \", \"\")\n",
    "        # Change job offer question\n",
    "        text = text.replace(\n",
    "            \"make the applicant a job offer\",\n",
    "            \"offer the applicant an interview\"\n",
    "        )\n",
    "        return text\n",
    "\n",
    "    df = df.copy()\n",
    "    df['filled_template'] = df['filled_template'].apply(modify)\n",
    "    return df\n",
    "\n",
    "anthropic_df = filter_anthropic_df(anthropic_df)\n",
    "anthropic_df = modify_anthropic_filled_templates(anthropic_df)\n",
    "print(anthropic_df.head())\n",
    "print(len(anthropic_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anthropic_df)):\n",
    "    print(anthropic_df.iloc[i]['filled_template'])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
