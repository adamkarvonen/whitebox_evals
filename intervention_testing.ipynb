{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Must set before importing torch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Callable, Optional\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import mypkg.whitebox_infra.attribution as attribution\n",
    "import mypkg.whitebox_infra.dictionaries.batch_topk_sae as batch_topk_sae\n",
    "import mypkg.whitebox_infra.data_utils as data_utils\n",
    "import mypkg.whitebox_infra.model_utils as model_utils\n",
    "import mypkg.whitebox_infra.interp_utils as interp_utils\n",
    "import mypkg.pipeline.setup.dataset as dataset_setup\n",
    "import mypkg.pipeline.infra.hiring_bias_prompts as hiring_bias_prompts\n",
    "from mypkg.eval_config import EvalConfig\n",
    "import mypkg.pipeline.infra.model_inference as model_inference\n",
    "import mypkg.whitebox_infra.intervention_hooks as intervention_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "# model_name = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "# model_name = \"google/gemma-2-9b-it\"\n",
    "# model_name = \"google/gemma-2-27b-it\"\n",
    "\n",
    "bias_type = \"gender\"\n",
    "bias_type = \"race\"\n",
    "# bias_type = \"political_orientation\"\n",
    "\n",
    "anti_bias_statement_file_idx = 3\n",
    "anti_bias_statement_file = \"v1.txt\"\n",
    "anti_bias_statement_file = f\"v{anti_bias_statement_file_idx}.txt\"\n",
    "# anti_bias_statement_file = \"v17.txt\"\n",
    "\n",
    "args = hiring_bias_prompts.HiringBiasArgs(\n",
    "    political_orientation=bias_type == \"political_orientation\",\n",
    "    employment_gap=bias_type == \"employment_gap\",\n",
    "    pregnancy=bias_type == \"pregnancy\",\n",
    "    race=bias_type == \"race\",\n",
    "    gender=bias_type == \"gender\",\n",
    "    misc=bias_type == \"misc\",\n",
    ")\n",
    "\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=dtype, device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "gradient_checkpointing = False\n",
    "\n",
    "if model_name == \"google/gemma-2-27b-it\":\n",
    "    gradient_checkpointing = True\n",
    "    batch_size = 1\n",
    "elif model_name == \"mistralai/Mistral-Small-24B-Instruct-2501\" or model_name == \"google/gemma-2-2b-it\":\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 3\n",
    "\n",
    "if gradient_checkpointing:\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "chosen_layer_percentage = [25]\n",
    "# chosen_layer_percentage = [50]\n",
    "\n",
    "# chosen_layer_percentage = [75]\n",
    "\n",
    "system_prompt = \"yes_no.txt\"\n",
    "# system_prompt = \"yes_no_qualifications.txt\"\n",
    "\n",
    "use_activation_loss_fn = True\n",
    "use_activation_loss_fn = False\n",
    "\n",
    "chosen_layers = []\n",
    "for layer_percent in chosen_layer_percentage:\n",
    "    chosen_layers.append(model_utils.MODEL_CONFIGS[model_name][\"layer_mappings\"][layer_percent][\"layer\"])\n",
    "\n",
    "eval_config = EvalConfig(\n",
    "        model_name=model_name,\n",
    "        political_orientation=True,\n",
    "        pregnancy=False,\n",
    "        employment_gap=False,\n",
    "        anthropic_dataset=False,\n",
    "        # downsample=150,\n",
    "        # downsample=5,\n",
    "        downsample=20,\n",
    "        gpu_inference=True,\n",
    "        anti_bias_statement_file=anti_bias_statement_file,\n",
    "        job_description_file=\"short_meta_job_description.txt\",\n",
    "        system_prompt_filename=system_prompt,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sae_repo = \"adamkarvonen/ministral_saes\"\n",
    "# sae_path = f\"mistralai_Ministral-8B-Instruct-2410_batch_top_k/resid_post_layer_{chosen_layers[0]}/trainer_1/ae.pt\"\n",
    "\n",
    "# sae = batch_topk_sae.load_dictionary_learning_batch_topk_sae(\n",
    "#     repo_id=sae_repo,\n",
    "#     filename=sae_path,\n",
    "#     model_name=model_name,\n",
    "#     device=device,\n",
    "#     dtype=dtype,\n",
    "#     layer=chosen_layers[0],\n",
    "#     local_dir=\"downloaded_saes\",\n",
    "# )\n",
    "trainer_id = model_utils.MODEL_CONFIGS[model_name][\"trainer_id\"]\n",
    "    \n",
    "sae = model_utils.load_model_sae(model_name, device, dtype, chosen_layer_percentage[0], trainer_id=trainer_id)\n",
    "\n",
    "submodules = [model_utils.get_submodule(model, chosen_layers[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset_setup.load_raw_dataset()\n",
    "\n",
    "industry = \"INFORMATION-TECHNOLOGY\"\n",
    "downsample = eval_config.downsample\n",
    "random_seed = eval_config.random_seed\n",
    "\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "df = dataset_setup.filter_by_industry(df, industry)\n",
    "\n",
    "df = dataset_setup.balanced_downsample(df, downsample, random_seed)\n",
    "\n",
    "\n",
    "\n",
    "prompts = hiring_bias_prompts.create_all_prompts_hiring_bias(df, args, eval_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels, train_resume_prompt_results = hiring_bias_prompts.process_hiring_bias_resumes_prompts(prompts, model_name, args)\n",
    "\n",
    "for i in range(len(train_texts)):\n",
    "    train_resume_prompt_results[i].prompt = train_texts[i]\n",
    "\n",
    "if system_prompt == \"yes_no_qualifications.txt\":\n",
    "    for i in range(len(train_texts)):\n",
    "        # train_texts[i] += \"The most important qualifications of the candidate to consider are:\"\n",
    "        # train_texts[i] += \"The most important factors about the candidate to consider are:\"\n",
    "        train_texts[i] += \"The most important qualities of the candidate to consider are:\"\n",
    "\n",
    "dataloader = data_utils.create_simple_dataloader(\n",
    "    train_texts, train_labels, prompts, model_name, device, batch_size=batch_size, max_length=2500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = model_inference.run_single_forward_pass_transformers(\n",
    "    prompts, model_name, batch_size=batch_size * 2, model=model\n",
    ")\n",
    "\n",
    "bias_scores = hiring_bias_prompts.evaluate_bias(\n",
    "    model_answers,\n",
    "    system_prompt\n",
    ")\n",
    "print(bias_scores)\n",
    "\n",
    "bias_probs = hiring_bias_prompts.evaluate_bias_probs(\n",
    "    model_answers,\n",
    "    system_prompt\n",
    ")\n",
    "print(bias_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ablation_features = intervention_hooks.lookup_sae_features(model_name, trainer_id, chosen_layer_percentage[0], anti_bias_statement_file_idx, bias_type)\n",
    "\n",
    "print(ablation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_features = torch.tensor([4356, 31477])\n",
    "model_answers = model_inference.run_single_forward_pass_transformers(\n",
    "    prompts, model_name, batch_size=batch_size * 2, model=model, ablation_features=ablation_features, ablation_type=\"clamping\"\n",
    ")\n",
    "\n",
    "bias_scores = hiring_bias_prompts.evaluate_bias(\n",
    "    model_answers,\n",
    "    system_prompt\n",
    ")\n",
    "\n",
    "print(bias_scores)\n",
    "\n",
    "bias_probs = hiring_bias_prompts.evaluate_bias_probs(\n",
    "    model_answers,\n",
    "    system_prompt\n",
    ")\n",
    "print(bias_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_features = torch.tensor([4356, 31477])\n",
    "model_answers = model_inference.run_single_forward_pass_transformers(\n",
    "    prompts, model_name, batch_size=batch_size * 2, model=model, ablation_features=ablation_features, ablation_type=\"adaptive_clamping\", scale=2.0\n",
    ")\n",
    "\n",
    "bias_scores = hiring_bias_prompts.evaluate_bias(\n",
    "    model_answers,\n",
    "    system_prompt\n",
    ")\n",
    "\n",
    "print(bias_scores)\n",
    "\n",
    "bias_probs = hiring_bias_prompts.evaluate_bias_probs(\n",
    "    model_answers,\n",
    "    system_prompt\n",
    ")\n",
    "print(bias_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_features = torch.tensor([4356, 31477])\n",
    "model_answers = model_inference.run_single_forward_pass_transformers(\n",
    "    prompts, model_name, batch_size=batch_size * 2, model=model, ablation_features=ablation_features, ablation_type=\"targeted\"\n",
    ")\n",
    "\n",
    "bias_scores = hiring_bias_prompts.evaluate_bias(\n",
    "    model_answers,\n",
    "    system_prompt\n",
    ")\n",
    "\n",
    "print(bias_scores)\n",
    "\n",
    "bias_probs = hiring_bias_prompts.evaluate_bias_probs(\n",
    "    model_answers,\n",
    "    system_prompt\n",
    ")\n",
    "print(bias_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise ValueError(\"Stop here\")\n",
    "\n",
    "# ablation_features = torch.tensor([4356])\n",
    "# model_answers = model_inference.run_single_forward_pass_transformers(\n",
    "#     prompts, model_name, batch_size=batch_size * 2, model=model, ablation_features=ablation_features\n",
    "# )\n",
    "\n",
    "# bias_scores = hiring_bias_prompts.evaluate_bias(\n",
    "#     model_answers,\n",
    "#     system_prompt\n",
    "# )\n",
    "\n",
    "# print(bias_scores)\n",
    "\n",
    "# bias_probs = hiring_bias_prompts.evaluate_bias_probs(\n",
    "#     model_answers,\n",
    "#     system_prompt\n",
    "# )\n",
    "# print(bias_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
