{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Literal, Optional\n",
    "import einops\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mypkg.whitebox_infra.attribution as attribution\n",
    "import mypkg.whitebox_infra.dictionaries.batch_topk_sae as batch_topk_sae\n",
    "import mypkg.whitebox_infra.data_utils as data_utils\n",
    "import mypkg.whitebox_infra.model_utils as model_utils\n",
    "import mypkg.whitebox_infra.interp_utils as interp_utils\n",
    "import mypkg.pipeline.setup.dataset as dataset_setup\n",
    "import mypkg.pipeline.infra.hiring_bias_prompts as hiring_bias_prompts\n",
    "from mypkg.eval_config import EvalConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=dtype, device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "batch_size = model_utils.MODEL_CONFIGS[model_name][\"batch_size\"] * 6\n",
    "context_length = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_layer = 18\n",
    "submodules = [model_utils.get_submodule(model, chosen_layer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(list(model.model.layers))\n",
    "print(num_layers)\n",
    "\n",
    "submodules = [model_utils.get_submodule(model, i) for i in range(num_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized dataset from tokens/togethercomputer_RedPajama-Data-V2_10000_google_gemma-2-2b-it.pt\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"togethercomputer/RedPajama-Data-V2\"\n",
    "num_tokens = 10_000\n",
    "\n",
    "batched_tokens = interp_utils.get_batched_tokens(\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=model_name,\n",
    "    dataset_name=dataset_name,\n",
    "    num_tokens=num_tokens,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    context_length=context_length,\n",
    "    force_rebuild_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 42.517857142857146, 1: 33.857142857142854, 2: 23.678571428571427, 3: 21.142857142857142, 4: 18.714285714285715, 5: 21.178571428571427, 6: 18.303571428571427, 7: 15.276785714285714, 8: 12.84375, 9: 12.584821428571429, 10: 12.214285714285714, 11: 11.303571428571429, 12: 11.008928571428571, 13: 9.258928571428571, 14: 8.915178571428571, 15: 7.6875, 16: 7.370535714285714, 17: 6.555803571428571, 18: 6.220982142857143, 19: 5.303571428571429, 20: 4.205357142857143, 21: 4.314174107142857, 22: 2.7472098214285716, 23: 1.7064732142857142, 24: 0.6989397321428571, 25: 0.05683244977678571}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:09<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 73.64285714285714, 1: 36.964285714285715, 2: 24.125, 3: 19.785714285714285, 4: 19.839285714285715, 5: 22.75, 6: 24.107142857142858, 7: 21.821428571428573, 8: 20.026785714285715, 9: 20.008928571428573, 10: 15.794642857142858, 11: 12.133928571428571, 12: 11.321428571428571, 13: 9.116071428571429, 14: 8.066964285714286, 15: 6.915178571428571, 16: 5.669642857142857, 17: 5.410714285714286, 18: 5.21875, 19: 4.549107142857143, 20: 3.595982142857143, 21: 3.200892857142857, 22: 2.5089285714285716, 23: 1.703125, 24: 0.7806919642857143, 25: 0.04380580357142857}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_activations_per_layer(\n",
    "    model: AutoModelForCausalLM,\n",
    "    submodules: list[torch.nn.Module],\n",
    "    tokens_batch: dict[str, torch.Tensor],\n",
    "    get_final_token_only: bool = False,\n",
    ") -> tuple[dict[torch.nn.Module, torch.Tensor], torch.Tensor]:\n",
    "    activations_BLD = {}\n",
    "\n",
    "    def gather_target_act_hook(module, inputs, outputs):\n",
    "        nonlocal activations_BLD\n",
    "        assert isinstance(outputs, tuple)\n",
    "        if get_final_token_only:\n",
    "            activations_BLD[module] = outputs[0][:, -1:, :]\n",
    "        else:\n",
    "            activations_BLD[module] = outputs[0]\n",
    "\n",
    "    all_handles = []\n",
    "\n",
    "    try:\n",
    "        for submodule in submodules:\n",
    "            handle = submodule.register_forward_hook(gather_target_act_hook)\n",
    "            all_handles.append(handle)\n",
    "\n",
    "        logits_BLV = model(**tokens_batch).logits\n",
    "\n",
    "        if get_final_token_only:\n",
    "            logits_BLV = logits_BLV[:, -1:, :]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise e\n",
    "    finally:\n",
    "        for handle in all_handles:\n",
    "            handle.remove()\n",
    "\n",
    "    return activations_BLD, logits_BLV\n",
    "\n",
    "\n",
    "def run_final_block(\n",
    "    acts_BLD: torch.Tensor,\n",
    "    model: AutoModelForCausalLM,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Feeds `acts_BLD` through the last Gemma2DecoderLayer (attention+MLP)\n",
    "    and returns the updated hidden states.\n",
    "    \"\"\"\n",
    "    B, L, _ = acts_BLD.shape\n",
    "    device = acts_BLD.device\n",
    "\n",
    "    pos_ids_BL = torch.arange(L, device=device).unsqueeze(0).expand(B, L)\n",
    "    position_embeddings = model.model.rotary_emb(acts_BLD, pos_ids_BL)\n",
    "\n",
    "    hidden_BLD = model.model.layers[-1](\n",
    "        hidden_states=acts_BLD,\n",
    "        position_embeddings=position_embeddings,\n",
    "    )\n",
    "    return hidden_BLD[0]\n",
    "\n",
    "\n",
    "def apply_logit_lens(\n",
    "    acts_BLD: torch.Tensor, model: AutoModelForCausalLM, final_token_only: bool = True\n",
    ") -> torch.Tensor:\n",
    "    if final_token_only:\n",
    "        acts_BLD = acts_BLD[:, -1:, :]\n",
    "\n",
    "    acts_BLD += run_final_block(acts_BLD, model)\n",
    "\n",
    "    logits_BLV = model.lm_head(model.model.norm(acts_BLD))\n",
    "    return logits_BLV\n",
    "\n",
    "\n",
    "def kl_between_logits(\n",
    "    logits_p_BLV: torch.Tensor,  # “teacher” / reference\n",
    "    logits_q_BLV: torch.Tensor,  # “student” / lens\n",
    "    reduction: Literal[\"batchmean\", \"mean\", \"none\"] = \"none\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    KL‖(p || q) for two logit tensors of shape [B, L, V].\n",
    "\n",
    "    * `p` is obtained with softmax on `logits_p_BLV`\n",
    "    * `q` is obtained with softmax on `logits_q_BLV`\n",
    "    * Uses `torch.kl_div(log_q, p)` so we pass log-probabilities for q\n",
    "    \"\"\"\n",
    "\n",
    "    B, L, V = logits_p_BLV.shape\n",
    "\n",
    "    # Convert logits → probabilities / log-probabilities\n",
    "    p_BLV = torch.softmax(logits_p_BLV, dim=-1)\n",
    "    log_q_BLV = torch.log_softmax(logits_q_BLV, dim=-1)\n",
    "\n",
    "    # KL divergence per token; `reduction` handles batching\n",
    "    kl = F.kl_div(log_q_BLV, p_BLV, reduction=reduction).sum(dim=-1)\n",
    "\n",
    "    return kl  # scalar if reduction ≠ \"none\", else [B, L]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_activations(\n",
    "    model: AutoModelForCausalLM,\n",
    "    submodules: list[torch.nn.Module],\n",
    "    batched_tokens: dict[str, torch.Tensor],\n",
    "    get_final_token_only: bool = False,\n",
    ") -> dict[int, float]:\n",
    "    mean_kl_per_layer = {}\n",
    "\n",
    "    for i, layer in enumerate(submodules):\n",
    "        mean_kl_per_layer[i] = 0.0\n",
    "\n",
    "    for batch in tqdm(batched_tokens):\n",
    "        activations_BLD, logits_BLV = get_activations_per_layer(\n",
    "            model, submodules, batch, get_final_token_only=get_final_token_only\n",
    "        )\n",
    "\n",
    "        for i, layer in enumerate(submodules):\n",
    "            logit_lens_BLV = apply_logit_lens(\n",
    "                activations_BLD[layer], model, final_token_only=get_final_token_only\n",
    "            )\n",
    "            kl = kl_between_logits(logits_BLV, logit_lens_BLV)\n",
    "            mean_kl_per_layer[i] += kl.mean().item()\n",
    "\n",
    "    for i, layer in enumerate(submodules):\n",
    "        mean_kl_per_layer[i] /= len(batched_tokens)\n",
    "\n",
    "    return mean_kl_per_layer\n",
    "\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# acts_BLD, logits_BLV = get_batch_activations(model, submodules, batched_tokens[0])\n",
    "\n",
    "kl = get_activations(model, submodules, batched_tokens, get_final_token_only=True)\n",
    "print(kl)\n",
    "kl = get_activations(model, submodules, batched_tokens, get_final_token_only=False)\n",
    "print(kl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 73.64285714285714, 1: 36.964285714285715, 2: 24.125, 3: 19.767857142857142, 4: 19.839285714285715, 5: 22.732142857142858, 6: 24.107142857142858, 7: 21.821428571428573, 8: 20.044642857142858, 9: 20.0, 10: 15.803571428571429, 11: 12.142857142857142, 12: 11.3125, 13: 9.107142857142858, 14: 8.0625, 15: 6.90625, 16: 5.669642857142857, 17: 5.40625, 18: 5.214285714285714, 19: 4.549107142857143, 20: 3.595982142857143, 21: 3.200892857142857, 22: 2.5044642857142856, 23: 1.7042410714285714, 24: 0.7801339285714286, 25: 0.04377092633928571}\n"
     ]
    }
   ],
   "source": [
    "print(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(teacher ‖ uniform)  :  10.3750  nats\n",
      "KL(uniform ‖ teacher)  :  11.1250  nats\n",
      "KL(uniform ‖ uniform)  :   0.0000  nats (≈0)\n",
      "log |V|                :  12.4529  nats  (upper bound)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import log\n",
    "\n",
    "def uniform_kl_sanity(\n",
    "    teacher_logits_BLV: torch.Tensor,\n",
    "    reduction: str = \"batchmean\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints KL(teacher ‖ uniform) and KL(uniform ‖ teacher) for a quick scale check.\n",
    "    \"\"\"\n",
    "    B, L, V = teacher_logits_BLV.shape\n",
    "    device  = teacher_logits_BLV.device\n",
    "    dtype   = teacher_logits_BLV.dtype\n",
    "\n",
    "    # 1) build uniform logits (all zeros ⇒ softmax == 1/V)\n",
    "    uniform_logits_BLV = torch.zeros((B, L, V), device=device, dtype=dtype)\n",
    "\n",
    "    # 2) helper : per-token KL averaged over tokens\n",
    "    # def kl_tok_avg(p_logits, q_logits):\n",
    "    #     p = torch.softmax(p_logits, dim=-1)\n",
    "    #     log_q = torch.log_softmax(q_logits, dim=-1)\n",
    "    #     # token-level average: sum over V then mean over B×L\n",
    "    #     return F.kl_div(log_q, p, reduction=\"none\").sum(-1).mean().item()\n",
    "\n",
    "    # kl_teacher_uniform   = kl_tok_avg(teacher_logits_BLV, uniform_logits_BLV)\n",
    "    # kl_uniform_teacher   = kl_tok_avg(uniform_logits_BLV, teacher_logits_BLV)\n",
    "    # kl_uniform_uniform   = kl_tok_avg(uniform_logits_BLV, uniform_logits_BLV)\n",
    "\n",
    "    kl_teacher_uniform = kl_between_logits(teacher_logits_BLV, uniform_logits_BLV).mean().item()\n",
    "    kl_uniform_teacher = kl_between_logits(uniform_logits_BLV, teacher_logits_BLV).mean().item()\n",
    "    kl_uniform_uniform = kl_between_logits(uniform_logits_BLV, uniform_logits_BLV).mean().item()\n",
    "\n",
    "    print(f\"KL(teacher ‖ uniform)  : {kl_teacher_uniform:8.4f}  nats\")\n",
    "    print(f\"KL(uniform ‖ teacher)  : {kl_uniform_teacher:8.4f}  nats\")\n",
    "    print(f\"KL(uniform ‖ uniform)  : {kl_uniform_uniform:8.4f}  nats (≈0)\")\n",
    "    print(f\"log |V|                : {log(V):8.4f}  nats  (upper bound)\")\n",
    "\n",
    "# --- example call on your first batch ----------------------------------------\n",
    "with torch.no_grad():\n",
    "    sample_logits_BLV = model(**batched_tokens[0]).logits\n",
    "    uniform_kl_sanity(sample_logits_BLV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{0: 44873.142857142855, 1: 32694.85714285714, 2: 24758.85714285714, 3: 19017.14285714286, 4: 14966.857142857143, 5: 13147.42857142857, 6: 10377.142857142857, 7: 8594.285714285714, 8: 7844.571428571428, 9: 6976.0, 10: 6258.285714285715, 11: 5490.285714285715, 12: 5339.428571428572, 13: 4781.714285714285, 14: 4530.285714285715, 15: 4093.714285714286, 16: 3332.5714285714284, 17: 3140.5714285714284, 18: 2980.5714285714284, 19: 2857.1428571428573, 20: 2253.714285714286, 21: 1982.857142857143, 22: 1657.142857142857, 23: 1313.142857142857, 24: 641.1428571428571, 25: 28.928571428571427}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
