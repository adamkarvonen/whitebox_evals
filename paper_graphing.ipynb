{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "from typing import Dict, Tuple, List, Optional, Any\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps  # Updated import\n",
    "from matplotlib.patches import Patch  # For custom legends\n",
    "import pandas as pd\n",
    "\n",
    "# --- Constants ---\n",
    "ANTI_BIAS_LABELS = {\n",
    "    \"v0.txt\": \"Prompt v0 (None)\",\n",
    "    \"v1.txt\": \"Prompt v1\",\n",
    "    \"v2.txt\": \"Prompt v2\",\n",
    "    \"v3.txt\": \"Prompt v3\",\n",
    "    \"v4.txt\": \"Prompt v4\",\n",
    "}\n",
    "Z_SCORE = 1.96  # For 95% CI (used in CI calculations and error bar averaging)\n",
    "# from scipy import stats # Alternative if you want exact Z_SCORE\n",
    "# Z_SCORE = stats.norm.ppf(1 - 0.05 / 2)\n",
    "\n",
    "# On Chain of Thought evals sometimes there are a few invalid responses\n",
    "INVALID_TOLERANCE = 6\n",
    "\n",
    "IMAGE_OUTPUT_DIR = \"paper_images\"\n",
    "os.makedirs(IMAGE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_DISPLAY_NAMES = {\n",
    "    \"google/gemini-2.5-flash-preview-05-20\": \"Gemini 2.5 Flash\",\n",
    "    \"anthropic/claude-3.5-sonnet\": \"Claude 3.5 Sonnet\",\n",
    "    \"openai/gpt-4o-2024-08-06\": \"GPT-4o\",\n",
    "    \"meta-llama/llama-3.3-70b-instruct\": \"Llama 3.3 70B\",\n",
    "    \"anthropic/claude-sonnet-4\": \"Claude Sonnet 4\",\n",
    "    \"google/gemma-2-27b-it\": \"Gemma-2 27B\",\n",
    "    \"google/gemma-3-12b-it\": \"Gemma-3 12B\",\n",
    "    \"google/gemma-3-27b-it\": \"Gemma-3 27B\",\n",
    "    \"mistralai/Mistral-Small-24B-Instruct-2501\": \"Mistral Small 24B\",\n",
    "}\n",
    "\n",
    "\n",
    "# --- Statistical Helper Functions ---\n",
    "def _calculate_mcnemar(n1_only: int, n2_only: int) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Continuity-corrected McNemar test.\n",
    "\n",
    "    When n1_only + n2_only == 0 (no discordant pairs) we return\n",
    "    statistic = 0 and p = 1 rather than raising an error.\n",
    "    \"\"\"\n",
    "    if n1_only + n2_only == 0:\n",
    "        return 0.0, 1.0  # models never disagreed → no evidence of difference\n",
    "\n",
    "    statistic = (abs(n1_only - n2_only) - 1) ** 2 / (n1_only + n2_only)\n",
    "    p_value = 1 - stats.chi2.cdf(statistic, df=1)\n",
    "    return statistic, p_value\n",
    "\n",
    "\n",
    "def wilson_confidence_interval(\n",
    "    successes: int, trials: int, alpha: float = 0.05\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculates Wilson score interval for a binomial proportion.\"\"\"\n",
    "    if trials == 0:\n",
    "        raise ValueError(\"No data found. Skipping.\")\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    current_z = Z_SCORE if alpha == 0.05 else stats.norm.ppf(1 - alpha / 2)\n",
    "\n",
    "    p_hat = successes / trials\n",
    "    denominator = 1 + current_z**2 / trials\n",
    "    center = (p_hat + current_z**2 / (2 * trials)) / denominator\n",
    "    term_under_sqrt = (p_hat * (1 - p_hat) / trials) + (current_z**2 / (4 * trials**2))\n",
    "    margin = (current_z / denominator) * math.sqrt(term_under_sqrt)\n",
    "\n",
    "    # Ensure CI bounds are within [0, 1]\n",
    "    lower_bound = max(0, center - margin)\n",
    "    upper_bound = min(1, center + margin)\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "\n",
    "def calculate_paired_difference(\n",
    "    n_a_only: int,\n",
    "    n_b_only: int,\n",
    "    n_pairs: int,\n",
    "    alpha: float = 0.05,\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Wald difference-in-proportions CI for paired data.\n",
    "    Returns 0 ± 0 when the models agree on every pair.\n",
    "    \"\"\"\n",
    "    if n_pairs == 0:\n",
    "        raise ValueError(\"No pairs found.\")\n",
    "\n",
    "    # If the models are identical on all pairs → diff = 0, SE = 0\n",
    "    if (n_a_only + n_b_only) == 0:\n",
    "        diff = 0.0\n",
    "        return diff, diff, diff  # CI collapses to the point estimate\n",
    "\n",
    "    # --- Standard case (at least one discordant pair) ---\n",
    "    diff = (n_a_only - n_b_only) / n_pairs\n",
    "    p1 = n_a_only / n_pairs\n",
    "    p2 = n_b_only / n_pairs\n",
    "    se = math.sqrt((p1 + p2 - (p1 - p2) ** 2) / n_pairs)\n",
    "\n",
    "    z = Z_SCORE if alpha == 0.05 else stats.norm.ppf(1 - alpha / 2)\n",
    "    margin = z * se\n",
    "    return diff, diff - margin, diff + margin\n",
    "\n",
    "\n",
    "# --- Data Processing Helper Functions ---\n",
    "def find_files_recursive(\n",
    "    folder_path: str, exclude_patterns: Optional[List[str]] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"Recursively finds files in a folder, optionally excluding patterns.\"\"\"\n",
    "    if exclude_patterns is None:\n",
    "        exclude_patterns = []\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        if any(pattern in root for pattern in exclude_patterns):\n",
    "            continue\n",
    "        for file in files:\n",
    "            if not any(pattern in file for pattern in exclude_patterns):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "    return all_files\n",
    "\n",
    "\n",
    "def process_response(resp: str, system_prompt_filename: str) -> Optional[float]:\n",
    "    \"\"\"Processes a model's response string to 0, 1, or None based on system prompt type.\"\"\"\n",
    "    resp_stripped = resp.strip().lower()\n",
    "    if system_prompt_filename in (\n",
    "        \"yes_no.txt\",\n",
    "        \"yes_no_anthropic.txt\",\n",
    "        \"yes_no_high_bar.txt\",\n",
    "    ):\n",
    "        if \"yes\" in resp_stripped and \"no\" in resp_stripped:\n",
    "            return None\n",
    "        if \"yes\" in resp_stripped:\n",
    "            return 1.0\n",
    "        if \"no\" in resp_stripped:\n",
    "            return 0.0\n",
    "        return None\n",
    "    elif system_prompt_filename in (\n",
    "        \"yes_no_cot.txt\",\n",
    "        \"yes_no_qualifications.txt\",\n",
    "        \"yes_no_high_bar_cot.txt\",\n",
    "    ):\n",
    "        if \"answer:\" in resp_stripped:\n",
    "            final_answer = resp_stripped.split(\"answer:\")[1].strip().lower()\n",
    "            if final_answer == \"yes\":\n",
    "                return 1.0\n",
    "            if final_answer == \"no\":\n",
    "                return 0.0\n",
    "        return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown system prompt filename: {system_prompt_filename}\")\n",
    "\n",
    "\n",
    "def simplify_job_desc(job_desc_file_path: str) -> Optional[str]:\n",
    "    \"\"\"Simplifies job description file path to 'base_description' or 'meta_job_description'.\"\"\"\n",
    "    if not job_desc_file_path:\n",
    "        return None\n",
    "    filename = os.path.basename(job_desc_file_path).lower()\n",
    "    if \"base_description\" in filename:\n",
    "        return \"base_description\"\n",
    "    if \"meta_job_description\" in filename:\n",
    "        return \"meta_job_description\"\n",
    "    if \"gm_job_description\" in filename:\n",
    "        return \"gm_job_description\"\n",
    "    print(\n",
    "        f\"Warning: Job description file '{job_desc_file_path}' not recognized. Using basename.\"\n",
    "    )\n",
    "    return os.path.basename(job_desc_file_path)\n",
    "\n",
    "\n",
    "def calculate_bias_rates(\n",
    "    all_resumes: Dict[str, Dict[Tuple[str, str], int]],\n",
    "    filepath: str,\n",
    "    alpha: float = 0.05,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Calculates overall acceptance rates, group rates, and bias gaps (race, gender).\"\"\"\n",
    "    races = sorted(\n",
    "        list(\n",
    "            set(\n",
    "                race\n",
    "                for resume_data in all_resumes.values()\n",
    "                for race, _ in resume_data.keys()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    genders = sorted(\n",
    "        list(\n",
    "            set(\n",
    "                gender\n",
    "                for resume_data in all_resumes.values()\n",
    "                for _, gender in resume_data.keys()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    assert \"Male\" in genders and \"Female\" in genders, (\n",
    "        \"Gender values must include 'Male' and 'Female'\"\n",
    "    )\n",
    "    assert \"White\" in races and \"Black\" in races, (\n",
    "        \"Race values must include 'White' and 'Black'\"\n",
    "    )\n",
    "\n",
    "    results: Dict[str, Any] = {}\n",
    "\n",
    "    # Overall stats\n",
    "    all_decisions = [val for rd in all_resumes.values() for val in rd.values()]\n",
    "    total_decisions = len(all_decisions)\n",
    "    total_accepted = sum(all_decisions)\n",
    "    overall_rate = total_accepted / total_decisions if total_decisions > 0 else 0.0\n",
    "    overall_ci_low, overall_ci_high = wilson_confidence_interval(\n",
    "        total_accepted, total_decisions, alpha\n",
    "    )\n",
    "    results[\"overall\"] = {\n",
    "        \"rate\": overall_rate,\n",
    "        \"ci_low\": overall_ci_low,\n",
    "        \"ci_high\": overall_ci_high,\n",
    "        \"n\": total_decisions,\n",
    "        \"n_accepted\": total_accepted,\n",
    "    }\n",
    "\n",
    "    # Group stats\n",
    "    group_stats = {}\n",
    "    for race_val in races:\n",
    "        for gender_val in genders:\n",
    "            group_decisions = [\n",
    "                rd[(race_val, gender_val)]\n",
    "                for rd in all_resumes.values()\n",
    "                if (race_val, gender_val) in rd\n",
    "            ]\n",
    "            accepted = sum(group_decisions)\n",
    "            total = len(group_decisions)\n",
    "            if total > 0:\n",
    "                rate = accepted / total\n",
    "                ci_low, ci_high = wilson_confidence_interval(accepted, total, alpha)\n",
    "                group_stats[f\"{race_val}_{gender_val}\"] = {\n",
    "                    \"rate\": rate,\n",
    "                    \"ci_low\": ci_low,\n",
    "                    \"ci_high\": ci_high,\n",
    "                    \"n\": total,\n",
    "                    \"n_accepted\": accepted,\n",
    "                }\n",
    "    results[\"groups\"] = group_stats\n",
    "\n",
    "    num_invalid_race_pairs = 0\n",
    "\n",
    "    # Race Bias Calculation\n",
    "    n_w_only, n_b_only, n_both_accepted, n_neither_accepted, n_pairs_race = (\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    )\n",
    "    for resume_data in all_resumes.values():\n",
    "        for gender_val in genders:  # Iterate over genders to pair within each gender\n",
    "            has_black = (\"Black\", gender_val) in resume_data\n",
    "            has_white = (\"White\", gender_val) in resume_data\n",
    "            if has_black and has_white:\n",
    "                n_pairs_race += 1\n",
    "                black_acc = resume_data[(\"Black\", gender_val)]\n",
    "                white_acc = resume_data[(\"White\", gender_val)]\n",
    "                if white_acc == 1 and black_acc == 0:\n",
    "                    n_w_only += 1\n",
    "                elif white_acc == 0 and black_acc == 1:\n",
    "                    n_b_only += 1\n",
    "                elif white_acc == 1 and black_acc == 1:\n",
    "                    n_both_accepted += 1\n",
    "                else:\n",
    "                    n_neither_accepted += 1  # both 0\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Invalid race pair: {race_val} {gender_val}, {has_black} {has_white} on {filepath}\"\n",
    "                )\n",
    "                num_invalid_race_pairs += 1\n",
    "                continue\n",
    "\n",
    "    if num_invalid_race_pairs > INVALID_TOLERANCE:\n",
    "        print(f\"num_invalid_race_pairs: {num_invalid_race_pairs}\")\n",
    "        print(\n",
    "            f\"n_w_only: {n_w_only}, n_b_only: {n_b_only}, n_pairs_race: {n_pairs_race}, {n_neither_accepted}, {n_both_accepted}, {filepath}\"\n",
    "        )\n",
    "        raise ValueError(\"Stop here\")\n",
    "\n",
    "    diff, ci_low, ci_high = calculate_paired_difference(\n",
    "        n_w_only, n_b_only, n_pairs_race, alpha\n",
    "    )\n",
    "    mcnemar_stat, p_value = _calculate_mcnemar(n_w_only, n_b_only)\n",
    "    results[\"race_gap\"] = {\n",
    "        \"difference\": diff,\n",
    "        \"ci_low\": ci_low,\n",
    "        \"ci_high\": ci_high,\n",
    "        \"n_pairs\": n_pairs_race,\n",
    "        \"n_white_favor\": n_w_only,\n",
    "        \"n_black_favor\": n_b_only,\n",
    "        \"n_both_accepted\": n_both_accepted,\n",
    "        \"n_neither_accepted\": n_neither_accepted,\n",
    "        \"mcnemar_statistic\": mcnemar_stat,\n",
    "        \"p_value\": p_value,\n",
    "    }\n",
    "\n",
    "    num_invalid_gender_pairs = 0\n",
    "\n",
    "    # Gender gap (Male - Female)\n",
    "    (\n",
    "        n_m_only,\n",
    "        n_f_only,\n",
    "        n_both_accepted_gender,\n",
    "        n_neither_accepted_gender,\n",
    "        n_pairs_gender,\n",
    "    ) = 0, 0, 0, 0, 0\n",
    "    for resume_data in all_resumes.values():\n",
    "        for race_val in races:  # Iterate over races to pair within each race\n",
    "            has_female = (race_val, \"Female\") in resume_data\n",
    "            has_male = (race_val, \"Male\") in resume_data\n",
    "            if has_female and has_male:\n",
    "                n_pairs_gender += 1\n",
    "                female_acc = resume_data[(race_val, \"Female\")]\n",
    "                male_acc = resume_data[(race_val, \"Male\")]\n",
    "                if male_acc == 1 and female_acc == 0:\n",
    "                    n_m_only += 1\n",
    "                elif male_acc == 0 and female_acc == 1:\n",
    "                    n_f_only += 1\n",
    "                elif male_acc == 1 and female_acc == 1:\n",
    "                    n_both_accepted_gender += 1\n",
    "                else:\n",
    "                    n_neither_accepted_gender += 1  # both 0\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Invalid race pair: {race_val} {gender_val}, {has_male} {has_female} on {filepath}\"\n",
    "                )\n",
    "                num_invalid_gender_pairs += 1\n",
    "                continue\n",
    "\n",
    "    if num_invalid_gender_pairs > INVALID_TOLERANCE:\n",
    "        print(f\"num_invalid_gender_pairs: {num_invalid_gender_pairs}\")\n",
    "        print(\n",
    "            f\"n_w_only: {n_m_only}, n_b_only: {n_f_only}, n_pairs_race: {n_pairs_gender}, {n_neither_accepted}, {n_both_accepted}, {filepath}\"\n",
    "        )\n",
    "        raise ValueError(\"Stop here\")\n",
    "\n",
    "    diff, ci_low, ci_high = calculate_paired_difference(\n",
    "        n_m_only, n_f_only, n_pairs_gender, alpha\n",
    "    )\n",
    "    mcnemar_stat, p_value = _calculate_mcnemar(n_m_only, n_f_only)\n",
    "    results[\"gender_gap\"] = {\n",
    "        \"difference\": diff,\n",
    "        \"ci_low\": ci_low,\n",
    "        \"ci_high\": ci_high,\n",
    "        \"n_pairs\": n_pairs_gender,\n",
    "        \"n_male_favor\": n_m_only,\n",
    "        \"n_female_favor\": n_f_only,\n",
    "        \"n_both_accepted\": n_both_accepted_gender,\n",
    "        \"n_neither_accepted\": n_neither_accepted_gender,\n",
    "        \"mcnemar_statistic\": mcnemar_stat,\n",
    "        \"p_value\": p_value,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- Main Data Loading Function ---\n",
    "def _parse_single_file_data(filepath: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Loads and processes data from a single pickle file.\"\"\"\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    eval_config = data[\"eval_config\"]\n",
    "    system_prompt_filename = eval_config[\"system_prompt_filename\"]\n",
    "    model_name = eval_config[\"model_name\"]\n",
    "    anti_bias_statement_file = eval_config[\"anti_bias_statement_file\"]\n",
    "    raw_job_desc_file = eval_config[\"job_description_file\"]\n",
    "    job_description = simplify_job_desc(raw_job_desc_file)\n",
    "    inference_mode = eval_config[\"inference_mode\"]\n",
    "\n",
    "    if not all(\n",
    "        [\n",
    "            system_prompt_filename,\n",
    "            model_name,\n",
    "            anti_bias_statement_file,\n",
    "            job_description,\n",
    "            inference_mode,\n",
    "        ]\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Missing one or more key eval_config fields in {filepath}. Skipping.\"\n",
    "        )\n",
    "\n",
    "    results_list = data[\"results\"]\n",
    "    if not results_list:\n",
    "        raise ValueError(\"No results found. Skipping.\")\n",
    "\n",
    "    all_resumes: Dict[str, Dict[Tuple[str, str], int]] = {}\n",
    "    for result in results_list:\n",
    "        if \"@gmail.com\" in result[\"resume\"]:\n",
    "            resume_key_part = result[\"resume\"].split(\"@gmail.com\")[-1]\n",
    "        elif \"Alumni Tech Network\" in result[\"resume\"]:\n",
    "            resume_key_part = result[\"resume\"].split(\"Alumni Tech Network\")[-1]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown resume format: {result['resume']}\")\n",
    "        race = result[\"race\"]\n",
    "        gender = result[\"gender\"]\n",
    "        response_str = result[\"response\"]\n",
    "\n",
    "        if not all([resume_key_part, race, gender, response_str]):\n",
    "            continue\n",
    "\n",
    "        accepted_val = process_response(response_str, system_prompt_filename)\n",
    "        if accepted_val is None:\n",
    "            continue\n",
    "\n",
    "        if resume_key_part not in all_resumes:\n",
    "            all_resumes[resume_key_part] = {}\n",
    "        all_resumes[resume_key_part][(race, gender)] = int(accepted_val)\n",
    "\n",
    "    if not all_resumes:\n",
    "        raise ValueError(\"No resumes found.\")\n",
    "\n",
    "    bias_stats = calculate_bias_rates(\n",
    "        all_resumes, filepath\n",
    "    )  # Uses global Z_SCORE via wilson_confidence_interval\n",
    "\n",
    "    if \"race_gap\" in bias_stats and bias_stats[\"race_gap\"][\"n_pairs\"] > 0:\n",
    "        race_diff = bias_stats[\"race_gap\"][\"difference\"]\n",
    "        race_ci_low = bias_stats[\"race_gap\"][\"ci_low\"]\n",
    "        race_ci_high = bias_stats[\"race_gap\"][\"ci_high\"]\n",
    "    else:\n",
    "        raise ValueError(\"No race data found. Skipping.\")\n",
    "\n",
    "    if \"gender_gap\" in bias_stats and bias_stats[\"gender_gap\"][\"n_pairs\"] > 0:\n",
    "        gender_diff = bias_stats[\"gender_gap\"][\"difference\"]\n",
    "        gender_ci_low = bias_stats[\"gender_gap\"][\"ci_low\"]\n",
    "        gender_ci_high = bias_stats[\"gender_gap\"][\"ci_high\"]\n",
    "    else:\n",
    "        raise ValueError(\"No gender data found. Skipping.\")\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"anti_bias_statement_file\": anti_bias_statement_file,\n",
    "        \"system_prompt_filename\": system_prompt_filename,\n",
    "        \"job_description\": job_description,\n",
    "        \"inference_mode\": inference_mode,\n",
    "        \"race_bias_diff\": race_diff,\n",
    "        \"race_bias_ci_low\": race_ci_low,\n",
    "        \"race_bias_ci_high\": race_ci_high,\n",
    "        \"gender_bias_diff\": gender_diff,\n",
    "        \"gender_bias_ci_low\": gender_ci_low,\n",
    "        \"gender_bias_ci_high\": gender_ci_high,\n",
    "    }\n",
    "\n",
    "\n",
    "def load_and_process_all_data(all_filenames_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Loads data from pickle files, processes responses, calculates bias, and returns a DataFrame.\"\"\"\n",
    "    processed_data_list = []\n",
    "    for filepath in all_filenames_list:\n",
    "        file_data = _parse_single_file_data(filepath)\n",
    "        if file_data:\n",
    "            processed_data_list.append(file_data)\n",
    "    return pd.DataFrame(processed_data_list)\n",
    "\n",
    "\n",
    "# --- Plotting Helper Function ---\n",
    "def _plot_bias_type_specific(\n",
    "    ax,\n",
    "    bias_type_to_plot: str,\n",
    "    plot_data: Dict,\n",
    "    conditions_spec: List[Dict[str, str]],\n",
    "    plottable_models: List[str],\n",
    "    x_main_indices: np.ndarray,\n",
    "    condition_xtick_labels: List[str],\n",
    "    filename: str,\n",
    "    ncol: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"Helper function to generate a plot for a specific bias type (Race or Gender).\"\"\"\n",
    "    n_conditions = len(conditions_spec)\n",
    "    n_plottable_models = len(plottable_models)\n",
    "\n",
    "    if n_plottable_models == 0:\n",
    "        raise ValueError(\"No plottable models found. Skipping plots.\")\n",
    "\n",
    "    group_span_all_models = 0.75\n",
    "    bar_slot_width = (\n",
    "        group_span_all_models / n_plottable_models if n_plottable_models > 0 else 0\n",
    "    )\n",
    "    bar_actual_width = 0.9 * bar_slot_width\n",
    "\n",
    "    model_colors_cmap = colormaps.get_cmap(\"tab10\")\n",
    "\n",
    "    for i_model, model_name in enumerate(plottable_models):\n",
    "        model_offset = (i_model - (n_plottable_models - 1) / 2.0) * bar_slot_width\n",
    "        current_model_bar_positions = x_main_indices + model_offset\n",
    "\n",
    "        biases = [\n",
    "            plot_data[cl][model_name][bias_type_to_plot][0]\n",
    "            for cl in condition_xtick_labels\n",
    "        ]\n",
    "        errors = [\n",
    "            plot_data[cl][model_name][bias_type_to_plot][1]\n",
    "            for cl in condition_xtick_labels\n",
    "        ]\n",
    "\n",
    "        ax.bar(\n",
    "            current_model_bar_positions,\n",
    "            biases,\n",
    "            bar_actual_width,\n",
    "            yerr=errors,\n",
    "            color=model_colors_cmap(i_model % model_colors_cmap.N),\n",
    "            label=MODEL_DISPLAY_NAMES.get(model_name, model_name),\n",
    "            capsize=4,\n",
    "            zorder=3,\n",
    "        )\n",
    "\n",
    "    for i in range(n_conditions - 1):\n",
    "        ax.axvline(\n",
    "            x_main_indices[i] + 0.5,\n",
    "            color=\"grey\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.2,\n",
    "            zorder=1,\n",
    "        )\n",
    "\n",
    "    if bias_type_to_plot == \"Race\":\n",
    "        ax.set_ylabel(\"Race Bias (Positive favors White applicants)\")\n",
    "    elif bias_type_to_plot == \"Gender\":\n",
    "        ax.set_ylabel(\"Gender Bias (Positive favors Male applicants)\")\n",
    "\n",
    "    ax.set_xticks(x_main_indices)\n",
    "    ax.set_xticklabels(condition_xtick_labels, rotation=0, ha=\"center\")\n",
    "    ax.axhline(0, color=\"black\", linewidth=0.8, linestyle=\"--\", zorder=2)\n",
    "\n",
    "    legend_elements = [\n",
    "        Patch(\n",
    "            facecolor=model_colors_cmap(i % model_colors_cmap.N),\n",
    "            label=MODEL_DISPLAY_NAMES.get(model_name, model_name),\n",
    "        )\n",
    "        for i, model_name in enumerate(plottable_models)\n",
    "    ]\n",
    "\n",
    "    if legend_elements:\n",
    "        # ax.legend(handles=legend_elements, title='Model', loc='center left',\n",
    "        #           bbox_to_anchor=(1.02, 0.5), ncol=1, frameon=False)\n",
    "        if ncol is None:\n",
    "            ncol = len(legend_elements)\n",
    "            # ncol = 2\n",
    "        ax.legend(\n",
    "            handles=legend_elements,\n",
    "            title=\"Model\",\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, -0.12),  # Centered below plot\n",
    "            ncol=ncol,  # Adjust number of columns as needed\n",
    "            frameon=False,\n",
    "        )\n",
    "\n",
    "    # Print data for text output\n",
    "    print(f\"Data for {filename} {bias_type_to_plot} Bias Plot:\")\n",
    "    for model_name in plottable_models:\n",
    "        print(\n",
    "            f\"\\nModel: {MODEL_DISPLAY_NAMES.get(model_name, model_name)}\"\n",
    "        )  # Use display name if available\n",
    "        for i, condition_label in enumerate(condition_xtick_labels):\n",
    "            bias = plot_data[condition_label][model_name][bias_type_to_plot][0]\n",
    "            error = plot_data[condition_label][model_name][bias_type_to_plot][1]\n",
    "            print(f\"  {condition_label}: Bias = {bias:.4f}, Error = ±{error:.4f}\")\n",
    "\n",
    "\n",
    "# --- Main Plotting Functions ---\n",
    "def create_graph1(\n",
    "    df: pd.DataFrame,\n",
    "    conditions_spec: List[Dict[str, str]],\n",
    "    filename: Optional[str] = None,\n",
    "    fig_width: Optional[float] = None,\n",
    "    ncol: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"Generates Graph 1 as two separate plots for Race and Gender bias, averaged over anti-bias prompts.\"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty. Cannot generate graphs.\")\n",
    "\n",
    "    model_names = sorted(df[\"model_name\"].dropna().unique())\n",
    "    if not model_names:\n",
    "        raise ValueError(\"No model names found. Cannot generate graphs.\")\n",
    "\n",
    "    plot_data: Dict[str, Dict[str, Dict[str, Tuple[float, float]]]] = {}\n",
    "\n",
    "    for cond_dict in conditions_spec:\n",
    "        cond_label = cond_dict[\"label\"]\n",
    "        plot_data[cond_label] = {}\n",
    "\n",
    "        # condition_df = df[\n",
    "        #     (df[\"inference_mode\"] == cond_dict[\"inference_mode\"]) &\n",
    "        #     (df[\"job_description\"] == cond_dict[\"job_description\"])\n",
    "        # ]\n",
    "\n",
    "        condition_df = df.copy()\n",
    "\n",
    "        for condition in cond_dict:\n",
    "            if condition == \"label\":\n",
    "                continue\n",
    "            condition_df = condition_df[condition_df[condition] == cond_dict[condition]]\n",
    "\n",
    "        if condition_df.empty:\n",
    "            raise ValueError(f\"No data for condition {cond_label}. Skipping.\")\n",
    "\n",
    "        for model_name in model_names:\n",
    "            model_cond_df = condition_df[condition_df[\"model_name\"] == model_name]\n",
    "            plot_data[cond_label][model_name] = {}\n",
    "\n",
    "            for bias_type in [\"Race\", \"Gender\"]:\n",
    "                diff_col = f\"{bias_type.lower()}_bias_diff\"\n",
    "                ci_low_col = f\"{bias_type.lower()}_bias_ci_low\"\n",
    "                ci_high_col = f\"{bias_type.lower()}_bias_ci_high\"\n",
    "\n",
    "                valid_entries = model_cond_df[\n",
    "                    [diff_col, ci_low_col, ci_high_col]\n",
    "                ].dropna()\n",
    "\n",
    "                if valid_entries.empty:\n",
    "                    plot_data[cond_label][model_name][bias_type] = (np.nan, np.nan)\n",
    "                    print(valid_entries)\n",
    "                    print(model_cond_df)\n",
    "                    raise ValueError(\n",
    "                        f\"No valid entries for {bias_type} bias for model {model_name} in condition {cond_label}.\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                diffs = valid_entries[diff_col].values\n",
    "                ci_lows = valid_entries[ci_low_col].values\n",
    "                ci_highs = valid_entries[ci_high_col].values\n",
    "\n",
    "                ses = (ci_highs - ci_lows) / (2 * Z_SCORE)  # Assumes CIs were 95%\n",
    "\n",
    "                avg_diff = np.mean(diffs)\n",
    "                avg_se = (\n",
    "                    np.sqrt(np.sum(ses**2) / (len(ses) ** 2))\n",
    "                    if len(ses) > 0\n",
    "                    else np.nan\n",
    "                )\n",
    "                error_bar_half_width = (\n",
    "                    Z_SCORE * avg_se if not np.isnan(avg_se) else np.nan\n",
    "                )\n",
    "\n",
    "                plot_data[cond_label][model_name][bias_type] = (\n",
    "                    avg_diff,\n",
    "                    error_bar_half_width,\n",
    "                )\n",
    "\n",
    "    plottable_models = [\n",
    "        m\n",
    "        for m in model_names\n",
    "        if any(\n",
    "            cond_spec[\"label\"] in plot_data\n",
    "            and m in plot_data[cond_spec[\"label\"]]\n",
    "            and not (\n",
    "                np.isnan(plot_data[cond_spec[\"label\"]][m][\"Race\"][0])\n",
    "                and np.isnan(plot_data[cond_spec[\"label\"]][m][\"Gender\"][0])\n",
    "            )\n",
    "            for cond_spec in conditions_spec\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    print(plot_data)\n",
    "\n",
    "    if not plottable_models:\n",
    "        raise ValueError(\"No plottable data after aggregation. Skipping plots.\")\n",
    "\n",
    "    n_conditions = len(conditions_spec)\n",
    "    condition_xtick_labels = [c[\"label\"] for c in conditions_spec]\n",
    "    x_main_indices = np.arange(n_conditions)\n",
    "\n",
    "    if fig_width is None:\n",
    "        fig_width = min(25, max(10, len(plottable_models) * n_conditions * 0.5))\n",
    "\n",
    "    plot_configs = [\n",
    "        {\"bias_type\": \"Race\", \"suffix\": \"_race_bias.png\"},\n",
    "        {\"bias_type\": \"Gender\", \"suffix\": \"_gender_bias.png\"},\n",
    "    ]\n",
    "\n",
    "    for config in plot_configs:\n",
    "        fig, ax = plt.subplots(figsize=(fig_width, 7.5))\n",
    "\n",
    "        if filename:\n",
    "            assert filename.endswith(\".png\"), \"Filename must end with .png\"\n",
    "            output_filename = filename.replace(\".png\", config[\"suffix\"])\n",
    "            output_filename = os.path.join(IMAGE_OUTPUT_DIR, output_filename)\n",
    "\n",
    "        _plot_bias_type_specific(\n",
    "            ax=ax,\n",
    "            bias_type_to_plot=config[\"bias_type\"],\n",
    "            plot_data=plot_data,\n",
    "            conditions_spec=conditions_spec,\n",
    "            plottable_models=plottable_models,\n",
    "            x_main_indices=x_main_indices,\n",
    "            condition_xtick_labels=condition_xtick_labels,\n",
    "            filename=output_filename,\n",
    "            ncol=ncol,\n",
    "        )\n",
    "        plt.tight_layout(rect=[0, 0.05, 0.85, 0])  # For legend on right\n",
    "        # plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
    "\n",
    "        if filename:\n",
    "            plt.savefig(output_filename, dpi=300, bbox_inches=\"tight\")\n",
    "            print(f\"Graph 1 ({config['bias_type']} bias) saved as {output_filename}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --- Data Retrieval ---\n",
    "def get_data_df(\n",
    "    folders_to_scan: List[str], exclude_patterns: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Finds result files, loads them, processes, and returns a master DataFrame.\"\"\"\n",
    "    all_result_filenames = []\n",
    "    for folder_path in folders_to_scan:\n",
    "        if not os.path.exists(folder_path):\n",
    "            raise ValueError(f\"Folder '{folder_path}' does not exist. Skipping.\")\n",
    "        print(f\"Searching in {folder_path}...\")\n",
    "        files_in_folder = find_files_recursive(\n",
    "            folder_path, exclude_patterns=exclude_patterns\n",
    "        )\n",
    "        all_result_filenames.extend(files_in_folder)\n",
    "        print(f\"Found {len(files_in_folder)} files in {folder_path} (after exclusion).\")\n",
    "\n",
    "    if not all_result_filenames:\n",
    "        raise ValueError(\"No result files found. Returning empty DataFrame.\")\n",
    "\n",
    "    return load_and_process_all_data(all_result_filenames)\n",
    "\n",
    "\n",
    "text_size = 18\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.size\": text_size,\n",
    "        \"axes.titlesize\": text_size,\n",
    "        \"axes.labelsize\": text_size,\n",
    "        \"xtick.labelsize\": text_size,\n",
    "        \"ytick.labelsize\": text_size,\n",
    "        \"legend.fontsize\": text_size,\n",
    "        \"legend.title_fontsize\": text_size,\n",
    "        \"figure.titlesize\": text_size,\n",
    "    }\n",
    ")\n",
    "\n",
    "folders_to_scan = [\"paper_data/figure_1_data\"]  # Example, update as needed\n",
    "current_exclude_patterns = [\"gm_job_description\", \"mmlu\", \"anthropic\", \"v0\"]\n",
    "\n",
    "master_data_df = get_data_df(folders_to_scan, current_exclude_patterns)\n",
    "\n",
    "# --- Graph 1: Bias Across Settings (Averaged over Prompts) ---\n",
    "conditions_spec_graph1 = [\n",
    "    {\n",
    "        \"inference_mode\": \"gpu_forward_pass\",\n",
    "        \"job_description\": \"base_description\",\n",
    "        \"label\": \"Standard Eval\\n(Simple Context)\",\n",
    "    },\n",
    "    {\n",
    "        \"inference_mode\": \"gpu_forward_pass\",\n",
    "        \"job_description\": \"meta_job_description\",\n",
    "        \"label\": \"Standard Eval\\n(Realistic Context)\",\n",
    "    },\n",
    "    {\n",
    "        \"inference_mode\": \"projection_ablations\",\n",
    "        \"job_description\": \"meta_job_description\",\n",
    "        \"label\": \"Internal Mitigation\\n(Realistic Context)\",\n",
    "    },\n",
    "]\n",
    "create_graph1(\n",
    "    master_data_df.copy(), conditions_spec_graph1, filename=\"figure_1.png\", fig_width=14\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_to_scan = [\n",
    "    \"paper_data/score_output_frontier_models\"\n",
    "]  # Current based on user's code snippet\n",
    "\n",
    "# Llama had many failed responses due to open router issues\n",
    "current_exclude_patterns = [\"llama\"]\n",
    "\n",
    "conditions_spec = [\n",
    "    {\n",
    "        \"inference_mode\": \"open_router\",\n",
    "        \"job_description\": \"base_description\",\n",
    "        \"label\": \"Standard Eval\\n(Simple Context)\",\n",
    "    },\n",
    "    {\n",
    "        \"inference_mode\": \"open_router\",\n",
    "        \"job_description\": \"meta_job_description\",\n",
    "        \"label\": \"Standard Eval\\n(Realistic Context)\",\n",
    "    },\n",
    "]\n",
    "\n",
    "master_data_df = get_data_df(folders_to_scan, current_exclude_patterns)\n",
    "\n",
    "print(\n",
    "    f\"\\nSuccessfully processed data into DataFrame with {master_data_df.shape[0]} entries.\"\n",
    ")\n",
    "create_graph1(\n",
    "    master_data_df.copy(), conditions_spec, filename=\"frontier_models_yes_no.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_to_scan = [\n",
    "    \"paper_data/score_output_college_name\"\n",
    "]  # Current based on user's code snippet\n",
    "\n",
    "current_exclude_patterns = []\n",
    "\n",
    "conditions_spec = [\n",
    "    # {\"inference_mode\": \"gpu_forward_pass\", \"job_description\": \"base_description\", \"label\": \"Standard Eval\\n(Simple Context)\"},\n",
    "    {\n",
    "        \"inference_mode\": \"gpu_forward_pass\",\n",
    "        \"job_description\": \"meta_job_description\",\n",
    "        \"label\": \"Standard Eval\\nMeta Company Context\\nCollege Names\",\n",
    "    },\n",
    "    {\n",
    "        \"inference_mode\": \"projection_ablations\",\n",
    "        \"job_description\": \"meta_job_description\",\n",
    "        \"label\": \"Internal Mitigation\\nMeta Company Context\\nCollege Names\",\n",
    "    },\n",
    "]\n",
    "\n",
    "master_data_df = get_data_df(folders_to_scan, current_exclude_patterns)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\\nSuccessfully processed data into DataFrame with {master_data_df.shape[0]} entries.\"\n",
    ")\n",
    "create_graph1(\n",
    "    master_data_df.copy(), conditions_spec, filename=\"open_source_college_names.png\", ncol=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_to_scan = [\n",
    "    \"paper_data/score_output_gm_high_bar_interventions\"\n",
    "]  # Current based on user's code snippet\n",
    "\n",
    "current_exclude_patterns = []\n",
    "\n",
    "conditions_spec = [\n",
    "    # {\"inference_mode\": \"gpu_forward_pass\", \"job_description\": \"base_description\", \"label\": \"Standard Eval\\n(Simple Context)\"},\n",
    "    {\n",
    "        \"inference_mode\": \"gpu_forward_pass\",\n",
    "        \"job_description\": \"gm_job_description\",\n",
    "        \"label\": \"Standard Eval\\nGM Company Context\\nSelective Hiring\",\n",
    "    },\n",
    "    {\n",
    "        \"inference_mode\": \"projection_ablations\",\n",
    "        \"job_description\": \"gm_job_description\",\n",
    "        \"label\": \"Internal Mitigation\\nGM Company Context\\nSelective Hiring\",\n",
    "    },\n",
    "]\n",
    "\n",
    "master_data_df = get_data_df(folders_to_scan, current_exclude_patterns)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\\nSuccessfully processed data into DataFrame with {master_data_df.shape[0]} entries.\"\n",
    ")\n",
    "create_graph1(\n",
    "    master_data_df.copy(), conditions_spec, filename=\"open_source_gm_selective_hiring.png\", ncol=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_to_scan = [\n",
    "    \"paper_data/score_output_frontier_low_bar_cot\",\n",
    "    \"paper_data/score_output_frontier_high_bar_cot\",\n",
    "]  # Current based on user's code snippet\n",
    "\n",
    "# Llama had many failed responses due to open router issues\n",
    "current_exclude_patterns = [\"llama\"]\n",
    "\n",
    "conditions_spec = [\n",
    "    # {\"inference_mode\": \"open_router\", \"job_description\": \"base_description\", \"label\": \"Standard Eval\\n(Simple Context)\"},\n",
    "    {\n",
    "        \"inference_mode\": \"open_router\",\n",
    "        \"job_description\": \"meta_job_description\",\n",
    "        \"system_prompt_filename\": \"yes_no_cot.txt\",\n",
    "        \"label\": \"Meta Company Context\",\n",
    "    },\n",
    "    {\n",
    "        \"inference_mode\": \"open_router\",\n",
    "        \"job_description\": \"meta_job_description\",\n",
    "        \"system_prompt_filename\": \"yes_no_high_bar_cot.txt\",\n",
    "        \"label\": \"Meta Company and\\nSelective Hiring Context\",\n",
    "    },\n",
    "]\n",
    "\n",
    "master_data_df = get_data_df(folders_to_scan, current_exclude_patterns)\n",
    "\n",
    "# TODO: Rerun gemini here\n",
    "\n",
    "print(\n",
    "    f\"\\nSuccessfully processed data into DataFrame with {master_data_df.shape[0]} entries.\"\n",
    ")\n",
    "create_graph1(\n",
    "    master_data_df.copy(), conditions_spec, filename=\"frontier_models_yes_no_cot.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph2(\n",
    "    df: pd.DataFrame, inference_mode_filter: str, filename_prefix: str = \"graph2\"\n",
    "):\n",
    "    \"\"\"Generates Graph 2: Bias by anti-bias prompt, per model, for a specific inference mode.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"Graph 2: Input DataFrame is empty. Cannot generate graphs.\")\n",
    "        return\n",
    "\n",
    "    model_names = sorted(df[\"model_name\"].dropna().unique())\n",
    "    if not model_names:\n",
    "        print(\"Graph 2: No model names found. Cannot generate graphs.\")\n",
    "        return\n",
    "\n",
    "    for model_name in model_names:\n",
    "        model_df = df[\n",
    "            (df[\"model_name\"] == model_name)\n",
    "            & (df[\"inference_mode\"] == inference_mode_filter)\n",
    "        ]\n",
    "\n",
    "        if model_df.empty:\n",
    "            continue  # Skip if no data for this model and inference mode\n",
    "\n",
    "        conditions_spec_g2 = [\n",
    "            {\"job_description\": \"base_description\", \"label\": \"Simple Eval\"},\n",
    "            {\"job_description\": \"meta_job_description\", \"label\": \"Realistic Eval\"},\n",
    "        ]\n",
    "\n",
    "        anti_bias_files = sorted(\n",
    "            model_df[\"anti_bias_statement_file\"].dropna().unique(),\n",
    "            key=lambda x: ANTI_BIAS_LABELS.get(x, x),\n",
    "        )\n",
    "\n",
    "        if not anti_bias_files:\n",
    "            continue\n",
    "\n",
    "        plottable_data_exists = any(\n",
    "            not model_df[\n",
    "                model_df[\"job_description\"] == cond_dict[\"job_description\"]\n",
    "            ].empty\n",
    "            for cond_dict in conditions_spec_g2\n",
    "        )\n",
    "        if not plottable_data_exists:\n",
    "            continue\n",
    "\n",
    "        bar_width_single = 0.35\n",
    "        n_prompts = len(anti_bias_files)\n",
    "\n",
    "        fig, ax = plt.subplots(\n",
    "            figsize=(max(10, n_prompts * len(conditions_spec_g2) * 0.9), 7)\n",
    "        )\n",
    "\n",
    "        x_tick_positions, x_tick_labels = [], []\n",
    "        current_x_pos = 0\n",
    "        group_gap = 0.5  # Gap between condition groups (Simple Eval, Realistic Eval)\n",
    "        prompt_gap = 0.3  # Gap between prompt bars within a condition\n",
    "\n",
    "        for i_cond, cond_dict in enumerate(conditions_spec_g2):\n",
    "            condition_label_short = cond_dict[\"label\"]\n",
    "            cond_df = model_df[\n",
    "                model_df[\"job_description\"] == cond_dict[\"job_description\"]\n",
    "            ]\n",
    "\n",
    "            if i_cond > 0:  # Add larger gap between different conditions\n",
    "                current_x_pos += group_gap\n",
    "\n",
    "            group_center_start = current_x_pos  # For potential condition label\n",
    "\n",
    "            for prompt_file_name in anti_bias_files:\n",
    "                prompt_label_short = ANTI_BIAS_LABELS.get(\n",
    "                    prompt_file_name, prompt_file_name\n",
    "                )\n",
    "                prompt_cond_df = cond_df[\n",
    "                    cond_df[\"anti_bias_statement_file\"] == prompt_file_name\n",
    "                ]\n",
    "\n",
    "                gender_bias, gender_err = np.nan, np.array([[np.nan], [np.nan]])\n",
    "                race_bias, race_err = np.nan, np.array([[np.nan], [np.nan]])\n",
    "\n",
    "                if not prompt_cond_df.empty:\n",
    "                    gb_row = prompt_cond_df.iloc[0]\n",
    "                    if not pd.isna(gb_row[\"gender_bias_diff\"]):\n",
    "                        gender_bias = gb_row[\"gender_bias_diff\"]\n",
    "                        gender_err = np.array(\n",
    "                            [\n",
    "                                [gender_bias - gb_row[\"gender_bias_ci_low\"]],\n",
    "                                [gb_row[\"gender_bias_ci_high\"] - gender_bias],\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                    if not pd.isna(gb_row[\"race_bias_diff\"]):\n",
    "                        race_bias = gb_row[\"race_bias_diff\"]\n",
    "                        race_err = np.array(\n",
    "                            [\n",
    "                                [race_bias - gb_row[\"race_bias_ci_low\"]],\n",
    "                                [gb_row[\"race_bias_ci_high\"] - race_bias],\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                ax.bar(\n",
    "                    current_x_pos - bar_width_single / 2,\n",
    "                    gender_bias,\n",
    "                    bar_width_single,\n",
    "                    yerr=gender_err,\n",
    "                    color=\"tab:blue\",\n",
    "                    capsize=3,\n",
    "                    label=\"Gender Bias\" if not x_tick_positions else None,\n",
    "                )  # Label once\n",
    "                ax.bar(\n",
    "                    current_x_pos + bar_width_single / 2,\n",
    "                    race_bias,\n",
    "                    bar_width_single,\n",
    "                    yerr=race_err,\n",
    "                    color=\"tab:orange\",\n",
    "                    hatch=\"//\",\n",
    "                    capsize=3,\n",
    "                    label=\"Race Bias\" if not x_tick_positions else None,\n",
    "                )  # Label once\n",
    "\n",
    "                x_tick_positions.append(current_x_pos)\n",
    "                # x_tick_labels.append(f\"{condition_label_short}\\n{prompt_label_short}\")\n",
    "                x_tick_labels.append(\n",
    "                    f\"{prompt_label_short}\"\n",
    "                )  # Simpler labels, condition indicated by grouping\n",
    "                current_x_pos += (bar_width_single * 2) + prompt_gap\n",
    "\n",
    "            # Add condition label below the group\n",
    "            group_center_end = (\n",
    "                current_x_pos - prompt_gap\n",
    "            )  # End of the last prompt bar for this condition\n",
    "            if (\n",
    "                x_tick_positions and group_center_start < group_center_end\n",
    "            ):  # If there were bars for this condition\n",
    "                group_label_x = (\n",
    "                    group_center_start + group_center_end - bar_width_single * 2\n",
    "                ) / 2\n",
    "                ax.text(\n",
    "                    group_label_x,\n",
    "                    ax.get_ylim()[0]\n",
    "                    - (ax.get_ylim()[1] - ax.get_ylim()[0])\n",
    "                    * 0.08,  # Adjust y dynamically\n",
    "                    condition_label_short,\n",
    "                    ha=\"center\",\n",
    "                    va=\"top\",\n",
    "                    fontsize=text_size * 0.9,\n",
    "                    weight=\"bold\",\n",
    "                )\n",
    "\n",
    "        ax.set_ylabel(\"Bias Score (Positive favors White/Male)\")\n",
    "        ax.set_title(\n",
    "            f\"Graph 2: Bias by Anti-Bias Prompt\\nModel: {MODEL_DISPLAY_NAMES.get(model_name, model_name)} ({inference_mode_filter})\"\n",
    "        )\n",
    "        ax.set_xticks(x_tick_positions)\n",
    "        ax.set_xticklabels(x_tick_labels, rotation=45, ha=\"right\")\n",
    "        ax.axhline(0, color=\"black\", linewidth=0.8, linestyle=\"--\")\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()  # Get unique handles/labels\n",
    "        if handles:  # Only create legend if there are labeled items\n",
    "            legend_handles = [\n",
    "                Patch(facecolor=\"tab:blue\", label=\"Gender Bias (Male - Female)\"),\n",
    "                Patch(\n",
    "                    facecolor=\"tab:orange\",\n",
    "                    hatch=\"//\",\n",
    "                    label=\"Race Bias (White - Black)\",\n",
    "                ),\n",
    "            ]\n",
    "            ax.legend(\n",
    "                handles=legend_handles,\n",
    "                title=\"Bias Type\",\n",
    "                loc=\"center left\",\n",
    "                bbox_to_anchor=(1.02, 0.5),\n",
    "            )\n",
    "\n",
    "        plt.subplots_adjust(\n",
    "            bottom=0.25, right=0.85\n",
    "        )  # Make space for rotated labels and legend\n",
    "        # plt.tight_layout(rect=[0, 0.1, 0.85, 0.93]) # Adjust for legend and title\n",
    "\n",
    "        filename_safe_model_name = model_name.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "        output_filename = f\"{filename_prefix}_bias_by_prompt_{filename_safe_model_name}_{inference_mode_filter}.png\"\n",
    "        plt.savefig(output_filename, dpi=300, bbox_inches=\"tight\")\n",
    "        print(\n",
    "            f\"Graph 2 for {model_name} ({inference_mode_filter}) saved as {output_filename}\"\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --- Graph 2: Bias by Anti-Bias Prompt (per model, specific inference mode) ---\n",
    "# You might want to loop this for different inference modes if needed\n",
    "# create_graph2(master_data_df.copy(), inference_mode_filter=\"gpu_forward_pass\", filename_prefix=\"graph2_gpu\")\n",
    "# Example for another inference mode if you have data for it:\n",
    "# create_graph2(master_data_df.copy(), inference_mode_filter=\"projection_ablations\", filename_prefix=\"graph2_proj_abl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
