{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "import torch\n",
    "import einops\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "\n",
    "import mypkg.whitebox_infra.dictionaries.batch_topk_sae as batch_topk_sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "dtype = torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=dtype, device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def get_submodule(model: AutoModelForCausalLM, layer: int):\n",
    "    \"\"\"Gets the residual stream submodule\"\"\"\n",
    "    model_name = model.config._name_or_path\n",
    "\n",
    "    if \"pythia\" in model_name:\n",
    "        return model.gpt_neox.layers[layer]\n",
    "    elif \"gemma\" in model_name or \"mistral\" in model_name:\n",
    "        return model.model.layers[layer]\n",
    "    else:\n",
    "        raise ValueError(f\"Please add submodule for model {model_name}\")\n",
    "\n",
    "\n",
    "chosen_layers = [18]\n",
    "sae_repo = \"adamkarvonen/ministral_saes\"\n",
    "sae_path = \"mistralai_Ministral-8B-Instruct-2410_batch_top_k/resid_post_layer_18/trainer_1/ae.pt\"\n",
    "\n",
    "sae = batch_topk_sae.load_dictionary_learning_batch_topk_sae(\n",
    "    repo_id=sae_repo,\n",
    "    filename=sae_path,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    layer=chosen_layers[0],\n",
    "    local_dir=\"downloaded_saes\",\n",
    ")\n",
    "\n",
    "submodules = [get_submodule(model, chosen_layers[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopException(Exception):\n",
    "    \"\"\"Custom exception for stopping model forward pass early.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_activations(model, submodule, inputs_BL):\n",
    "    \"\"\"\n",
    "    Registers a forward hook on the submodule to capture the residual (or hidden)\n",
    "    activations. We then raise an EarlyStopException to skip unneeded computations.\n",
    "    \"\"\"\n",
    "    activations_BLD = None\n",
    "\n",
    "    def gather_target_act_hook(module, inputs, outputs):\n",
    "        nonlocal activations_BLD\n",
    "        # For many models, the submodule outputs are a tuple or a single tensor:\n",
    "        # If \"outputs\" is a tuple, pick the relevant item:\n",
    "        #   e.g. if your layer returns (hidden, something_else), you'd do outputs[0]\n",
    "        # Otherwise just do outputs\n",
    "        if isinstance(outputs, tuple):\n",
    "            activations_BLD = outputs[0]\n",
    "        else:\n",
    "            activations_BLD = outputs\n",
    "\n",
    "        raise EarlyStopException(\"Early stopping after capturing activations\")\n",
    "\n",
    "    handle = submodule.register_forward_hook(gather_target_act_hook)\n",
    "\n",
    "    try:\n",
    "        _ = model(input_ids=inputs_BL.to(model.device))\n",
    "    except EarlyStopException:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during forward pass: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "    return activations_BLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral has some weird tokenization shit going on, not sure if we need to deal with it\n",
    "\n",
    "# # Import needed packages:\n",
    "# from mistral_common.protocol.instruct.messages import (\n",
    "#     UserMessage,\n",
    "# )\n",
    "# from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "# from mistral_common.protocol.instruct.tool_calls import (\n",
    "#     Function,\n",
    "#     Tool,\n",
    "# )\n",
    "# from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# # Load Mistral tokenizer\n",
    "\n",
    "# model_name = \"open-mixtral-8x22b\"\n",
    "\n",
    "# mistral_tokenizer = MistralTokenizer.from_model(model_name, strict=True)\n",
    "\n",
    "# # Tokenize a list of messages\n",
    "# tokenized = mistral_tokenizer.encode_chat_completion(\n",
    "#     ChatCompletionRequest(\n",
    "#         messages=[\n",
    "#             UserMessage(content=\"How can I center a div?\"),\n",
    "#         ],\n",
    "#         model=model_name,\n",
    "#     )\n",
    "# )\n",
    "# tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# # Count the number of tokens\n",
    "# print(len(tokens))\n",
    "# print(type(tokens))\n",
    "# tokens = torch.tensor(tokens).unsqueeze(0)\n",
    "# print(tokens.shape)\n",
    "\n",
    "# print(tokens, text)\n",
    "\n",
    "# activations_BLD = collect_activations(model, submodules[0], tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = \"Can you continue this sentence? The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science\"\n",
    "\n",
    "test_input = \"[INST]Can you continue this story? The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science[/INST]assistant\"\n",
    "\n",
    "\n",
    "# test_input = \"[INST]How can I center a div?[/INST]assistant\"\n",
    "\n",
    "tokens = tokenizer(test_input, return_tensors=\"pt\", add_special_tokens=True).to(device)[\"input_ids\"]\n",
    "# tokens = tokenizer(test_input, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "\n",
    "print(tokens.shape)\n",
    "\n",
    "activations_BLD = collect_activations(model, submodules[0], tokens)\n",
    "\n",
    "print(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms_BL = activations_BLD.norm(dim=-1)\n",
    "print(norms_BL)\n",
    "print(norms_BL.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.use_threshold = True\n",
    "encoded_BLF = sae.encode(activations_BLD)\n",
    "decoded_BLD = sae.decode(encoded_BLF)\n",
    "\n",
    "torch.set_printoptions(precision=8, sci_mode=False)\n",
    "\n",
    "nonzero_BL = einops.reduce((encoded_BLF > 0).float(), \"b l f -> b l\", \"sum\")\n",
    "print(nonzero_BL)\n",
    "mean_nonzero = nonzero_BL.mean()\n",
    "print(mean_nonzero, \"\\n\\n\")\n",
    "\n",
    "MSE_BL = (activations_BLD - decoded_BLD).pow(2).mean(dim=-1)\n",
    "print(MSE_BL)\n",
    "mean_MSE = MSE_BL.mean()\n",
    "print(mean_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def reconstruct_activations(model, submodule, sae, inputs_BL):\n",
    "\n",
    "    def gather_target_act_hook(module, inputs, outputs):\n",
    "        # For many models, the submodule outputs are a tuple or a single tensor:\n",
    "        # If \"outputs\" is a tuple, pick the relevant item:\n",
    "        #   e.g. if your layer returns (hidden, something_else), you'd do outputs[0]\n",
    "        # Otherwise just do outputs\n",
    "        if isinstance(outputs, tuple):\n",
    "            activations_BLD = outputs[0]\n",
    "        else:\n",
    "            activations_BLD = outputs\n",
    "\n",
    "        encoded_BLF = sae.encode(activations_BLD)\n",
    "        decoded_BLD = sae.decode(encoded_BLF)\n",
    "\n",
    "        outputs = (decoded_BLD,) + outputs[1:]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    handle = submodule.register_forward_hook(gather_target_act_hook)\n",
    "\n",
    "    try:\n",
    "        outputs = model(input_ids=inputs_BL.to(model.device), labels=inputs_BL.to(model.device))\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during forward pass: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "    return outputs\n",
    "\n",
    "original_loss = model(input_ids=tokens.to(model.device), labels=tokens.to(model.device)).loss\n",
    "\n",
    "outputs = reconstruct_activations(model, submodules[0], sae, tokens)\n",
    "\n",
    "print(outputs.loss)\n",
    "print(original_loss)\n",
    "ratio = outputs.loss / original_loss\n",
    "print(ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
